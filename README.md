# ğŸš€ Projeto ETL com Apache Spark & Azure Data Lake

<div align="center">

![Python](https://img.shields.io/badge/Python-3.10+-blue?style=for-the-badge&logo=python&logoColor=white)
![Apache Spark](https://img.shields.io/badge/Apache%20Spark-3.x-orange?style=for-the-badge&logo=apache-spark&logoColor=white)
![Apache Airflow](https://img.shields.io/badge/Apache%20Airflow-2.x-017CEE?style=for-the-badge&logo=apache-airflow&logoColor=white)
![Azure](https://img.shields.io/badge/Microsoft%20Azure-0078D4?style=for-the-badge&logo=microsoft-azure&logoColor=white)
![Docker](https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white)
![Delta Lake](https://img.shields.io/badge/Delta%20Lake-00ADD8?style=for-the-badge&logo=delta&logoColor=white)

**Pipeline ETL Moderno para Processamento de Dados em Larga Escala**

[ğŸ“– DocumentaÃ§Ã£o Completa](https://seu-dominio.github.io/projeto_etl_spark/) â€¢ [ğŸš€ InÃ­cio RÃ¡pido](#-inÃ­cio-rÃ¡pido) â€¢ [ğŸ—ï¸ Arquitetura](#ï¸-arquitetura)

</div>

---

## ğŸ“‹ Ãndice

- [ğŸ“– Sobre o Projeto](#-sobre-o-projeto)
- [ğŸ¯ Objetivos](#-objetivos)
- [ğŸ—ï¸ Arquitetura](#ï¸-arquitetura)
- [âš¡ Funcionalidades](#-funcionalidades)
- [ğŸ› ï¸ Tecnologias](#ï¸-tecnologias)
- [ğŸš€ InÃ­cio RÃ¡pido](#-inÃ­cio-rÃ¡pido)
- [ğŸ“ Estrutura do Projeto](#-estrutura-do-projeto)
- [ğŸ”§ ConfiguraÃ§Ã£o](#-configuraÃ§Ã£o)
- [ğŸ“Š Pipeline de Dados](#-pipeline-de-dados)
- [ğŸ§ª Testes](#-testes)
- [ğŸ¤ ContribuiÃ§Ã£o](#-contribuiÃ§Ã£o)
- [ğŸ‘¥ Equipe](#-equipe)
- [ğŸ“„ LicenÃ§a](#-licenÃ§a)

---

## ğŸ“– Sobre o Projeto

Este projeto implementa um **pipeline ETL moderno e escalÃ¡vel** que extrai dados de um banco SQL Server, processa e transforma os dados usando Apache Spark, e armazena no Azure Data Lake seguindo a arquitetura **Medallion (Bronze, Silver, Gold)**. Todo o processo Ã© orquestrado pelo Apache Airflow com containerizaÃ§Ã£o Docker.

### ğŸ¯ Contexto de NegÃ³cio

O projeto simula um sistema de **logÃ­stica e transporte**, processando dados de:
- ğŸ‘¥ Clientes e motoristas
- ğŸš› VeÃ­culos e frotas
- ğŸ“¦ Entregas e coletas
- ğŸ›£ï¸ Rotas e trajetos
- ğŸ”§ ManutenÃ§Ãµes e abastecimentos
- ğŸš¨ Multas e infraÃ§Ãµes

---

## ğŸ¯ Objetivos

- âœ… **Extrair** dados do SQL Server de forma eficiente
- âœ… **Armazenar** dados no Azure Data Lake com organizaÃ§Ã£o em camadas
- âœ… **Processar** dados com Apache Spark usando Delta Lake
- âœ… **Transformar** dados seguindo melhores prÃ¡ticas de qualidade
- âœ… **Automatizar** todo o pipeline com Apache Airflow
- âœ… **Monitorar** execuÃ§Ãµes e performance
- âœ… **Implementar** modelo dimensional para analytics

---

## ğŸ—ï¸ Arquitetura

![image](docs/assets/pipeline.jpeg)

## âš¡ Funcionalidades

### ğŸ“Š Camadas de Dados (Medallion)
- **ğŸ¥‰ Bronze**: Dados brutos em formato Delta
- **ğŸ¥ˆ Silver**: Dados limpos e padronizados
- **ğŸ¥‡ Gold**: Modelo dimensional e KPIs

### ğŸ›ï¸ OrquestraÃ§Ã£o AvanÃ§ada
- **DAGs parametrizÃ¡veis** no Airflow
- **Retry automÃ¡tico** em caso de falhas
- **NotificaÃ§Ãµes** de status
- **Monitoramento** em tempo real

### ğŸ“ˆ Analytics e KPIs
- **Percentual de entregas no prazo**
- **Custo mÃ©dio de frete por rota**
- **Total de entregas por tipo de veÃ­culo**
- **Valor total de frete por cliente**
- **MÃ©tricas mensais** de performance

---

## ğŸ› ï¸ Tecnologias

<table>
<tr>
<td align="center"><strong>Processamento</strong></td>
<td align="center"><strong>OrquestraÃ§Ã£o</strong></td>
<td align="center"><strong>Armazenamento</strong></td>
<td align="center"><strong>Infraestrutura</strong></td>
</tr>
<tr>
<td align="center">
<img src="https://spark.apache.org/images/spark-logo-trademark.png" width="60"/><br/>
Apache Spark 3.x
</td>
<td align="center">
<img src="https://airflow.apache.org/docs/apache-airflow/stable/_images/pin_large.png" width="60"/><br/>
Apache Airflow 2.x
</td>
<td align="center">
<img src="https://upload.wikimedia.org/wikipedia/commons/f/fa/Microsoft_Azure.svg" width="60"/><br/>
Azure Data Lake
</td>
<td align="center">
<img src="https://www.docker.com/wp-content/uploads/2022/03/vertical-logo-monochromatic.png" width="60"/><br/>
Docker & Compose
</td>
</tr>
<tr>
<td align="center">
<img src="https://delta.io/static/images/delta-lake-logo.png" width="60"/><br/>
Delta Lake
</td>
<td align="center">
<img src="https://www.python.org/static/community_logos/python-logo-master-v3-TM.png" width="60"/><br/>
Python 3.10+
</td>
<td align="center">
<img src="https://upload.wikimedia.org/wikipedia/commons/2/29/Postgresql_elephant.svg" width="60"/><br/>
SQL Server
</td>
<td align="center">
<img src="https://python-poetry.org/images/logo-origami.svg" width="60"/><br/>
Poetry
</td>
</tr>
</table>

---

## ğŸš€ InÃ­cio RÃ¡pido

### ğŸ“‹ PrÃ©-requisitos

Certifique-se de ter instalado:

- ğŸ [Python 3.10+](https://www.python.org/downloads/)
- ğŸ³ [Docker & Docker Compose](https://www.docker.com/)
- â˜ï¸ [Azure CLI](https://learn.microsoft.com/pt-br/cli/azure/install-azure-cli)
- ğŸ“¦ [Poetry](https://python-poetry.org/docs/#installation)

### InstalaÃ§Ã£0

1. Clone o repositÃ³rio
   ```bash
   git clone https://github.com/arturoburigo/projeto_etl_spark
   ```

2. **Inicie o Docker**:
```bash
docker run --platform linux/amd64 -e "ACCEPT_EULA=Y" -e "SA_PASSWORD=satc@2025" -p 1433:1433 --name etl_entregas -d mcr.microsoft.com/mssql/server:2022-latest
```

3. **Rode os scripts de populacao de dados**:
```bash
    poetry run data/create_schema_and_columns.py
    poetry run data/create_tables.py
    poetry run data/faker_data.py

```

4. Com sua conta Microsoft/Azure criada e apta para uso dos recursos pagos, no <a href="https://portal.azure.com/">```Portal Azure```</a> crie um workspace seguindo a <a href="https://learn.microsoft.com/en-us/azure/databricks/getting-started/">```documentaÃ§Ã£o```</a> fornecida pela Microsoft. Durante a execuÃ§Ã£o deste processo, vocÃª irÃ¡ criar um ```resource group```. Salve o nome informado no ```resource group``` pois ele serÃ¡ utilizado logo em seguida.
3. Com o ```Terraform``` instalado e o ```resource group``` em mÃ£os, no arquivo <a href="https://github.com/arturoburigo/projeto_etl_spark/blob/iac/variables.tf">```/iac/variables.tf```</a> modifique a seguinte vÃ¡riavel adicionando o ```resource group``` que vocÃª criou previamente.

![image](docs/assets/terraform_var.png)

4. Nesta etapa, iremos iniciar o deploy do nosso ambiente cloud. ApÃ³s alterar a variÃ¡vel no Ãºltimo passo, acesse a pasta ```/iac``` e execute os seguintes comandos:
   ```bash
   terraform init
   ```

   ```bash
   terraform apply
   ```
5. Com a execuÃ§Ã£o dos comandos finalizada, verifique no <a href="https://portal.azure.com/">```Portal Azure```</a> o ```MS SQL Server```, ```MS SQL Database``` e o ```ADLS Gen2``` contendo os containers ```landing-zone```, ```bronze```, ```silver``` e ```gold``` que foram criados no passo anterior. 

6. No <a href="https://portal.azure.com/">```Portal Azure```</a>, gere um ```SAS TOKEN``` para o contÃªiner ```landing-zone``` seguindo esta <a href="https://learn.microsoft.com/en-us/azure/ai-services/translator/document-translation/how-to-guides/create-sas-tokens?tabs=Containers#create-sas-tokens-in-the-azure-portal">```documentaÃ§Ã£o```</a>. Guarde este token em um local seguro pois ele serÃ¡ utilizado no prÃ³ximo passo. 

7. crie um arquivo .env no diretorio raiz e na pasta astro

8. No mesmo diretÃ³rio, vamos iniciar o processo de populaÃ§Ã£o do nosso banco de dados. Verifique corretamente o preenchimento das vÃ¡riaveis no arquivo ```.env``` e prossiga com os seguintes comandos:
   1. Iniciar ```venv``` (ambiente virtual) do poetry:
        ```bash
        poetry env activate
        ```
   2. Instalar as dependencias`:
        ```bash
        poetry install
        ```

### ğŸ”§ ConfiguraÃ§Ã£o RÃ¡pida

1. **Configure o Azure**:

```bash
az login
# Configure suas credenciais no arquivo .env
```

2. **Inicie o Airflow**:
```bash
cd astro
astro dev start
```

3. **Acesse a interface**:
- ğŸŒ Airflow UI: [http://localhost:8080](http://localhost:8080)

4. **Execute o pipeline**:
- Navegue atÃ© a DAG `sqlserver_to_bronze_adls`
- Clique em "Trigger DAG"

---

## ğŸ“ Estrutura do Projeto

```
projeto_etl_spark/
â”œâ”€â”€ ğŸ“ astro/                    # Ambiente Apache Airflow
â”‚   â”œâ”€â”€ ğŸ“ dags/                 # DAGs de orquestraÃ§Ã£o
â”‚   â”‚   â””â”€â”€ ğŸ“„ main.py           # Pipeline principal ETL
â”‚   â”œâ”€â”€ ğŸ“ tests/                # Testes automatizados
â”‚   â”œâ”€â”€ ğŸ“ include/              # Arquivos auxiliares
â”‚   â”œâ”€â”€ ğŸ“„ Dockerfile            # Imagem customizada Airflow
â”‚   â””â”€â”€ ğŸ“„ requirements.txt      # DependÃªncias Python
â”‚
â”œâ”€â”€ ğŸ“ data/                     # Scripts de processamento
â”‚   â”œâ”€â”€ ğŸ“ bronze/               # Camada Bronze (dados brutos)
â”‚   â”œâ”€â”€ ğŸ“ silver/               # Camada Silver (dados limpos)
â”‚   â”œâ”€â”€ ğŸ“ gold/                 # Camada Gold (modelo dimensional)
â”‚   â”œâ”€â”€ ğŸ“ landingzone/          # Scripts de extraÃ§Ã£o
â”‚   â”œâ”€â”€ ğŸ“ sql/                  # Scripts SQL
â”‚   â”œâ”€â”€ ğŸ“„ faker_data.py         # Gerador de dados sintÃ©ticos
â”‚   â””â”€â”€ ğŸ“„ create_tables.py      # CriaÃ§Ã£o de tabelas
â”‚
â”œâ”€â”€ ğŸ“ docs/                     # DocumentaÃ§Ã£o MkDocs
â”‚   â”œâ”€â”€ ğŸ“ assets/               # Imagens e diagramas
â”‚   â”œâ”€â”€ ğŸ“„ index.md              # PÃ¡gina inicial
â”‚   â”œâ”€â”€ ğŸ“„ instalacao.md         # Guia de instalaÃ§Ã£o
â”‚   â”œâ”€â”€ ğŸ“„ pipeline_etl.md       # DocumentaÃ§Ã£o do pipeline
â”‚   â””â”€â”€ ğŸ“„ arquitetura.md        # DocumentaÃ§Ã£o da arquitetura
â”‚
â”œâ”€â”€ ğŸ“ iac/                      # Infrastructure as Code
â”‚   â”œâ”€â”€ ğŸ“„ main.tf               # Recursos Azure (Terraform)
â”‚   â”œâ”€â”€ ğŸ“„ variables.tf          # VariÃ¡veis Terraform
â”‚   â””â”€â”€ ğŸ“„ provider.tf           # Providers Terraform
â”‚
â”œâ”€â”€ ğŸ“„ pyproject.toml            # ConfiguraÃ§Ã£o Poetry
â”œâ”€â”€ ğŸ“„ mkdocs.yml                # ConfiguraÃ§Ã£o MkDocs
â”œâ”€â”€ ğŸ“„ README.md                 # Este arquivo
â””â”€â”€ ğŸ“„ .env.example              # Exemplo de variÃ¡veis de ambiente
```

---

## ğŸ”§ ConfiguraÃ§Ã£o

### ğŸ” VariÃ¡veis de Ambiente

Crie um arquivo `.env` baseado no `.env.example`:

```bash
# Azure Data Lake
ADLS_ACCOUNT_NAME=seu_storage_account
ADLS_FILE_SYSTEM_NAME=landing
ADLS_BRONZE_CONTAINER_NAME=bronze
ADLS_SILVER_CONTAINER_NAME=silver
ADLS_GOLD_CONTAINER_NAME=gold
ADLS_SAS_TOKEN=seu_sas_token

# SQL Server
SQL_SERVER=seu_servidor.database.windows.net
SQL_DATABASE=seu_database
SQL_SCHEMA=dbo
SQL_USERNAME=seu_usuario
SQL_PASSWORD=sua_senha

# Spark Configuration
SPARK_DRIVER_MEMORY=4g
SPARK_EXECUTOR_MEMORY=4g
SPARK_EXECUTOR_CORES=2
```

### âš™ï¸ ConfiguraÃ§Ã£o do Spark

O projeto inclui configuraÃ§Ãµes otimizadas do Spark:

```python
spark = SparkSession.builder \
    .appName("projeto_etl_spark") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .config("spark.jars.packages", "io.delta:delta-core_2.12:2.3.0") \
    .config("spark.driver.memory", "4g") \
    .config("spark.executor.memory", "4g") \
    .config("spark.sql.adaptive.enabled", "true") \
    .getOrCreate()
```

---

## ğŸ“Š Pipeline de Dados

### ğŸ”„ Fluxo de ExecuÃ§Ã£o

1. **ğŸ” Landing Zone**: ExtraÃ§Ã£o dos dados do SQL Server para CSV
2. **ğŸ¥‰ Bronze Layer**: IngestÃ£o dos CSVs em formato Delta
3. **ğŸ¥ˆ Silver Layer**: Limpeza, padronizaÃ§Ã£o e qualidade dos dados
4. **ğŸ¥‡ Gold Layer**: Modelo dimensional e cÃ¡lculo de KPIs

### ğŸ“ˆ KPIs Calculados

| KPI | DescriÃ§Ã£o | FrequÃªncia |
|-----|-----------|------------|
| **On-Time Delivery** | % de entregas realizadas no prazo | DiÃ¡rio |
| **Custo por Rota** | Custo mÃ©dio de frete por rota | Semanal |
| **UtilizaÃ§Ã£o da Frota** | Total de entregas por tipo de veÃ­culo | Mensal |
| **Revenue por Cliente** | Valor total de frete por cliente | Mensal |

### ğŸ¯ Modelo Dimensional

```
Fato_Entregas
â”œâ”€â”€ Dim_Data (Tempo)
â”œâ”€â”€ Dim_Cliente (Remetente/DestinatÃ¡rio)
â”œâ”€â”€ Dim_Motorista
â”œâ”€â”€ Dim_Veiculo
â”œâ”€â”€ Dim_Rota
â””â”€â”€ Dim_Tipo_Carga
```

---

## ğŸ§ª Testes

Execute os testes automatizados:

```bash
# Testes unitÃ¡rios
poetry run pytest astro/tests/

# Testes de integraÃ§Ã£o
poetry run pytest astro/tests/test_dag_example.py

# Teste de conexÃ£o SQL Server
poetry run python astro/tests/test_sqlserver_connection.py
```

### ğŸ“Š Cobertura de Testes

- âœ… Testes de DAGs
- âœ… Testes de conexÃ£o com SQL Server
- âœ… Testes de transformaÃ§Ãµes Spark
- âœ… Testes de qualidade de dados

---


## ğŸ¤ ContribuiÃ§Ã£o

ContribuiÃ§Ãµes sÃ£o sempre bem-vindas! Siga estes passos:

1. **Fork** o projeto
2. **Crie** uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. **Commit** suas mudanÃ§as (`git commit -m 'Add some AmazingFeature'`)
4. **Push** para a branch (`git push origin feature/AmazingFeature`)
5. **Abra** um Pull Request



## ğŸ‘¥ Equipe

<table>
<tr>
<td align="center">
<a href="https://github.com/arturoburigo">
<img src="https://github.com/arturoburigo.png" width="100px;" alt="Arturo Burigo"/><br />
<sub><b>Arturo Burigo</b></sub>
</a><br />
<sub>Airflow | Terraform | ETL</sub>
</td>
<td align="center">
<a href="https://github.com/bezerraluiz">
<img src="https://github.com/bezerraluiz.png" width="100px;" alt="Luiz Bezerra"/><br />
<sub><b>Luiz Bezerra</b></sub>
</a><br />
<sub>Bronze | Gold | BI</sub>
</td>
<td align="center">
<a href="https://github.com/M0rona">
<img src="https://github.com/M0rona.png" width="100px;" alt="Gabriel Morona"/><br />
<sub><b>Gabriel Morona</b></sub>
</a><br />
<sub>Silver | BI </sub>
</td>
<td align="center">
<a href="https://github.com/laura27241">
<img src="https://github.com/laura27241.png" width="100px;" alt="Maria Laura"/><br />
<sub><b>Maria Laura</b></sub>
</a><br />
<sub>Gold | Docs</sub>
</td>
<td align="center">
<a href="https://github.com/amandadimas">
<img src="https://github.com/amandadimas.png" width="100px;" alt="Amanda Dimas"/><br />
<sub><b>Amanda Dimas</b></sub>
</a><br />
<sub>Gold | SQL | Docs</sub>
</td>
</tr>
</table>

---

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob a **MIT License** - veja o arquivo [LICENSE](LICENSE) para detalhes.

---


