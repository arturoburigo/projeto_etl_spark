# ğŸš€ Projeto ETL com Apache Spark & Azure Data Lake

<div align="center">

![Python](https://img.shields.io/badge/Python-3.10+-blue?style=for-the-badge&logo=python&logoColor=white)
![Apache Spark](https://img.shields.io/badge/Apache%20Spark-3.x-orange?style=for-the-badge&logo=apache-spark&logoColor=white)
![Apache Airflow](https://img.shields.io/badge/Apache%20Airflow-2.x-017CEE?style=for-the-badge&logo=apache-airflow&logoColor=white)
![Azure](https://img.shields.io/badge/Microsoft%20Azure-0078D4?style=for-the-badge&logo=microsoft-azure&logoColor=white)
![Docker](https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white)
![Delta Lake](https://img.shields.io/badge/Delta%20Lake-00ADD8?style=for-the-badge&logo=delta&logoColor=white)

**Pipeline ETL Moderno para Processamento de Dados em Larga Escala**

[ğŸ“– DocumentaÃ§Ã£o Completa](https://seu-dominio.github.io/projeto_etl_spark/) â€¢ [ğŸš€ InÃ­cio RÃ¡pido](#-inÃ­cio-rÃ¡pido) â€¢ [ğŸ—ï¸ Arquitetura](#ï¸-arquitetura)

</div>

---

## ğŸ“‹ Ãndice

- [ğŸ“– Sobre o Projeto](#-sobre-o-projeto)
- [ğŸ¯ Objetivos](#-objetivos)
- [ğŸ—ï¸ Arquitetura](#ï¸-arquitetura)
- [âš¡ Funcionalidades](#-funcionalidades)
- [ğŸ› ï¸ Tecnologias](#ï¸-tecnologias)
- [ğŸš€ InÃ­cio RÃ¡pido](#-inÃ­cio-rÃ¡pido)
- [ğŸ“ Estrutura do Projeto](#-estrutura-do-projeto)
- [ğŸ”§ ConfiguraÃ§Ã£o](#-configuraÃ§Ã£o)
- [ğŸ“Š Pipeline de Dados](#-pipeline-de-dados)
- [ğŸ§ª Testes](#-testes)
- [ğŸ“ˆ Monitoramento](#-monitoramento)
- [ğŸ¤ ContribuiÃ§Ã£o](#-contribuiÃ§Ã£o)
- [ğŸ‘¥ Equipe](#-equipe)
- [ğŸ“„ LicenÃ§a](#-licenÃ§a)

---

## ğŸ“– Sobre o Projeto

Este projeto implementa um **pipeline ETL moderno e escalÃ¡vel** que extrai dados de um banco SQL Server, processa e transforma os dados usando Apache Spark, e armazena no Azure Data Lake seguindo a arquitetura **Medallion (Bronze, Silver, Gold)**. Todo o processo Ã© orquestrado pelo Apache Airflow com containerizaÃ§Ã£o Docker.

### ğŸ¯ Contexto de NegÃ³cio

O projeto simula um sistema de **logÃ­stica e transporte**, processando dados de:
- ğŸ‘¥ Clientes e motoristas
- ğŸš› VeÃ­culos e frotas
- ğŸ“¦ Entregas e coletas
- ğŸ›£ï¸ Rotas e trajetos
- ğŸ”§ ManutenÃ§Ãµes e abastecimentos
- ğŸš¨ Multas e infraÃ§Ãµes

---

## ğŸ¯ Objetivos

- âœ… **Extrair** dados do SQL Server de forma eficiente
- âœ… **Armazenar** dados no Azure Data Lake com organizaÃ§Ã£o em camadas
- âœ… **Processar** dados com Apache Spark usando Delta Lake
- âœ… **Transformar** dados seguindo melhores prÃ¡ticas de qualidade
- âœ… **Automatizar** todo o pipeline com Apache Airflow
- âœ… **Monitorar** execuÃ§Ãµes e performance
- âœ… **Implementar** modelo dimensional para analytics

---

## ğŸ—ï¸ Arquitetura

```mermaid
graph TB
    subgraph "Fonte de Dados"
        SQL[SQL Server<br/>Base Transacional]
    end
    
    subgraph "OrquestraÃ§Ã£o"
        AF[Apache Airflow<br/>Scheduler & DAGs]
    end
    
    subgraph "Azure Data Lake - Medallion Architecture"
        LZ[Landing Zone<br/>Dados Brutos CSV]
        BR[Bronze Layer<br/>Delta Tables]
        SV[Silver Layer<br/>Dados Limpos]
        GD[Gold Layer<br/>Modelo Dimensional]
    end
    
    subgraph "Processamento"
        SP[Apache Spark<br/>Distributed Processing]
        DL[Delta Lake<br/>ACID Transactions]
    end
    
    subgraph "Analytics"
        BI[Power BI / Tableau<br/>Dashboards]
        KPI[KPIs & MÃ©tricas<br/>Business Intelligence]
    end
    
    SQL -->|Extract| AF
    AF -->|Orchestrate| LZ
    LZ -->|Spark ETL| BR
    BR -->|Transform| SV
    SV -->|Aggregate| GD
    SP -.->|Process| BR
    SP -.->|Process| SV
    SP -.->|Process| GD
    DL -.->|ACID| BR
    DL -.->|ACID| SV
    DL -.->|ACID| GD
    GD -->|Consume| BI
    GD -->|Metrics| KPI
```

---

## âš¡ Funcionalidades

### ğŸ”„ Pipeline ETL Completo
- **ExtraÃ§Ã£o incremental** do SQL Server
- **Processamento distribuÃ­do** com Spark
- **TransformaÃ§Ãµes de qualidade** de dados
- **Modelo dimensional** para analytics

### ğŸ“Š Camadas de Dados (Medallion)
- **ğŸ¥‰ Bronze**: Dados brutos em formato Delta
- **ğŸ¥ˆ Silver**: Dados limpos e padronizados
- **ğŸ¥‡ Gold**: Modelo dimensional e KPIs

### ğŸ›ï¸ OrquestraÃ§Ã£o AvanÃ§ada
- **DAGs parametrizÃ¡veis** no Airflow
- **Retry automÃ¡tico** em caso de falhas
- **NotificaÃ§Ãµes** de status
- **Monitoramento** em tempo real

### ğŸ“ˆ Analytics e KPIs
- **Percentual de entregas no prazo**
- **Custo mÃ©dio de frete por rota**
- **Total de entregas por tipo de veÃ­culo**
- **Valor total de frete por cliente**
- **MÃ©tricas mensais** de performance

---

## ğŸ› ï¸ Tecnologias

<table>
<tr>
<td align="center"><strong>Processamento</strong></td>
<td align="center"><strong>OrquestraÃ§Ã£o</strong></td>
<td align="center"><strong>Armazenamento</strong></td>
<td align="center"><strong>Infraestrutura</strong></td>
</tr>
<tr>
<td align="center">
<img src="https://spark.apache.org/images/spark-logo-trademark.png" width="60"/><br/>
Apache Spark 3.x
</td>
<td align="center">
<img src="https://airflow.apache.org/docs/apache-airflow/stable/_images/pin_large.png" width="60"/><br/>
Apache Airflow 2.x
</td>
<td align="center">
<img src="https://upload.wikimedia.org/wikipedia/commons/f/fa/Microsoft_Azure.svg" width="60"/><br/>
Azure Data Lake
</td>
<td align="center">
<img src="https://www.docker.com/wp-content/uploads/2022/03/vertical-logo-monochromatic.png" width="60"/><br/>
Docker & Compose
</td>
</tr>
<tr>
<td align="center">
<img src="https://delta.io/static/images/delta-lake-logo.png" width="60"/><br/>
Delta Lake
</td>
<td align="center">
<img src="https://www.python.org/static/community_logos/python-logo-master-v3-TM.png" width="60"/><br/>
Python 3.10+
</td>
<td align="center">
<img src="https://upload.wikimedia.org/wikipedia/commons/2/29/Postgresql_elephant.svg" width="60"/><br/>
SQL Server
</td>
<td align="center">
<img src="https://python-poetry.org/images/logo-origami.svg" width="60"/><br/>
Poetry
</td>
</tr>
</table>

---

## ğŸš€ InÃ­cio RÃ¡pido

### ğŸ“‹ PrÃ©-requisitos

Certifique-se de ter instalado:

- ğŸ [Python 3.10+](https://www.python.org/downloads/)
- ğŸ³ [Docker & Docker Compose](https://www.docker.com/)
- â˜ï¸ [Azure CLI](https://learn.microsoft.com/pt-br/cli/azure/install-azure-cli)
- ğŸ“¦ [Poetry](https://python-poetry.org/docs/#installation)

### âš¡ InstalaÃ§Ã£o em 3 Passos

```bash
# 1. Clone o repositÃ³rio
git clone https://github.com/seu-usuario/projeto_etl_spark.git
cd projeto_etl_spark

# 2. Instale as dependÃªncias
poetry install

# 3. Configure as variÃ¡veis de ambiente
cp .env.example .env
# Edite o arquivo .env com suas credenciais
```

### ğŸ”§ ConfiguraÃ§Ã£o RÃ¡pida

1. **Configure o Azure**:
```bash
az login
# Configure suas credenciais no arquivo .env
```

2. **Inicie o Airflow**:
```bash
cd astro
astro dev start
```

3. **Acesse a interface**:
- ğŸŒ Airflow UI: [http://localhost:8080](http://localhost:8080)
- ğŸ‘¤ UsuÃ¡rio: `admin` / Senha: `admin`

4. **Execute o pipeline**:
- Navegue atÃ© a DAG `sqlserver_to_bronze_adls`
- Clique em "Trigger DAG"

---

## ğŸ“ Estrutura do Projeto

```
projeto_etl_spark/
â”œâ”€â”€ ğŸ“ astro/                    # Ambiente Apache Airflow
â”‚   â”œâ”€â”€ ğŸ“ dags/                 # DAGs de orquestraÃ§Ã£o
â”‚   â”‚   â””â”€â”€ ğŸ“„ main.py           # Pipeline principal ETL
â”‚   â”œâ”€â”€ ğŸ“ tests/                # Testes automatizados
â”‚   â”œâ”€â”€ ğŸ“ include/              # Arquivos auxiliares
â”‚   â”œâ”€â”€ ğŸ“„ Dockerfile            # Imagem customizada Airflow
â”‚   â””â”€â”€ ğŸ“„ requirements.txt      # DependÃªncias Python
â”‚
â”œâ”€â”€ ğŸ“ data/                     # Scripts de processamento
â”‚   â”œâ”€â”€ ğŸ“ bronze/               # Camada Bronze (dados brutos)
â”‚   â”œâ”€â”€ ğŸ“ silver/               # Camada Silver (dados limpos)
â”‚   â”œâ”€â”€ ğŸ“ gold/                 # Camada Gold (modelo dimensional)
â”‚   â”œâ”€â”€ ğŸ“ landingzone/          # Scripts de extraÃ§Ã£o
â”‚   â”œâ”€â”€ ğŸ“ sql/                  # Scripts SQL
â”‚   â”œâ”€â”€ ğŸ“„ faker_data.py         # Gerador de dados sintÃ©ticos
â”‚   â””â”€â”€ ğŸ“„ create_tables.py      # CriaÃ§Ã£o de tabelas
â”‚
â”œâ”€â”€ ğŸ“ docs/                     # DocumentaÃ§Ã£o MkDocs
â”‚   â”œâ”€â”€ ğŸ“ assets/               # Imagens e diagramas
â”‚   â”œâ”€â”€ ğŸ“„ index.md              # PÃ¡gina inicial
â”‚   â”œâ”€â”€ ğŸ“„ instalacao.md         # Guia de instalaÃ§Ã£o
â”‚   â”œâ”€â”€ ğŸ“„ pipeline_etl.md       # DocumentaÃ§Ã£o do pipeline
â”‚   â””â”€â”€ ğŸ“„ arquitetura.md        # DocumentaÃ§Ã£o da arquitetura
â”‚
â”œâ”€â”€ ğŸ“ iac/                      # Infrastructure as Code
â”‚   â”œâ”€â”€ ğŸ“„ main.tf               # Recursos Azure (Terraform)
â”‚   â”œâ”€â”€ ğŸ“„ variables.tf          # VariÃ¡veis Terraform
â”‚   â””â”€â”€ ğŸ“„ provider.tf           # Providers Terraform
â”‚
â”œâ”€â”€ ğŸ“„ pyproject.toml            # ConfiguraÃ§Ã£o Poetry
â”œâ”€â”€ ğŸ“„ mkdocs.yml                # ConfiguraÃ§Ã£o MkDocs
â”œâ”€â”€ ğŸ“„ README.md                 # Este arquivo
â””â”€â”€ ğŸ“„ .env.example              # Exemplo de variÃ¡veis de ambiente
```

---

## ğŸ”§ ConfiguraÃ§Ã£o

### ğŸ” VariÃ¡veis de Ambiente

Crie um arquivo `.env` baseado no `.env.example`:

```bash
# Azure Data Lake
ADLS_ACCOUNT_NAME=seu_storage_account
ADLS_FILE_SYSTEM_NAME=landing
ADLS_BRONZE_CONTAINER_NAME=bronze
ADLS_SILVER_CONTAINER_NAME=silver
ADLS_GOLD_CONTAINER_NAME=gold
ADLS_SAS_TOKEN=seu_sas_token

# SQL Server
SQL_SERVER=seu_servidor.database.windows.net
SQL_DATABASE=seu_database
SQL_SCHEMA=dbo
SQL_USERNAME=seu_usuario
SQL_PASSWORD=sua_senha

# Spark Configuration
SPARK_DRIVER_MEMORY=4g
SPARK_EXECUTOR_MEMORY=4g
SPARK_EXECUTOR_CORES=2
```

### âš™ï¸ ConfiguraÃ§Ã£o do Spark

O projeto inclui configuraÃ§Ãµes otimizadas do Spark:

```python
spark = SparkSession.builder \
    .appName("projeto_etl_spark") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .config("spark.jars.packages", "io.delta:delta-core_2.12:2.3.0") \
    .config("spark.driver.memory", "4g") \
    .config("spark.executor.memory", "4g") \
    .config("spark.sql.adaptive.enabled", "true") \
    .getOrCreate()
```

---

## ğŸ“Š Pipeline de Dados

### ğŸ”„ Fluxo de ExecuÃ§Ã£o

1. **ğŸ” Landing Zone**: ExtraÃ§Ã£o dos dados do SQL Server para CSV
2. **ğŸ¥‰ Bronze Layer**: IngestÃ£o dos CSVs em formato Delta
3. **ğŸ¥ˆ Silver Layer**: Limpeza, padronizaÃ§Ã£o e qualidade dos dados
4. **ğŸ¥‡ Gold Layer**: Modelo dimensional e cÃ¡lculo de KPIs

### ğŸ“ˆ KPIs Calculados

| KPI | DescriÃ§Ã£o | FrequÃªncia |
|-----|-----------|------------|
| **On-Time Delivery** | % de entregas realizadas no prazo | DiÃ¡rio |
| **Custo por Rota** | Custo mÃ©dio de frete por rota | Semanal |
| **UtilizaÃ§Ã£o da Frota** | Total de entregas por tipo de veÃ­culo | Mensal |
| **Revenue por Cliente** | Valor total de frete por cliente | Mensal |

### ğŸ¯ Modelo Dimensional

```
Fato_Entregas
â”œâ”€â”€ Dim_Data (Tempo)
â”œâ”€â”€ Dim_Cliente (Remetente/DestinatÃ¡rio)
â”œâ”€â”€ Dim_Motorista
â”œâ”€â”€ Dim_Veiculo
â”œâ”€â”€ Dim_Rota
â””â”€â”€ Dim_Tipo_Carga
```

---

## ğŸ§ª Testes

Execute os testes automatizados:

```bash
# Testes unitÃ¡rios
poetry run pytest astro/tests/

# Testes de integraÃ§Ã£o
poetry run pytest astro/tests/test_dag_example.py

# Teste de conexÃ£o SQL Server
poetry run python astro/tests/test_sqlserver_connection.py
```

### ğŸ“Š Cobertura de Testes

- âœ… Testes de DAGs
- âœ… Testes de conexÃ£o com SQL Server
- âœ… Testes de transformaÃ§Ãµes Spark
- âœ… Testes de qualidade de dados

---

## ğŸ“ˆ Monitoramento

### ğŸ›ï¸ Airflow UI

- **Dashboard**: VisÃ£o geral das execuÃ§Ãµes
- **Logs detalhados**: Para debug e troubleshooting
- **Alertas**: NotificaÃ§Ãµes em caso de falhas
- **MÃ©tricas**: Performance e SLA

### ğŸ“Š MÃ©tricas de Performance

- **Tempo de execuÃ§Ã£o** por task
- **Volume de dados** processados
- **Taxa de sucesso** das execuÃ§Ãµes
- **UtilizaÃ§Ã£o de recursos** (CPU/MemÃ³ria)

---

## ğŸ¤ ContribuiÃ§Ã£o

ContribuiÃ§Ãµes sÃ£o sempre bem-vindas! Siga estes passos:

1. **Fork** o projeto
2. **Crie** uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. **Commit** suas mudanÃ§as (`git commit -m 'Add some AmazingFeature'`)
4. **Push** para a branch (`git push origin feature/AmazingFeature`)
5. **Abra** um Pull Request

### ğŸ“ PadrÃµes de CÃ³digo

- Use **Black** para formataÃ§Ã£o
- Siga **PEP 8**
- Adicione **docstrings** nas funÃ§Ãµes
- Mantenha **testes atualizados**

---

## ğŸ‘¥ Equipe

<table>
<tr>
<td align="center">
<a href="https://github.com/arturoburigo">
<img src="https://github.com/arturoburigo.png" width="100px;" alt="Arturo Burigo"/><br />
<sub><b>Arturo Burigo</b></sub>
</a><br />
<sub>Tech Lead & Architecture</sub>
</td>
<td align="center">
<a href="https://github.com/bezerraluiz">
<img src="https://github.com/bezerraluiz.png" width="100px;" alt="Luiz Bezerra"/><br />
<sub><b>Luiz Bezerra</b></sub>
</a><br />
<sub>Data Engineer</sub>
</td>
<td align="center">
<a href="https://github.com/M0rona">
<img src="https://github.com/M0rona.png" width="100px;" alt="Gabriel Morona"/><br />
<sub><b>Gabriel Morona</b></sub>
</a><br />
<sub>Spark Developer</sub>
</td>
<td align="center">
<a href="https://github.com/laura27241">
<img src="https://github.com/laura27241.png" width="100px;" alt="Maria Laura"/><br />
<sub><b>Maria Laura</b></sub>
</a><br />
<sub>Data Analyst</sub>
</td>
<td align="center">
<a href="https://github.com/amandadimas">
<img src="https://github.com/amandadimas.png" width="100px;" alt="Amanda Dimas"/><br />
<sub><b>Amanda Dimas</b></sub>
</a><br />
<sub>QA Engineer</sub>
</td>
</tr>
</table>

---

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob a **MIT License** - veja o arquivo [LICENSE](LICENSE) para detalhes.

---

<div align="center">

**ğŸŒŸ Se este projeto foi Ãºtil, considere dar uma estrela!**

[![GitHub stars](https://img.shields.io/github/stars/seu-usuario/projeto_etl_spark?style=social)](https://github.com/seu-usuario/projeto_etl_spark/stargazers)
[![GitHub forks](https://img.shields.io/github/forks/seu-usuario/projeto_etl_spark?style=social)](https://github.com/seu-usuario/projeto_etl_spark/network)

**Feito com â¤ï¸ pela equipe de Data Engineering**

</div>
