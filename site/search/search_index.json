{"config":{"lang":["pt"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83d\ude80 Projeto ETL com Apache Spark &amp; Azure Data Lake","text":"<ul> <li> <p> Pipeline ETL Moderno</p> <p>Pipeline escal\u00e1vel para processamento de dados em larga escala usando Apache Spark, orquestrado por Apache Airflow e armazenado no Azure Data Lake.</p> <p> In\u00edcio R\u00e1pido</p> </li> <li> <p> Arquitetura Medallion</p> <p>Implementa\u00e7\u00e3o da arquitetura Medallion (Bronze, Silver, Gold) para organiza\u00e7\u00e3o e processamento de dados com qualidade empresarial.</p> <p> Ver Arquitetura</p> </li> <li> <p> Analytics &amp; KPIs</p> <p>Modelo dimensional com KPIs de neg\u00f3cio para an\u00e1lise de performance log\u00edstica e m\u00e9tricas operacionais.</p> <p> Ver KPIs</p> </li> <li> <p> Orquestra\u00e7\u00e3o Avan\u00e7ada</p> <p>DAGs parametriz\u00e1veis no Airflow com monitoramento, retry autom\u00e1tico e notifica\u00e7\u00f5es de status.</p> <p> Ver Airflow</p> </li> </ul>"},{"location":"#sobre-o-projeto","title":"\ud83c\udfaf Sobre o Projeto","text":"<p>Este projeto implementa um pipeline ETL completo e moderno que demonstra as melhores pr\u00e1ticas para processamento de dados em larga escala. O sistema processa dados de um contexto de log\u00edstica e transporte, incluindo informa\u00e7\u00f5es sobre clientes, motoristas, ve\u00edculos, entregas, rotas e manuten\u00e7\u00f5es.</p>"},{"location":"#principais-caracteristicas","title":"\u2728 Principais Caracter\u00edsticas","text":"<ul> <li>\ud83d\udd04 ETL Completo: Extra\u00e7\u00e3o do SQL Server, transforma\u00e7\u00e3o com Spark, carregamento no Data Lake</li> <li>\ud83c\udfd7\ufe0f Arquitetura Medallion: Organiza\u00e7\u00e3o em camadas Bronze, Silver e Gold</li> <li>\u26a1 Processamento Distribu\u00eddo: Apache Spark com Delta Lake para ACID transactions</li> <li>\ud83c\udf9b\ufe0f Orquestra\u00e7\u00e3o: Apache Airflow para automa\u00e7\u00e3o e monitoramento</li> <li>\u2601\ufe0f Cloud Native: Integra\u00e7\u00e3o completa com Azure Data Lake Storage</li> <li>\ud83d\udcca Analytics Ready: Modelo dimensional para Business Intelligence</li> </ul>"},{"location":"#stack-tecnologico","title":"\ud83d\udee0\ufe0f Stack Tecnol\u00f3gico","text":"<ul> <li> <p> Python 3.10+</p> <p>Linguagem principal para desenvolvimento do pipeline, com bibliotecas especializadas em processamento de dados.</p> </li> <li> <p> Apache Spark 3.x</p> <p>Engine de processamento distribu\u00eddo para transforma\u00e7\u00f5es de dados em larga escala com Delta Lake.</p> </li> <li> <p> Apache Airflow 2.x</p> <p>Plataforma de orquestra\u00e7\u00e3o para automa\u00e7\u00e3o, agendamento e monitoramento do pipeline.</p> </li> <li> <p>:simple-microsoftazure:{ .lg .middle } Azure Data Lake</p> <p>Armazenamento escal\u00e1vel e seguro para dados estruturados e semi-estruturados.</p> </li> <li> <p> Docker &amp; Compose</p> <p>Containeriza\u00e7\u00e3o para ambiente consistente e deploy simplificado.</p> </li> <li> <p> Terraform</p> <p>Infrastructure as Code para provisionamento automatizado de recursos Azure.</p> </li> </ul>"},{"location":"#modelo-de-dados","title":"\ud83d\udcca Modelo de Dados","text":"<p>O projeto processa dados de um sistema de log\u00edstica e transporte com as seguintes entidades principais:</p> <pre><code>erDiagram\n    CLIENTES ||--o{ ENTREGAS : \"remetente/destinatario\"\n    MOTORISTAS ||--o{ ENTREGAS : \"responsavel\"\n    VEICULOS ||--o{ ENTREGAS : \"transporta\"\n    VEICULOS ||--o{ MANUTENCOES : \"sofre\"\n    VEICULOS ||--o{ ABASTECIMENTOS : \"consome\"\n    VEICULOS ||--o{ MULTAS : \"recebe\"\n    ROTAS ||--o{ ENTREGAS : \"utiliza\"\n    TIPOS_CARGA ||--o{ ENTREGAS : \"categoria\"\n    ENTREGAS ||--o{ COLETAS : \"inclui\"\n\n    CLIENTES {\n        int id_cliente\n        string nome_cliente\n        string tipo_cliente\n        string cpf_cnpj\n        string email\n        string telefone\n        string endereco\n    }\n\n    ENTREGAS {\n        int id_entrega\n        int id_cliente_remetente\n        int id_cliente_destinatario\n        int id_motorista\n        int id_veiculo\n        int id_rota\n        date data_inicio\n        date data_previsao\n        string status_entrega\n        decimal valor_frete\n    }\n\n    VEICULOS {\n        int id_veiculo\n        string placa\n        string modelo\n        string marca\n        int ano_fabricacao\n        decimal capacidade_kg\n        string tipo_veiculo\n    }</code></pre>"},{"location":"#inicio-rapido","title":"\ud83d\ude80 In\u00edcio R\u00e1pido","text":""},{"location":"#1-pre-requisitos","title":"1. Pr\u00e9-requisitos","text":"<p>Certifique-se de ter instalado:</p> <ul> <li> Python 3.10+</li> <li> Docker &amp; Docker Compose</li> <li> Azure CLI</li> <li> Poetry (gerenciador de depend\u00eancias)</li> </ul>"},{"location":"#2-instalacao","title":"2. Instala\u00e7\u00e3o","text":"<pre><code># Clone o reposit\u00f3rio\ngit clone https://github.com/seu-usuario/projeto_etl_spark.git\ncd projeto_etl_spark\n\n# Instale as depend\u00eancias\npoetry install\n\n# Configure as vari\u00e1veis de ambiente\ncp .env.example .env\n# Edite o arquivo .env com suas credenciais Azure\n</code></pre>"},{"location":"#3-execucao","title":"3. Execu\u00e7\u00e3o","text":"<pre><code># Inicie o Airflow\ncd astro\nastro dev start\n\n# Acesse a interface web\n# http://localhost:8080 (admin/admin)\n</code></pre>"},{"location":"#4-execute-o-pipeline","title":"4. Execute o Pipeline","text":"<ol> <li>Navegue at\u00e9 a DAG <code>sqlserver_to_bronze_adls</code></li> <li>Clique em \"Trigger DAG\"</li> <li>Monitore a execu\u00e7\u00e3o na interface do Airflow</li> </ol> <p>Dica</p> <p>Para uma configura\u00e7\u00e3o mais detalhada, consulte o Guia de Instala\u00e7\u00e3o completo.</p>"},{"location":"#pipeline-de-dados","title":"\ud83d\udcc8 Pipeline de Dados","text":""},{"location":"#fluxo-de-execucao","title":"\ud83d\udd04 Fluxo de Execu\u00e7\u00e3o","text":"<pre><code>graph LR\n    A[SQL Server] --&gt; B[Landing Zone]\n    B --&gt; C[Bronze Layer]\n    C --&gt; D[Silver Layer]\n    D --&gt; E[Gold Layer]\n    E --&gt; F[Analytics &amp; BI]\n\n    subgraph \"Processamento\"\n        C -.-&gt; G[Spark ETL]\n        D -.-&gt; G\n        E -.-&gt; G\n    end\n\n    subgraph \"Orquestra\u00e7\u00e3o\"\n        H[Airflow DAGs]\n        H -.-&gt; B\n        H -.-&gt; C\n        H -.-&gt; D\n        H -.-&gt; E\n    end</code></pre>"},{"location":"#camadas-de-dados","title":"\ud83d\udcca Camadas de Dados","text":"Camada Descri\u00e7\u00e3o Formato Finalidade \ud83d\udd0d Landing Dados brutos extra\u00eddos CSV Staging inicial \ud83e\udd49 Bronze Dados hist\u00f3ricos completos Delta Data Lake \ud83e\udd48 Silver Dados limpos e padronizados Delta Analytics \ud83e\udd47 Gold Modelo dimensional Delta Business Intelligence"},{"location":"#kpis-e-metricas","title":"\ud83c\udfaf KPIs e M\u00e9tricas","text":"<p>O projeto calcula automaticamente os seguintes indicadores de performance:</p> <ul> <li> <p> On-Time Delivery</p> <p>Percentual de entregas realizadas dentro do prazo estabelecido.</p> <p>Meta: &gt; 95%</p> </li> <li> <p> Custo por Rota</p> <p>Custo m\u00e9dio de frete por quil\u00f4metro em cada rota.</p> <p>An\u00e1lise: Semanal</p> </li> <li> <p> Utiliza\u00e7\u00e3o da Frota</p> <p>Total de entregas por tipo de ve\u00edculo e taxa de ocupa\u00e7\u00e3o.</p> <p>Frequ\u00eancia: Mensal</p> </li> <li> <p> Revenue por Cliente</p> <p>Valor total de frete gerado por cada cliente.</p> <p>Segmenta\u00e7\u00e3o: Por regi\u00e3o</p> </li> </ul>"},{"location":"#proximos-passos","title":"\ud83d\udcda Pr\u00f3ximos Passos","text":"<ul> <li> <p> \ud83d\udcd6 Documenta\u00e7\u00e3o Completa</p> <p>Guias detalhados de instala\u00e7\u00e3o, configura\u00e7\u00e3o e uso do sistema.</p> </li> <li> <p>:material-architecture:{ .lg .middle } \ud83c\udfd7\ufe0f Arquitetura</p> <p>Vis\u00e3o detalhada da arquitetura do sistema e decis\u00f5es de design.</p> </li> <li> <p> \ud83d\udd27 Pipeline ETL</p> <p>Documenta\u00e7\u00e3o t\u00e9cnica do pipeline de dados e transforma\u00e7\u00f5es.</p> </li> <li> <p> \ud83e\uddea Testes</p> <p>Estrat\u00e9gia de testes, cobertura e como executar os testes.</p> </li> </ul>"},{"location":"#contribuicao","title":"\ud83e\udd1d Contribui\u00e7\u00e3o","text":"<p>Contribui\u00e7\u00f5es s\u00e3o sempre bem-vindas! Este projeto segue as melhores pr\u00e1ticas de desenvolvimento colaborativo:</p> <ul> <li>Code Review: Todos os PRs passam por revis\u00e3o</li> <li>Testes Automatizados: Cobertura de testes &gt; 80%</li> <li>Documenta\u00e7\u00e3o: Toda funcionalidade deve ser documentada</li> <li>Padr\u00f5es: Seguimos PEP 8 e usamos Black para formata\u00e7\u00e3o</li> </ul> <p>Como Contribuir \u2192</p>"},{"location":"#equipe","title":"\ud83d\udc65 Equipe","text":"<p>Este projeto foi desenvolvido por uma equipe multidisciplinar de especialistas em dados:</p> <ul> <li>Arturo Burigo - Tech Lead &amp; Architecture</li> <li>Luiz Bezerra - Data Engineer</li> <li>Gabriel Morona - Spark Developer  </li> <li>Maria Laura - Data Analyst</li> <li>Amanda Dimas - QA Engineer</li> </ul> <p>Licen\u00e7a</p> <p>Este projeto est\u00e1 licenciado sob a MIT License. Veja o arquivo LICENSE para detalhes.</p>"},{"location":"airflow/","title":"Airflow e Orquestra\u00e7\u00e3o","text":""},{"location":"airflow/#dag-principal-sqlserver_to_adls_dagpy","title":"DAG principal: <code>sqlserver_to_adls_dag.py</code>","text":"<ul> <li>Extrai dados do SQL Server</li> <li>Move dados para o Azure Data Lake</li> <li>Pode ser agendado ou executado manualmente</li> </ul>"},{"location":"airflow/#testes","title":"Testes","text":"<ul> <li>Localizados em <code>astro/tests/</code></li> </ul>"},{"location":"arquitetura/","title":"\ud83c\udfd7\ufe0f Arquitetura do Sistema","text":""},{"location":"arquitetura/#visao-geral","title":"\ud83d\udccb Vis\u00e3o Geral","text":"<p>O projeto implementa uma arquitetura moderna de dados baseada na metodologia Medallion Architecture (Bronze, Silver, Gold), utilizando as melhores pr\u00e1ticas para processamento de dados em larga escala. A solu\u00e7\u00e3o \u00e9 cloud-native, escal\u00e1vel e resiliente.</p>"},{"location":"arquitetura/#principios-arquiteturais","title":"\ud83c\udfaf Princ\u00edpios Arquiteturais","text":""},{"location":"arquitetura/#separation-of-concerns","title":"\ud83d\udd27 Separation of Concerns","text":"<p>Cada camada tem responsabilidades bem definidas, facilitando manuten\u00e7\u00e3o e evolu\u00e7\u00e3o.</p>"},{"location":"arquitetura/#scalability-first","title":"\u26a1 Scalability First","text":"<p>Arquitetura preparada para crescimento horizontal e vertical conforme demanda.</p>"},{"location":"arquitetura/#fault-tolerance","title":"\ud83d\udee1\ufe0f Fault Tolerance","text":"<p>Mecanismos de retry, checkpointing e recupera\u00e7\u00e3o autom\u00e1tica.</p>"},{"location":"arquitetura/#data-quality","title":"\ud83d\udcca Data Quality","text":"<p>Valida\u00e7\u00f5es e transforma\u00e7\u00f5es em cada camada para garantir qualidade dos dados.</p>"},{"location":"arquitetura/#security-by-design","title":"\ud83d\udd12 Security by Design","text":"<p>Criptografia, controle de acesso e auditoria em todos os componentes.</p>"},{"location":"arquitetura/#arquitetura-geral","title":"\ud83c\udfdb\ufe0f Arquitetura Geral","text":"<pre><code>graph TB\n    subgraph \"Data Sources\"\n        SQL[SQL Server&lt;br/&gt;Transactional Database]\n    end\n\n    subgraph \"Orchestration Layer\"\n        AF[Apache Airflow&lt;br/&gt;Workflow Management]\n        AF_DB[(Airflow&lt;br/&gt;Metadata DB)]\n        AF --&gt; AF_DB\n    end\n\n    subgraph \"Azure Data Lake - Medallion Architecture\"\n        LZ[Landing Zone&lt;br/&gt;Raw CSV Files]\n\n        subgraph \"Bronze Layer\"\n            BR_DT[Delta Tables&lt;br/&gt;Raw Data]\n            BR_META[Metadata&lt;br/&gt;Lineage]\n        end\n\n        subgraph \"Silver Layer\"\n            SV_DT[Delta Tables&lt;br/&gt;Clean Data]\n            SV_QUAL[Data Quality&lt;br/&gt;Validations]\n        end\n\n        subgraph \"Gold Layer\"\n            GD_DIM[Dimension Tables&lt;br/&gt;Master Data]\n            GD_FACT[Fact Tables&lt;br/&gt;Transactional Data]\n            GD_KPI[KPI Tables&lt;br/&gt;Aggregated Metrics]\n        end\n    end\n\n    subgraph \"Processing Engine\"\n        SPARK[Apache Spark&lt;br/&gt;Distributed Processing]\n        DELTA[Delta Lake&lt;br/&gt;ACID Transactions]\n        SPARK --&gt; DELTA\n    end\n\n    subgraph \"Infrastructure\"\n        DOCKER[Docker Containers&lt;br/&gt;Airflow Environment]\n        AZURE[Azure Cloud&lt;br/&gt;Storage &amp; Compute]\n    end\n\n    subgraph \"Analytics &amp; Consumption\"\n        BI[Power BI&lt;br/&gt;Dashboards]\n        API[REST API&lt;br/&gt;Data Access]\n        EXPORT[Data Export&lt;br/&gt;CSV/Parquet]\n    end\n\n    SQL --&gt;|Extract| AF\n    AF --&gt;|Orchestrate| LZ\n    LZ --&gt;|Ingest| BR_DT\n    BR_DT --&gt;|Transform| SV_DT\n    SV_DT --&gt;|Aggregate| GD_DIM\n    SV_DT --&gt;|Aggregate| GD_FACT\n    GD_FACT --&gt;|Calculate| GD_KPI\n\n    SPARK -.-&gt;|Process| BR_DT\n    SPARK -.-&gt;|Process| SV_DT\n    SPARK -.-&gt;|Process| GD_DIM\n    SPARK -.-&gt;|Process| GD_FACT\n\n    GD_KPI --&gt;|Consume| BI\n    GD_FACT --&gt;|Query| API\n    GD_DIM --&gt;|Export| EXPORT\n\n    DOCKER -.-&gt;|Host| AF\n    AZURE -.-&gt;|Store| LZ\n    AZURE -.-&gt;|Store| BR_DT\n    AZURE -.-&gt;|Store| SV_DT\n    AZURE -.-&gt;|Store| GD_DIM</code></pre>"},{"location":"arquitetura/#arquitetura-medallion","title":"\ud83d\udcca Arquitetura Medallion","text":""},{"location":"arquitetura/#landing-zone","title":"\ud83d\udd0d Landing Zone","text":"<p>Finalidade: Staging inicial dos dados brutos extra\u00eddos</p> <ul> <li>Formato: CSV files</li> <li>Reten\u00e7\u00e3o: 30 dias</li> <li>Particionamento: Por data de extra\u00e7\u00e3o</li> <li>Schema: Schema-on-read</li> </ul> <pre><code>landing/\n\u251c\u2500\u2500 clientes_20241201_143022.csv\n\u251c\u2500\u2500 motoristas_20241201_143025.csv\n\u251c\u2500\u2500 veiculos_20241201_143028.csv\n\u2514\u2500\u2500 entregas_20241201_143030.csv\n</code></pre>"},{"location":"arquitetura/#bronze-layer-raw-data","title":"\ud83e\udd49 Bronze Layer (Raw Data)","text":"<p>Finalidade: Armazenamento hist\u00f3rico completo e imut\u00e1vel</p> <ul> <li>Formato: Delta Lake tables</li> <li>Schema: Preserva estrutura original + metadados</li> <li>Particionamento: Por ano/m\u00eas/dia</li> <li>Reten\u00e7\u00e3o: Ilimitada (dados hist\u00f3ricos)</li> </ul> <p>Caracter\u00edsticas: - \u2705 ACID transactions via Delta Lake - \u2705 Time travel e versionamento - \u2705 Metadados de ingest\u00e3o - \u2705 Compacta\u00e7\u00e3o autom\u00e1tica</p> <pre><code># Estrutura Bronze\nbronze_schema = {\n    \"original_columns\": \"preservadas_como_extra\u00eddas\",\n    \"processing_date\": \"data_do_processamento\",\n    \"processing_timestamp\": \"timestamp_do_processamento\", \n    \"source_file_name\": \"nome_do_arquivo_origem\",\n    \"_bronze_ingestion_id\": \"id_\u00fanico_da_ingest\u00e3o\"\n}\n</code></pre>"},{"location":"arquitetura/#silver-layer-clean-data","title":"\ud83e\udd48 Silver Layer (Clean Data)","text":"<p>Finalidade: Dados limpos, padronizados e enriquecidos</p> <ul> <li>Formato: Delta Lake tables</li> <li>Schema: Padronizado e normalizado</li> <li>Qualidade: Valida\u00e7\u00f5es e limpezas aplicadas</li> <li>Particionamento: Por data de neg\u00f3cio</li> </ul> <p>Transforma\u00e7\u00f5es Aplicadas: - \ud83e\uddf9 Data Cleansing: Remo\u00e7\u00e3o de duplicatas, valores nulos - \ud83d\udccf Standardization: Padroniza\u00e7\u00e3o de formatos e tipos - \ud83d\udd24 Normalization: Convers\u00e3o para mai\u00fasculas, trim - \u2705 Validation: Regras de neg\u00f3cio e qualidade - \ud83c\udff7\ufe0f Enrichment: Adi\u00e7\u00e3o de metadados Silver</p> <pre><code># Exemplo de transforma\u00e7\u00f5es Silver\ndef silver_transformations(df):\n    return df \\\n        .dropDuplicates() \\\n        .withColumn(\"nome_cliente\", upper(trim(col(\"nome_cliente\")))) \\\n        .withColumn(\"cpf_cnpj\", regexp_replace(col(\"cpf_cnpj\"), \"[^0-9]\", \"\")) \\\n        .withColumn(\"_silver_ingestion_timestamp\", current_timestamp()) \\\n        .withColumn(\"_source_table\", lit(table_name))\n</code></pre>"},{"location":"arquitetura/#gold-layer-business-data","title":"\ud83e\udd47 Gold Layer (Business Data)","text":"<p>Finalidade: Modelo dimensional para analytics e BI</p> <ul> <li>Formato: Delta Lake tables</li> <li>Schema: Star schema / Snowflake</li> <li>Otimiza\u00e7\u00e3o: \u00cdndices e particionamento para queries</li> <li>Agrega\u00e7\u00f5es: KPIs e m\u00e9tricas pr\u00e9-calculadas</li> </ul> <p>Estrutura Dimensional:</p> <pre><code>erDiagram\n    Fato_Entregas ||--|| Dim_Data : \"data_entrega\"\n    Fato_Entregas ||--|| Dim_Cliente_Remetente : \"remetente\"\n    Fato_Entregas ||--|| Dim_Cliente_Destinatario : \"destinatario\"\n    Fato_Entregas ||--|| Dim_Motorista : \"motorista\"\n    Fato_Entregas ||--|| Dim_Veiculo : \"veiculo\"\n    Fato_Entregas ||--|| Dim_Rota : \"rota\"\n    Fato_Entregas ||--|| Dim_Tipo_Carga : \"tipo_carga\"\n\n    Fato_Entregas {\n        bigint id_entrega_key PK\n        int data_inicio_key FK\n        int data_previsao_key FK\n        int data_fim_key FK\n        int cliente_remetente_key FK\n        int cliente_destinatario_key FK\n        int motorista_key FK\n        int veiculo_key FK\n        int rota_key FK\n        int tipo_carga_key FK\n        decimal valor_frete\n        decimal peso_carga_kg\n        string status_entrega\n    }</code></pre>"},{"location":"arquitetura/#componentes-tecnicos","title":"\u2699\ufe0f Componentes T\u00e9cnicos","text":""},{"location":"arquitetura/#apache-airflow","title":"\ud83c\udf9b\ufe0f Apache Airflow","text":"<p>Responsabilidade: Orquestra\u00e7\u00e3o e agendamento</p> <pre><code># Configura\u00e7\u00e3o otimizada\nAIRFLOW_CONFIG = {\n    \"executor\": \"LocalExecutor\",\n    \"max_active_runs\": 1,\n    \"max_active_tasks\": 4,\n    \"catchup\": False,\n    \"depends_on_past\": False,\n    \"retries\": 3,\n    \"retry_delay\": timedelta(minutes=5)\n}\n</code></pre> <p>DAG Principal: <pre><code>@dag(\n    dag_id=\"sqlserver_to_bronze_adls\",\n    schedule=\"0 2 * * *\",  # Daily at 2 AM\n    start_date=datetime(2024, 1, 1),\n    catchup=False,\n    tags=[\"etl\", \"production\"]\n)\ndef etl_pipeline():\n    extract_task = extract_from_sqlserver()\n    bronze_task = process_bronze_layer()\n    silver_task = process_silver_layer()\n    gold_task = process_gold_layer()\n\n    extract_task &gt;&gt; bronze_task &gt;&gt; silver_task &gt;&gt; gold_task\n</code></pre></p>"},{"location":"arquitetura/#apache-spark","title":"\u26a1 Apache Spark","text":"<p>Responsabilidade: Processamento distribu\u00eddo de dados</p> <p>Configura\u00e7\u00f5es Otimizadas: <pre><code>spark_config = {\n    # Core Settings\n    \"spark.app.name\": \"projeto_etl_spark\",\n    \"spark.sql.adaptive.enabled\": \"true\",\n    \"spark.sql.adaptive.coalescePartitions.enabled\": \"true\",\n\n    # Delta Lake\n    \"spark.sql.extensions\": \"io.delta.sql.DeltaSparkSessionExtension\",\n    \"spark.sql.catalog.spark_catalog\": \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n\n    # Memory Management\n    \"spark.driver.memory\": \"4g\",\n    \"spark.executor.memory\": \"4g\",\n    \"spark.driver.maxResultSize\": \"2g\",\n\n    # Performance\n    \"spark.serializer\": \"org.apache.spark.serializer.KryoSerializer\",\n    \"spark.sql.adaptive.skewJoin.enabled\": \"true\",\n\n    # Azure Integration\n    \"spark.hadoop.fs.azure.account.auth.type\": \"SAS\",\n    \"spark.hadoop.fs.azure.sas.token.provider.type\": \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\"\n}\n</code></pre></p>"},{"location":"arquitetura/#delta-lake","title":"\ud83d\udd3a Delta Lake","text":"<p>Responsabilidade: ACID transactions e versionamento</p> <p>Benef\u00edcios: - \u2705 ACID Transactions: Consist\u00eancia em opera\u00e7\u00f5es concorrentes - \u2705 Time Travel: Acesso a vers\u00f5es hist\u00f3ricas - \u2705 Schema Evolution: Mudan\u00e7as de schema sem quebrar compatibilidade - \u2705 Merge Operations: Upserts eficientes - \u2705 Automatic Compaction: Otimiza\u00e7\u00e3o autom\u00e1tica de arquivos</p> <pre><code># Exemplo de merge operation\ndelta_table = DeltaTable.forPath(spark, gold_path)\ndelta_table.alias(\"target\") \\\n    .merge(updates_df.alias(\"source\"), \"target.id = source.id\") \\\n    .whenMatchedUpdateAll() \\\n    .whenNotMatchedInsertAll() \\\n    .execute()\n</code></pre>"},{"location":"arquitetura/#infraestrutura-azure","title":"\u2601\ufe0f Infraestrutura Azure","text":""},{"location":"arquitetura/#azure-data-lake-storage-gen2","title":"\ud83d\uddc4\ufe0f Azure Data Lake Storage Gen2","text":"<p>Configura\u00e7\u00e3o: - Replication: LRS (Locally Redundant Storage) - Access Tier: Hot (dados frequentemente acessados) - Hierarchical Namespace: Habilitado - Security: SAS tokens com permiss\u00f5es granulares</p> <p>Estrutura de Containers: <pre><code>adlsaccount/\n\u251c\u2500\u2500 landing/          # Raw CSV files\n\u251c\u2500\u2500 bronze/           # Delta tables - raw data\n\u251c\u2500\u2500 silver/           # Delta tables - clean data\n\u251c\u2500\u2500 gold/             # Delta tables - dimensional model\n\u2514\u2500\u2500 checkpoints/      # Spark checkpoints\n</code></pre></p>"},{"location":"arquitetura/#seguranca","title":"\ud83d\udd10 Seguran\u00e7a","text":"<ul> <li>Authentication: SAS Tokens com expira\u00e7\u00e3o</li> <li>Authorization: RBAC no Azure AD</li> <li>Encryption: At rest e in transit</li> <li>Network: Private endpoints (quando necess\u00e1rio)</li> </ul>"},{"location":"arquitetura/#fluxo-de-dados-detalhado","title":"\ud83d\udd04 Fluxo de Dados Detalhado","text":""},{"location":"arquitetura/#1-extracao-sql-server-landing-zone","title":"1. Extra\u00e7\u00e3o (SQL Server \u2192 Landing Zone)","text":"<pre><code>def extract_from_sqlserver():\n    # Conex\u00e3o otimizada com pool de conex\u00f5es\n    engine = create_engine(\n        connection_string,\n        pool_size=10,\n        pool_recycle=3600,\n        pool_pre_ping=True\n    )\n\n    # Extra\u00e7\u00e3o incremental baseada em timestamp\n    query = f\"\"\"\n    SELECT * FROM {schema}.{table} \n    WHERE last_modified &gt;= '{last_extraction_time}'\n    \"\"\"\n\n    df = pd.read_sql(query, engine)\n\n    # Upload para ADLS com timestamp\n    filename = f\"{table}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n    upload_to_adls(df, filename)\n</code></pre>"},{"location":"arquitetura/#2-processamento-bronze-landing-bronze","title":"2. Processamento Bronze (Landing \u2192 Bronze)","text":"<pre><code>def process_bronze_layer():\n    # Leitura dos CSVs com schema inference\n    df = spark.read \\\n        .option(\"header\", \"true\") \\\n        .option(\"inferSchema\", \"true\") \\\n        .csv(landing_path)\n\n    # Adi\u00e7\u00e3o de metadados\n    df_bronze = df \\\n        .withColumn(\"processing_date\", current_date()) \\\n        .withColumn(\"processing_timestamp\", current_timestamp()) \\\n        .withColumn(\"source_file_name\", input_file_name())\n\n    # Escrita em Delta format\n    df_bronze.write \\\n        .format(\"delta\") \\\n        .mode(\"append\") \\\n        .option(\"mergeSchema\", \"true\") \\\n        .save(bronze_path)\n</code></pre>"},{"location":"arquitetura/#3-processamento-silver-bronze-silver","title":"3. Processamento Silver (Bronze \u2192 Silver)","text":"<pre><code>def process_silver_layer():\n    # Leitura da camada Bronze\n    df_bronze = spark.read.format(\"delta\").load(bronze_path)\n\n    # Aplica\u00e7\u00e3o de transforma\u00e7\u00f5es de qualidade\n    df_silver = df_bronze \\\n        .dropDuplicates() \\\n        .filter(col(\"id\").isNotNull()) \\\n        .withColumn(\"nome\", upper(trim(col(\"nome\")))) \\\n        .withColumn(\"cpf\", regexp_replace(col(\"cpf\"), \"[^0-9]\", \"\"))\n\n    # Valida\u00e7\u00f5es de neg\u00f3cio\n    df_validated = apply_business_rules(df_silver)\n\n    # Escrita com particionamento\n    df_validated.write \\\n        .format(\"delta\") \\\n        .mode(\"overwrite\") \\\n        .partitionBy(\"ano\", \"mes\") \\\n        .save(silver_path)\n</code></pre>"},{"location":"arquitetura/#4-processamento-gold-silver-gold","title":"4. Processamento Gold (Silver \u2192 Gold)","text":"<pre><code>def process_gold_layer():\n    # Cria\u00e7\u00e3o das dimens\u00f5es\n    create_dimensions()\n\n    # Cria\u00e7\u00e3o das tabelas fato\n    create_fact_tables()\n\n    # C\u00e1lculo de KPIs\n    calculate_kpis()\n</code></pre>"},{"location":"arquitetura/#estrategia-de-particionamento","title":"\ud83d\udcca Estrat\u00e9gia de Particionamento","text":""},{"location":"arquitetura/#bronze-layer","title":"Bronze Layer","text":"<pre><code>bronze/\n\u2514\u2500\u2500 clientes/\n    \u2514\u2500\u2500 year=2024/\n        \u2514\u2500\u2500 month=12/\n            \u2514\u2500\u2500 day=01/\n                \u251c\u2500\u2500 part-00000.parquet\n                \u2514\u2500\u2500 _delta_log/\n</code></pre>"},{"location":"arquitetura/#silver-layer","title":"Silver Layer","text":"<pre><code>silver/\n\u2514\u2500\u2500 clientes/\n    \u2514\u2500\u2500 ano=2024/\n        \u2514\u2500\u2500 mes=12/\n            \u251c\u2500\u2500 part-00000.parquet\n            \u2514\u2500\u2500 _delta_log/\n</code></pre>"},{"location":"arquitetura/#gold-layer","title":"Gold Layer","text":"<pre><code>gold/\n\u251c\u2500\u2500 dim_cliente/\n\u2502   \u2514\u2500\u2500 part-00000.parquet\n\u251c\u2500\u2500 dim_data/\n\u2502   \u2514\u2500\u2500 part-00000.parquet\n\u2514\u2500\u2500 fato_entregas/\n    \u2514\u2500\u2500 data_entrega=2024-12-01/\n        \u2514\u2500\u2500 part-00000.parquet\n</code></pre>"},{"location":"arquitetura/#monitoramento-e-observabilidade","title":"\ud83d\udd0d Monitoramento e Observabilidade","text":""},{"location":"arquitetura/#metricas-coletadas","title":"M\u00e9tricas Coletadas","text":"<ul> <li>\u23f1\ufe0f Performance: Tempo de execu\u00e7\u00e3o por task</li> <li>\ud83d\udcca Volume: Quantidade de registros processados</li> <li>\ud83d\udcbe Storage: Tamanho dos dados por camada</li> <li>\u274c Errors: Taxa de falhas e tipos de erro</li> <li>\ud83d\udd04 Throughput: Registros processados por segundo</li> </ul>"},{"location":"arquitetura/#alertas-configurados","title":"Alertas Configurados","text":"<ul> <li>\ud83d\udea8 Falha de execu\u00e7\u00e3o de DAGs</li> <li>\u23f0 SLA breach (execu\u00e7\u00e3o &gt; 2 horas)</li> <li>\ud83d\udcc8 Volume an\u00f4malo de dados</li> <li>\ud83d\udcbe Espa\u00e7o em disco baixo</li> </ul>"},{"location":"arquitetura/#dashboards","title":"Dashboards","text":"<ul> <li>\ud83d\udcca Airflow UI: Status das execu\u00e7\u00f5es</li> <li>\ud83d\udcc8 Spark UI: Performance das jobs</li> <li>\u2601\ufe0f Azure Monitor: M\u00e9tricas de infraestrutura</li> </ul>"},{"location":"arquitetura/#estrategia-de-backup-e-recovery","title":"\ud83d\udd04 Estrat\u00e9gia de Backup e Recovery","text":""},{"location":"arquitetura/#backup","title":"Backup","text":"<ul> <li>\ud83d\udcc5 Daily: Snapshot autom\u00e1tico das tabelas Gold</li> <li>\ud83d\udcc5 Weekly: Backup completo das camadas Silver e Bronze</li> <li>\ud83d\udcc5 Monthly: Archive para storage de longo prazo</li> </ul>"},{"location":"arquitetura/#recovery","title":"Recovery","text":"<ul> <li>\ud83d\udd04 Time Travel: Delta Lake permite recupera\u00e7\u00e3o point-in-time</li> <li>\ud83d\udccb Replay: Re-execu\u00e7\u00e3o de DAGs a partir de qualquer data</li> <li>\ud83d\udd04 Rollback: Revers\u00e3o para vers\u00e3o anterior em caso de problemas</li> </ul>"},{"location":"arquitetura/#escalabilidade","title":"\ud83d\ude80 Escalabilidade","text":""},{"location":"arquitetura/#horizontal-scaling","title":"Horizontal Scaling","text":"<ul> <li>Spark: Aumento do n\u00famero de executors</li> <li>Airflow: M\u00faltiplos workers</li> <li>Azure: Auto-scaling baseado em demanda</li> </ul>"},{"location":"arquitetura/#vertical-scaling","title":"Vertical Scaling","text":"<ul> <li>Memory: Aumento da mem\u00f3ria por executor</li> <li>CPU: Mais cores por m\u00e1quina</li> <li>Storage: Upgrade para SSDs premium</li> </ul>"},{"location":"arquitetura/#otimizacoes","title":"Otimiza\u00e7\u00f5es","text":"<ul> <li>Caching: Dados frequentemente acessados</li> <li>Indexing: \u00cdndices Z-Order no Delta Lake</li> <li>Compaction: Otimiza\u00e7\u00e3o autom\u00e1tica de arquivos pequenos</li> </ul> <p>Esta arquitetura garante alta disponibilidade, escalabilidade e manutenibilidade, seguindo as melhores pr\u00e1ticas da ind\u00fastria para pipelines de dados modernos. </p>"},{"location":"env_example/","title":"\ud83d\udd10 Configura\u00e7\u00e3o de Vari\u00e1veis de Ambiente","text":""},{"location":"env_example/#visao-geral","title":"\ud83d\udccb Vis\u00e3o Geral","text":"<p>Este documento descreve todas as vari\u00e1veis de ambiente necess\u00e1rias para configurar e executar o projeto ETL com Apache Spark e Azure Data Lake.</p>"},{"location":"env_example/#arquivo-envexample","title":"\ud83d\udcc1 Arquivo <code>.env.example</code>","text":"<p>Crie um arquivo <code>.env</code> na raiz do projeto baseado no template abaixo:</p> <pre><code># =============================================================================\n# PROJETO ETL COM APACHE SPARK &amp; AZURE DATA LAKE\n# Arquivo de Configura\u00e7\u00e3o de Vari\u00e1veis de Ambiente\n# =============================================================================\n\n# =============================================================================\n# AZURE DATA LAKE STORAGE\n# =============================================================================\n\n# Nome da conta de armazenamento Azure\nADLS_ACCOUNT_NAME=seuaccountstorage\n\n# Containers do Data Lake (Medallion Architecture)\nADLS_FILE_SYSTEM_NAME=landing\nADLS_BRONZE_CONTAINER_NAME=bronze\nADLS_SILVER_CONTAINER_NAME=silver\nADLS_GOLD_CONTAINER_NAME=gold\n\n# Token SAS para acesso ao Azure Storage\n# Gere um novo token com permiss\u00f5es: read, add, create, write, delete, list, update, process, tag, filter, setimmutability\n# V\u00e1lido para: blob, file, queue, table\nADLS_SAS_TOKEN=\"sv=2022-11-02&amp;ss=bfqt&amp;srt=sco&amp;sp=rwdlacupitfx&amp;se=2024-12-31T23:59:59Z&amp;st=2024-01-01T00:00:00Z&amp;spr=https&amp;sig=SUA_ASSINATURA_SAS_AQUI\"\n\n# =============================================================================\n# SQL SERVER DATABASE\n# =============================================================================\n\n# Servidor SQL Server (Azure SQL Database ou SQL Server local)\nSQL_SERVER=seu-servidor.database.windows.net\n\n# Nome do banco de dados\nSQL_DATABASE=LogisticaDB\n\n# Schema do banco de dados\nSQL_SCHEMA=dbo\n\n# Credenciais de acesso\nSQL_USERNAME=admin\nSQL_PASSWORD=SuaSenhaSegura123!\n\n# =============================================================================\n# APACHE SPARK CONFIGURATION\n# =============================================================================\n\n# Mem\u00f3ria do Driver Spark\nSPARK_DRIVER_MEMORY=4g\n\n# Mem\u00f3ria dos Executors Spark\nSPARK_EXECUTOR_MEMORY=4g\n\n# N\u00famero de cores por Executor\nSPARK_EXECUTOR_CORES=2\n\n# N\u00famero de inst\u00e2ncias de Executors\nSPARK_EXECUTOR_INSTANCES=2\n\n# N\u00famero de parti\u00e7\u00f5es para shuffle operations\nSPARK_SQL_SHUFFLE_PARTITIONS=200\n\n# Paralelismo padr\u00e3o\nSPARK_DEFAULT_PARALLELISM=8\n\n# =============================================================================\n# AIRFLOW CONFIGURATION\n# =============================================================================\n\n# Ambiente do Airflow (development, staging, production)\nAIRFLOW_ENV=development\n\n# Timezone\nAIRFLOW_TIMEZONE=America/Sao_Paulo\n\n# Email para notifica\u00e7\u00f5es (opcional)\nAIRFLOW_ADMIN_EMAIL=admin@empresa.com\n\n# =============================================================================\n# LOGGING &amp; MONITORING\n# =============================================================================\n\n# N\u00edvel de log (DEBUG, INFO, WARNING, ERROR, CRITICAL)\nLOG_LEVEL=INFO\n\n# Diret\u00f3rio para logs\nLOG_DIR=/tmp/logs\n\n# Habilitar logs detalhados do Spark\nSPARK_VERBOSE_LOGGING=false\n\n# =============================================================================\n# PERFORMANCE TUNING\n# =============================================================================\n\n# Tamanho do arquivo alvo para Delta Lake (em bytes)\nDELTA_TARGET_FILE_SIZE=134217728\n\n# Habilitar otimiza\u00e7\u00f5es autom\u00e1ticas\nDELTA_AUTO_OPTIMIZE=true\nDELTA_AUTO_COMPACT=true\n\n# Timeout de rede (em segundos)\nNETWORK_TIMEOUT=800\n\n# =============================================================================\n# DEVELOPMENT &amp; TESTING\n# =============================================================================\n\n# Modo de desenvolvimento (habilita logs extras e configura\u00e7\u00f5es de debug)\nDEV_MODE=true\n\n# Usar dados sint\u00e9ticos (para desenvolvimento/testes)\nUSE_SYNTHETIC_DATA=false\n\n# N\u00famero de registros para dados sint\u00e9ticos\nSYNTHETIC_DATA_RECORDS=10000\n\n# =============================================================================\n# SECURITY\n# =============================================================================\n\n# Chave de criptografia para dados sens\u00edveis (opcional)\nENCRYPTION_KEY=sua-chave-de-32-caracteres-aqui\n\n# Habilitar SSL/TLS para conex\u00f5es\nENABLE_SSL=true\n\n# =============================================================================\n# BUSINESS CONFIGURATION\n# =============================================================================\n\n# Configura\u00e7\u00f5es espec\u00edficas do neg\u00f3cio de log\u00edstica\n\n# Fuso hor\u00e1rio para opera\u00e7\u00f5es de neg\u00f3cio\nBUSINESS_TIMEZONE=America/Sao_Paulo\n\n# Hor\u00e1rio de in\u00edcio das opera\u00e7\u00f5es (formato HH:MM)\nBUSINESS_START_TIME=06:00\n\n# Hor\u00e1rio de fim das opera\u00e7\u00f5es (formato HH:MM)\nBUSINESS_END_TIME=22:00\n\n# Dias \u00fateis (1=Segunda, 7=Domingo)\nBUSINESS_DAYS=1,2,3,4,5,6\n\n# =============================================================================\n# ALERTAS E NOTIFICA\u00c7\u00d5ES\n# =============================================================================\n\n# Email para alertas cr\u00edticos\nALERT_EMAIL=alertas@empresa.com\n\n# Webhook para notifica\u00e7\u00f5es Slack (opcional)\nSLACK_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK\n\n# Threshold para alertas de performance\nPERFORMANCE_ALERT_THRESHOLD=90\n\n# =============================================================================\n# BACKUP E RECOVERY\n# =============================================================================\n\n# Habilitar backup autom\u00e1tico\nENABLE_AUTO_BACKUP=true\n\n# Reten\u00e7\u00e3o de backups (em dias)\nBACKUP_RETENTION_DAYS=30\n\n# Diret\u00f3rio de backup\nBACKUP_DIR=/tmp/backups\n</code></pre>"},{"location":"env_example/#configuracao-passo-a-passo","title":"\ud83d\udd27 Configura\u00e7\u00e3o Passo a Passo","text":""},{"location":"env_example/#1-criacao-do-arquivo","title":"1. Cria\u00e7\u00e3o do Arquivo","text":"<pre><code># Copiar template\ncp .env.example .env\n\n# Editar com suas configura\u00e7\u00f5es\nnano .env  # ou vim .env, code .env, etc.\n</code></pre>"},{"location":"env_example/#2-azure-data-lake-storage","title":"2. Azure Data Lake Storage","text":""},{"location":"env_example/#criar-storage-account","title":"Criar Storage Account","text":"<pre><code># Login no Azure\naz login\n\n# Criar Resource Group\naz group create --name rg-projeto-etl --location brazilsouth\n\n# Criar Storage Account\naz storage account create \\\n    --name seuaccountstorage \\\n    --resource-group rg-projeto-etl \\\n    --location brazilsouth \\\n    --sku Standard_LRS \\\n    --kind StorageV2 \\\n    --hierarchical-namespace true\n</code></pre>"},{"location":"env_example/#criar-containers","title":"Criar Containers","text":"<pre><code># Criar containers para Medallion Architecture\naz storage container create --name landing --account-name seuaccountstorage\naz storage container create --name bronze --account-name seuaccountstorage\naz storage container create --name silver --account-name seuaccountstorage\naz storage container create --name gold --account-name seuaccountstorage\n</code></pre>"},{"location":"env_example/#gerar-sas-token","title":"Gerar SAS Token","text":"<pre><code># Gerar SAS Token v\u00e1lido por 1 ano\naz storage account generate-sas \\\n    --account-name seuaccountstorage \\\n    --account-key $(az storage account keys list --account-name seuaccountstorage --query '[0].value' -o tsv) \\\n    --expiry 2024-12-31T23:59:59Z \\\n    --permissions racwdlupitfx \\\n    --resource-types sco \\\n    --services bfqt\n</code></pre>"},{"location":"env_example/#3-sql-server-configuration","title":"3. SQL Server Configuration","text":""},{"location":"env_example/#azure-sql-database","title":"Azure SQL Database","text":"<pre><code># Criar SQL Server\naz sql server create \\\n    --name seu-sql-server \\\n    --resource-group rg-projeto-etl \\\n    --location brazilsouth \\\n    --admin-user admin \\\n    --admin-password SuaSenhaSegura123!\n\n# Criar Database\naz sql db create \\\n    --resource-group rg-projeto-etl \\\n    --server seu-sql-server \\\n    --name LogisticaDB \\\n    --service-objective Basic\n</code></pre>"},{"location":"env_example/#configurar-firewall","title":"Configurar Firewall","text":"<pre><code># Permitir acesso do Azure\naz sql server firewall-rule create \\\n    --resource-group rg-projeto-etl \\\n    --server seu-sql-server \\\n    --name AllowAzureServices \\\n    --start-ip-address 0.0.0.0 \\\n    --end-ip-address 0.0.0.0\n\n# Permitir seu IP local\naz sql server firewall-rule create \\\n    --resource-group rg-projeto-etl \\\n    --server seu-sql-server \\\n    --name AllowMyIP \\\n    --start-ip-address SEU_IP \\\n    --end-ip-address SEU_IP\n</code></pre>"},{"location":"env_example/#configuracoes-por-ambiente","title":"\ud83d\udcca Configura\u00e7\u00f5es por Ambiente","text":""},{"location":"env_example/#desenvolvimento-local","title":"Desenvolvimento Local","text":"<pre><code># Recursos limitados para desenvolvimento\nSPARK_DRIVER_MEMORY=2g\nSPARK_EXECUTOR_MEMORY=2g\nSPARK_EXECUTOR_CORES=1\nSPARK_EXECUTOR_INSTANCES=1\nSPARK_SQL_SHUFFLE_PARTITIONS=50\n\n# Logs verbosos para debug\nLOG_LEVEL=DEBUG\nSPARK_VERBOSE_LOGGING=true\nDEV_MODE=true\n</code></pre>"},{"location":"env_example/#stagingteste","title":"Staging/Teste","text":"<pre><code># Recursos m\u00e9dios para testes\nSPARK_DRIVER_MEMORY=4g\nSPARK_EXECUTOR_MEMORY=4g\nSPARK_EXECUTOR_CORES=2\nSPARK_EXECUTOR_INSTANCES=2\nSPARK_SQL_SHUFFLE_PARTITIONS=200\n\n# Logs moderados\nLOG_LEVEL=INFO\nDEV_MODE=false\n</code></pre>"},{"location":"env_example/#producao","title":"Produ\u00e7\u00e3o","text":"<pre><code># Recursos maximizados para produ\u00e7\u00e3o\nSPARK_DRIVER_MEMORY=8g\nSPARK_EXECUTOR_MEMORY=8g\nSPARK_EXECUTOR_CORES=4\nSPARK_EXECUTOR_INSTANCES=4\nSPARK_SQL_SHUFFLE_PARTITIONS=400\n\n# Logs otimizados\nLOG_LEVEL=WARNING\nSPARK_VERBOSE_LOGGING=false\nDEV_MODE=false\n\n# Seguran\u00e7a refor\u00e7ada\nENABLE_SSL=true\nENCRYPTION_KEY=sua-chave-segura-de-32-chars\n</code></pre>"},{"location":"env_example/#seguranca","title":"\ud83d\udd12 Seguran\u00e7a","text":""},{"location":"env_example/#protecao-de-credenciais","title":"Prote\u00e7\u00e3o de Credenciais","text":"<pre><code># Nunca commite o arquivo .env\necho \".env\" &gt;&gt; .gitignore\n\n# Use vari\u00e1veis de ambiente do sistema em produ\u00e7\u00e3o\nexport ADLS_SAS_TOKEN=\"seu_token_aqui\"\nexport SQL_PASSWORD=\"sua_senha_aqui\"\n</code></pre>"},{"location":"env_example/#rotacao-de-credenciais","title":"Rota\u00e7\u00e3o de Credenciais","text":"<pre><code># Gerar novo SAS Token mensalmente\naz storage account generate-sas \\\n    --account-name seuaccountstorage \\\n    --account-key $(az storage account keys list --account-name seuaccountstorage --query '[0].value' -o tsv) \\\n    --expiry $(date -d \"+1 month\" +%Y-%m-%dT23:59:59Z) \\\n    --permissions racwdlupitfx \\\n    --resource-types sco \\\n    --services bfqt\n</code></pre>"},{"location":"env_example/#validacao-da-configuracao","title":"\ud83e\uddea Valida\u00e7\u00e3o da Configura\u00e7\u00e3o","text":""},{"location":"env_example/#script-de-teste","title":"Script de Teste","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"\nScript para validar configura\u00e7\u00e3o das vari\u00e1veis de ambiente\n\"\"\"\n\nimport os\nfrom dotenv import load_dotenv\n\ndef validate_config():\n    load_dotenv()\n\n    required_vars = [\n        'ADLS_ACCOUNT_NAME',\n        'ADLS_SAS_TOKEN',\n        'SQL_SERVER',\n        'SQL_DATABASE',\n        'SQL_USERNAME',\n        'SQL_PASSWORD'\n    ]\n\n    missing_vars = []\n    for var in required_vars:\n        if not os.getenv(var):\n            missing_vars.append(var)\n\n    if missing_vars:\n        print(f\"\u274c Vari\u00e1veis faltando: {', '.join(missing_vars)}\")\n        return False\n\n    print(\"\u2705 Todas as vari\u00e1veis obrigat\u00f3rias est\u00e3o configuradas\")\n    return True\n\ndef test_azure_connection():\n    \"\"\"Testa conex\u00e3o com Azure Storage\"\"\"\n    try:\n        from azure.storage.blob import BlobServiceClient\n\n        account_name = os.getenv('ADLS_ACCOUNT_NAME')\n        sas_token = os.getenv('ADLS_SAS_TOKEN')\n\n        client = BlobServiceClient(\n            account_url=f\"https://{account_name}.blob.core.windows.net\",\n            credential=sas_token\n        )\n\n        # Listar containers\n        containers = list(client.list_containers())\n        print(f\"\u2705 Conex\u00e3o Azure OK. Containers encontrados: {len(containers)}\")\n        return True\n\n    except Exception as e:\n        print(f\"\u274c Erro na conex\u00e3o Azure: {e}\")\n        return False\n\ndef test_sql_connection():\n    \"\"\"Testa conex\u00e3o com SQL Server\"\"\"\n    try:\n        from sqlalchemy import create_engine\n        from urllib.parse import quote_plus\n\n        server = os.getenv('SQL_SERVER')\n        database = os.getenv('SQL_DATABASE')\n        username = os.getenv('SQL_USERNAME')\n        password = quote_plus(os.getenv('SQL_PASSWORD'))\n\n        conn_str = f\"mssql+pyodbc://{username}:{password}@{server}/{database}?driver=ODBC+Driver+17+for+SQL+Server\"\n        engine = create_engine(conn_str)\n\n        # Teste simples\n        with engine.connect() as conn:\n            result = conn.execute(\"SELECT 1 as test\")\n            print(\"\u2705 Conex\u00e3o SQL Server OK\")\n            return True\n\n    except Exception as e:\n        print(f\"\u274c Erro na conex\u00e3o SQL Server: {e}\")\n        return False\n\nif __name__ == \"__main__\":\n    print(\"\ud83d\udd0d Validando configura\u00e7\u00e3o...\")\n\n    if validate_config():\n        print(\"\\n\ud83e\uddea Testando conex\u00f5es...\")\n        test_azure_connection()\n        test_sql_connection()\n\n    print(\"\\n\u2705 Valida\u00e7\u00e3o conclu\u00edda!\")\n</code></pre>"},{"location":"env_example/#executar-validacao","title":"Executar Valida\u00e7\u00e3o","text":"<pre><code># Salvar como validate_config.py e executar\npython validate_config.py\n</code></pre>"},{"location":"env_example/#troubleshooting","title":"\ud83d\udcdd Troubleshooting","text":""},{"location":"env_example/#problemas-comuns","title":"Problemas Comuns","text":""},{"location":"env_example/#1-token-sas-invalido","title":"1. Token SAS Inv\u00e1lido","text":"<pre><code># Erro: \"Server failed to authenticate the request\"\n# Solu\u00e7\u00e3o: Gerar novo token SAS\n\naz storage account generate-sas \\\n    --account-name seuaccountstorage \\\n    --account-key $(az storage account keys list --account-name seuaccountstorage --query '[0].value' -o tsv) \\\n    --expiry 2024-12-31T23:59:59Z \\\n    --permissions racwdlupitfx \\\n    --resource-types sco \\\n    --services bfqt\n</code></pre>"},{"location":"env_example/#2-conexao-sql-server-negada","title":"2. Conex\u00e3o SQL Server Negada","text":"<pre><code># Erro: \"Login failed for user\"\n# Solu\u00e7\u00e3o: Verificar firewall e credenciais\n\n# Listar regras de firewall\naz sql server firewall-rule list \\\n    --resource-group rg-projeto-etl \\\n    --server seu-sql-server\n\n# Adicionar seu IP\naz sql server firewall-rule create \\\n    --resource-group rg-projeto-etl \\\n    --server seu-sql-server \\\n    --name AllowMyIP \\\n    --start-ip-address $(curl -s ifconfig.me) \\\n    --end-ip-address $(curl -s ifconfig.me)\n</code></pre>"},{"location":"env_example/#3-erro-de-memoria-spark","title":"3. Erro de Mem\u00f3ria Spark","text":"<pre><code># Erro: \"OutOfMemoryError\"\n# Solu\u00e7\u00e3o: Reduzir configura\u00e7\u00f5es de mem\u00f3ria\n\nSPARK_DRIVER_MEMORY=2g\nSPARK_EXECUTOR_MEMORY=2g\nSPARK_SQL_SHUFFLE_PARTITIONS=50\n</code></pre>"},{"location":"env_example/#referencias","title":"\ud83d\udcda Refer\u00eancias","text":"<ul> <li>Azure Storage SAS Tokens</li> <li>Azure SQL Database</li> <li>Apache Spark Configuration</li> <li>Apache Airflow Configuration</li> </ul> <p>Importante</p> <p>Nunca commite credenciais reais no reposit\u00f3rio. Use sempre o arquivo <code>.env</code> local ou vari\u00e1veis de ambiente do sistema em produ\u00e7\u00e3o. </p>"},{"location":"estrutura_projeto/","title":"Estrutura do Projeto","text":"<pre><code>\u251c\u2500\u2500 astro/              # Ambiente Airflow\n\u2502   \u251c\u2500\u2500 dags/           # DAGs do Airflow\n\u2502   \u251c\u2500\u2500 tests/          # Testes dos DAGs\n\u2502   \u2514\u2500\u2500 Dockerfile      # Imagem do Airflow\n\u251c\u2500\u2500 data/               # Scripts auxiliares\n\u251c\u2500\u2500 pyproject.toml      # Depend\u00eancias do projeto\n\u251c\u2500\u2500 poetry.lock         # Lock de depend\u00eancias\n\u251c\u2500\u2500 comandos.txt        # Comandos \u00fateis\n\u2514\u2500\u2500 README.md           # Documenta\u00e7\u00e3o inicial\n</code></pre>"},{"location":"inicio_rapido/","title":"\ud83d\ude80 In\u00edcio R\u00e1pido","text":""},{"location":"inicio_rapido/#pre-requisitos","title":"\ud83d\udccb Pr\u00e9-requisitos","text":"<p>Antes de come\u00e7ar, certifique-se de ter os seguintes itens instalados em seu sistema:</p>"},{"location":"inicio_rapido/#ferramentas-essenciais","title":"\ud83d\udee0\ufe0f Ferramentas Essenciais","text":"WindowsmacOSLinux (Ubuntu/Debian) <pre><code># Instalar Python 3.10+\nwinget install Python.Python.3.10\n\n# Instalar Docker Desktop\nwinget install Docker.DockerDesktop\n\n# Instalar Azure CLI\nwinget install Microsoft.AzureCLI\n\n# Instalar Git\nwinget install Git.Git\n</code></pre> <pre><code># Instalar Homebrew (se n\u00e3o tiver)\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\n# Instalar Python 3.10+\nbrew install python@3.10\n\n# Instalar Docker Desktop\nbrew install --cask docker\n\n# Instalar Azure CLI\nbrew install azure-cli\n\n# Instalar Git\nbrew install git\n</code></pre> <pre><code># Atualizar reposit\u00f3rios\nsudo apt update\n\n# Instalar Python 3.10+\nsudo apt install python3.10 python3.10-pip python3.10-venv\n\n# Instalar Docker\ncurl -fsSL https://get.docker.com -o get-docker.sh\nsh get-docker.sh\n\n# Instalar Azure CLI\ncurl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\n\n# Instalar Git\nsudo apt install git\n</code></pre>"},{"location":"inicio_rapido/#poetry-gerenciador-de-dependencias","title":"\ud83d\udce6 Poetry (Gerenciador de Depend\u00eancias)","text":"<pre><code># Instalar Poetry\ncurl -sSL https://install.python-poetry.org | python3 -\n\n# Adicionar ao PATH (adicione ao seu .bashrc/.zshrc)\nexport PATH=\"$HOME/.local/bin:$PATH\"\n\n# Verificar instala\u00e7\u00e3o\npoetry --version\n</code></pre>"},{"location":"inicio_rapido/#instalacao-em-5-minutos","title":"\u26a1 Instala\u00e7\u00e3o em 5 Minutos","text":""},{"location":"inicio_rapido/#1-clone-o-repositorio","title":"1. Clone o Reposit\u00f3rio","text":"<pre><code>git clone https://github.com/seu-usuario/projeto_etl_spark.git\ncd projeto_etl_spark\n</code></pre>"},{"location":"inicio_rapido/#2-configure-o-ambiente-python","title":"2. Configure o Ambiente Python","text":"<pre><code># Criar ambiente virtual com Poetry\npoetry install\n\n# Ativar o ambiente virtual\npoetry shell\n</code></pre>"},{"location":"inicio_rapido/#3-configure-as-variaveis-de-ambiente","title":"3. Configure as Vari\u00e1veis de Ambiente","text":"<pre><code># Copiar arquivo de exemplo\ncp .env.example .env\n\n# Editar com suas credenciais\nnano .env  # ou vim .env, ou qualquer editor de sua prefer\u00eancia\n</code></pre> <p>Exemplo de configura\u00e7\u00e3o <code>.env</code>: <pre><code># Azure Data Lake\nADLS_ACCOUNT_NAME=seuaccountstorage\nADLS_FILE_SYSTEM_NAME=landing\nADLS_BRONZE_CONTAINER_NAME=bronze\nADLS_SILVER_CONTAINER_NAME=silver\nADLS_GOLD_CONTAINER_NAME=gold\nADLS_SAS_TOKEN=\"sv=2022-11-02&amp;ss=bfqt&amp;srt=sco&amp;sp=rwdlacupitfx&amp;se=2024-12-31T23:59:59Z&amp;st=2024-01-01T00:00:00Z&amp;spr=https&amp;sig=SUA_ASSINATURA_SAS\"\n\n# SQL Server\nSQL_SERVER=seu-servidor.database.windows.net\nSQL_DATABASE=LogisticaDB\nSQL_SCHEMA=dbo\nSQL_USERNAME=admin\nSQL_PASSWORD=SuaSenhaSegura123!\n\n# Spark Configuration (opcional)\nSPARK_DRIVER_MEMORY=4g\nSPARK_EXECUTOR_MEMORY=4g\nSPARK_EXECUTOR_CORES=2\n</code></pre></p>"},{"location":"inicio_rapido/#4-inicie-o-airflow","title":"4. Inicie o Airflow","text":"<pre><code># Navegar para o diret\u00f3rio do Airflow\ncd astro\n\n# Iniciar o ambiente Airflow\nastro dev start\n</code></pre> <p>Dica</p> <p>O comando <code>astro dev start</code> pode demorar alguns minutos na primeira execu\u00e7\u00e3o, pois precisa baixar as imagens Docker.</p>"},{"location":"inicio_rapido/#5-acesse-a-interface-web","title":"5. Acesse a Interface Web","text":"<p>Ap\u00f3s a inicializa\u00e7\u00e3o completa, acesse:</p> <ul> <li>\ud83c\udf10 Airflow UI: http://localhost:8080</li> <li>\ud83d\udc64 Credenciais: <code>admin</code> / <code>admin</code></li> </ul>"},{"location":"inicio_rapido/#primeira-execucao","title":"\ud83c\udfaf Primeira Execu\u00e7\u00e3o","text":""},{"location":"inicio_rapido/#1-verifique-as-conexoes","title":"1. Verifique as Conex\u00f5es","text":"<p>No Airflow UI, v\u00e1 para Admin \u2192 Connections e verifique se as conex\u00f5es est\u00e3o configuradas:</p> <ul> <li>\u2705 <code>azure_data_lake_default</code></li> <li>\u2705 <code>sql_server_default</code></li> </ul>"},{"location":"inicio_rapido/#2-execute-o-pipeline","title":"2. Execute o Pipeline","text":"<ol> <li>Navegue para DAGs na interface do Airflow</li> <li>Encontre a DAG <code>sqlserver_to_bronze_adls</code></li> <li>Clique no bot\u00e3o \u25b6\ufe0f Trigger DAG</li> <li>Monitore a execu\u00e7\u00e3o na aba Graph View</li> </ol>"},{"location":"inicio_rapido/#3-verifique-os-resultados","title":"3. Verifique os Resultados","text":"<p>Ap\u00f3s a execu\u00e7\u00e3o bem-sucedida, voc\u00ea deve ver:</p> <ul> <li>\u2705 Landing Zone: Arquivos CSV no container <code>landing</code></li> <li>\u2705 Bronze Layer: Tabelas Delta no container <code>bronze</code></li> <li>\u2705 Silver Layer: Dados limpos no container <code>silver</code></li> <li>\u2705 Gold Layer: Modelo dimensional no container <code>gold</code></li> </ul>"},{"location":"inicio_rapido/#configuracao-avancada","title":"\ud83d\udd27 Configura\u00e7\u00e3o Avan\u00e7ada","text":""},{"location":"inicio_rapido/#azure-data-lake-setup","title":"Azure Data Lake Setup","text":"<p>Se voc\u00ea ainda n\u00e3o tem um Azure Data Lake configurado:</p> <pre><code># Login no Azure\naz login\n\n# Criar Resource Group\naz group create --name rg-projeto-etl --location brazilsouth\n\n# Criar Storage Account\naz storage account create \\\n    --name seuaccountstorage \\\n    --resource-group rg-projeto-etl \\\n    --location brazilsouth \\\n    --sku Standard_LRS \\\n    --kind StorageV2 \\\n    --hierarchical-namespace true\n\n# Criar containers\naz storage container create --name landing --account-name seuaccountstorage\naz storage container create --name bronze --account-name seuaccountstorage\naz storage container create --name silver --account-name seuaccountstorage\naz storage container create --name gold --account-name seuaccountstorage\n\n# Gerar SAS Token (v\u00e1lido por 1 ano)\naz storage account generate-sas \\\n    --account-name seuaccountstorage \\\n    --account-key $(az storage account keys list --account-name seuaccountstorage --query '[0].value' -o tsv) \\\n    --expiry 2024-12-31T23:59:59Z \\\n    --permissions racwdlup \\\n    --resource-types sco \\\n    --services bfqt\n</code></pre>"},{"location":"inicio_rapido/#sql-server-setup-opcional","title":"SQL Server Setup (Opcional)","text":"<p>Se voc\u00ea quiser usar dados de exemplo:</p> <pre><code># Navegar para o diret\u00f3rio de dados\ncd data\n\n# Executar script de cria\u00e7\u00e3o de tabelas\npython create_tables.py\n\n# Gerar dados sint\u00e9ticos\npython faker_data.py\n</code></pre>"},{"location":"inicio_rapido/#teste-rapido","title":"\ud83e\uddea Teste R\u00e1pido","text":""},{"location":"inicio_rapido/#teste-de-conexao-sql-server","title":"Teste de Conex\u00e3o SQL Server","text":"<pre><code># Dentro do ambiente Poetry\npoetry shell\n\n# Executar teste de conex\u00e3o\npython astro/tests/test_sqlserver_connection.py\n</code></pre>"},{"location":"inicio_rapido/#teste-de-dag","title":"Teste de DAG","text":"<pre><code># Teste de sintaxe da DAG\npython astro/dags/main.py\n\n# Teste unit\u00e1rio\npytest astro/tests/test_dag_example.py\n</code></pre>"},{"location":"inicio_rapido/#teste-de-spark","title":"Teste de Spark","text":"<pre><code># Abrir Python interativo\npython\n\n# Testar Spark\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .appName(\"teste_rapido\") \\\n    .getOrCreate()\n\n# Criar DataFrame de teste\ndf = spark.createDataFrame([(1, \"teste\"), (2, \"spark\")], [\"id\", \"nome\"])\ndf.show()\n\nspark.stop()\n</code></pre>"},{"location":"inicio_rapido/#monitoramento","title":"\ud83d\udcca Monitoramento","text":""},{"location":"inicio_rapido/#logs-do-airflow","title":"Logs do Airflow","text":"<pre><code># Ver logs em tempo real\nastro dev logs\n\n# Logs espec\u00edficos de um servi\u00e7o\nastro dev logs scheduler\nastro dev logs webserver\n</code></pre>"},{"location":"inicio_rapido/#interface-de-monitoramento","title":"Interface de Monitoramento","text":"<ul> <li>\ud83d\udcca Airflow UI: http://localhost:8080</li> <li>\ud83d\udcc8 Flower (Celery): http://localhost:5555</li> <li>\ud83d\uddc4\ufe0f Postgres: http://localhost:5432</li> </ul>"},{"location":"inicio_rapido/#troubleshooting","title":"\ud83d\udea8 Troubleshooting","text":""},{"location":"inicio_rapido/#problemas-comuns","title":"Problemas Comuns","text":""},{"location":"inicio_rapido/#1-erro-de-conexao-com-azure","title":"1. Erro de Conex\u00e3o com Azure","text":"<pre><code># Verificar conectividade\naz storage blob list --container-name landing --account-name seuaccountstorage\n\n# Testar SAS Token\ncurl \"https://seuaccountstorage.blob.core.windows.net/landing?sv=2022-11-02&amp;ss=bfqt...\"\n</code></pre>"},{"location":"inicio_rapido/#2-erro-de-memoria-no-spark","title":"2. Erro de Mem\u00f3ria no Spark","text":"<p>Edite o arquivo <code>.env</code>: <pre><code>SPARK_DRIVER_MEMORY=2g\nSPARK_EXECUTOR_MEMORY=2g\n</code></pre></p>"},{"location":"inicio_rapido/#3-airflow-nao-inicia","title":"3. Airflow n\u00e3o Inicia","text":"<pre><code># Parar todos os servi\u00e7os\nastro dev stop\n\n# Limpar containers\ndocker system prune -f\n\n# Reiniciar\nastro dev start\n</code></pre>"},{"location":"inicio_rapido/#4-problemas-de-permissao","title":"4. Problemas de Permiss\u00e3o","text":"<pre><code># Linux/macOS - Ajustar permiss\u00f5es\nsudo chown -R $USER:$USER .\nchmod -R 755 .\n</code></pre>"},{"location":"inicio_rapido/#comandos-uteis","title":"Comandos \u00dateis","text":"<pre><code># Verificar status dos containers\ndocker ps\n\n# Ver logs de um container espec\u00edfico\ndocker logs &lt;container_id&gt;\n\n# Reiniciar apenas o Airflow\nastro dev restart\n\n# Executar comando dentro do container\nastro dev bash\n</code></pre>"},{"location":"inicio_rapido/#proximos-passos","title":"\ud83d\udcda Pr\u00f3ximos Passos","text":"<p>Agora que voc\u00ea tem o ambiente funcionando, explore:</p> <ol> <li>\ud83d\udcd6 Arquitetura - Entenda como o sistema funciona</li> <li>\ud83d\udd27 Pipeline ETL - Detalhes do processamento de dados</li> <li>\ud83d\udcca KPIs e M\u00e9tricas - Indicadores calculados</li> <li>\ud83c\udf9b\ufe0f Airflow - Orquestra\u00e7\u00e3o avan\u00e7ada</li> <li>\ud83e\uddea Testes - Como testar o sistema</li> </ol>"},{"location":"inicio_rapido/#precisa-de-ajuda","title":"\ud83c\udd98 Precisa de Ajuda?","text":"<ul> <li>\ud83d\udcd6 Documenta\u00e7\u00e3o: docs/</li> <li>\ud83d\udc1b Issues: GitHub Issues</li> <li>\ud83d\udcac Discuss\u00f5es: GitHub Discussions</li> <li>\ud83d\udce7 Email: contato@projeto-etl.com</li> </ul> <p>Parab\u00e9ns! \ud83c\udf89</p> <p>Voc\u00ea configurou com sucesso o projeto ETL com Apache Spark e Azure Data Lake. O pipeline est\u00e1 pronto para processar dados em larga escala! </p>"},{"location":"instalacao/","title":"Instala\u00e7\u00e3o","text":""},{"location":"instalacao/#pre-requisitos","title":"Pr\u00e9-requisitos","text":"<ul> <li>Python &gt;= 3.10</li> <li>Docker (para rodar o Airflow)</li> <li>Azure CLI (para configurar o Data Lake)</li> </ul>"},{"location":"instalacao/#instalacao-dos-pacotes","title":"Instala\u00e7\u00e3o dos pacotes","text":"<p>Execute:</p> <pre><code>poetry install\n</code></pre>"},{"location":"instalacao/#subindo-o-airflow","title":"Subindo o Airflow","text":"<pre><code>cd astro\ndocker-compose up -d\n</code></pre>"},{"location":"kpis_metricas/","title":"\ud83d\udcc8 KPIs e M\u00e9tricas","text":""},{"location":"kpis_metricas/#visao-geral","title":"\ud83d\udccb Vis\u00e3o Geral","text":"<p>O sistema calcula automaticamente um conjunto abrangente de KPIs (Key Performance Indicators) e m\u00e9tricas operacionais que fornecem insights valiosos sobre a performance do neg\u00f3cio de log\u00edstica e transporte. Todos os indicadores s\u00e3o atualizados automaticamente atrav\u00e9s do pipeline ETL e armazenados na camada Gold para consumo por ferramentas de BI.</p>"},{"location":"kpis_metricas/#kpis-principais","title":"\ud83c\udfaf KPIs Principais","text":""},{"location":"kpis_metricas/#1-on-time-delivery-otd","title":"\ud83d\ude9a 1. On-Time Delivery (OTD)","text":"<p>Descri\u00e7\u00e3o: Percentual de entregas realizadas dentro do prazo estabelecido.</p> <p>F\u00f3rmula:  <pre><code>OTD = (Entregas no Prazo / Total de Entregas) \u00d7 100\n</code></pre></p> <p>Implementa\u00e7\u00e3o: <pre><code>kpi_otd = fato_entregas.withColumn(\"on_time\",\n    (col(\"data_fim_real_entrega_key\") &lt;= col(\"data_previsao_fim_entrega_key\")) | \n    (col(\"data_previsao_fim_entrega_key\").isNull())\n).agg(\n    (sum(when(col(\"on_time\") == True, 1).otherwise(0)) / count(\"*\") * 100)\n    .alias(\"percentual_entregas_no_prazo\")\n)\n</code></pre></p> <p>Metas de Neg\u00f3cio: - \ud83c\udfaf Excelente: &gt; 95% - \u2705 Bom: 90-95% - \u26a0\ufe0f Aten\u00e7\u00e3o: 80-90% - \ud83d\udea8 Cr\u00edtico: &lt; 80%</p> <p>Frequ\u00eancia de Atualiza\u00e7\u00e3o: Di\u00e1ria Granularidade: Por dia, semana, m\u00eas, cliente, rota</p>"},{"location":"kpis_metricas/#2-custo-medio-de-frete-por-rota","title":"\ud83d\udcb0 2. Custo M\u00e9dio de Frete por Rota","text":"<p>Descri\u00e7\u00e3o: Custo m\u00e9dio de transporte por quil\u00f4metro em cada rota.</p> <p>F\u00f3rmula:  <pre><code>Custo por KM = Valor Total do Frete / Dist\u00e2ncia da Rota (KM)\n</code></pre></p> <p>Implementa\u00e7\u00e3o: <pre><code>kpi_custo_rota = fato_entregas.join(dim_rota, \n    fato_entregas[\"id_rota_origem\"] == dim_rota[\"id_rota_origem\"], \"left\"\n).groupBy(\n    dim_rota[\"nome_rota\"], \n    dim_rota[\"origem\"], \n    dim_rota[\"destino\"],\n    dim_rota[\"distancia_km\"]\n).agg(\n    avg(\"valor_frete\").alias(\"custo_medio_frete\"),\n    (avg(\"valor_frete\") / first(\"distancia_km\")).alias(\"custo_por_km\")\n)\n</code></pre></p> <p>An\u00e1lises Dispon\u00edveis: - \ud83d\udcca Por Rota: Identifica\u00e7\u00e3o de rotas mais/menos rent\u00e1veis - \ud83d\udcc8 Tend\u00eancia Temporal: Evolu\u00e7\u00e3o dos custos ao longo do tempo - \ud83d\udd0d Benchmarking: Compara\u00e7\u00e3o entre rotas similares - \ud83d\udca1 Otimiza\u00e7\u00e3o: Identifica\u00e7\u00e3o de oportunidades de melhoria</p> <p>Frequ\u00eancia de Atualiza\u00e7\u00e3o: Semanal Granularidade: Por rota, regi\u00e3o, tipo de carga</p>"},{"location":"kpis_metricas/#3-utilizacao-da-frota","title":"\ud83d\ude9b 3. Utiliza\u00e7\u00e3o da Frota","text":"<p>Descri\u00e7\u00e3o: An\u00e1lise da utiliza\u00e7\u00e3o dos ve\u00edculos por tipo e performance.</p> <p>M\u00e9tricas Calculadas: - Total de entregas por tipo de ve\u00edculo - Taxa de ocupa\u00e7\u00e3o da frota - Quilometragem m\u00e9dia por ve\u00edculo - Tempo m\u00e9dio em tr\u00e2nsito</p> <p>Implementa\u00e7\u00e3o: <pre><code>kpi_frota = fato_entregas.join(dim_veiculo, \n    fato_entregas[\"id_veiculo_origem\"] == dim_veiculo[\"id_veiculo_origem\"], \"left\"\n).groupBy(\n    dim_veiculo[\"tipo_veiculo\"],\n    dim_veiculo[\"marca\"],\n    dim_veiculo[\"capacidade_carga_kg\"]\n).agg(\n    count(\"*\").alias(\"total_entregas\"),\n    sum(\"peso_carga_kg\").alias(\"peso_total_transportado\"),\n    avg(\"peso_carga_kg\").alias(\"peso_medio_por_entrega\"),\n    (sum(\"peso_carga_kg\") / first(\"capacidade_carga_kg\") * 100).alias(\"taxa_ocupacao_media\")\n)\n</code></pre></p> <p>Insights Gerados: - \ud83c\udfaf Efici\u00eancia por Tipo: Caminh\u00f5es vs Vans vs Utilit\u00e1rios - \ud83d\udcca Capacidade: Taxa de ocupa\u00e7\u00e3o da carga - \ud83d\udd04 Rotatividade: Frequ\u00eancia de uso por ve\u00edculo - \ud83d\udcb0 ROI: Retorno sobre investimento por ve\u00edculo</p> <p>Frequ\u00eancia de Atualiza\u00e7\u00e3o: Mensal Granularidade: Por ve\u00edculo, tipo, marca, regi\u00e3o</p>"},{"location":"kpis_metricas/#4-revenue-por-cliente","title":"\ud83d\udcbc 4. Revenue por Cliente","text":"<p>Descri\u00e7\u00e3o: Valor total de frete gerado por cada cliente e an\u00e1lise de rentabilidade.</p> <p>M\u00e9tricas Calculadas: - Valor total de frete por cliente - Ticket m\u00e9dio por entrega - Frequ\u00eancia de envios - Rentabilidade por cliente</p> <p>Implementa\u00e7\u00e3o: <pre><code>kpi_cliente = fato_entregas.join(dim_cliente, \n    fato_entregas[\"id_cliente_remetente_origem\"] == dim_cliente[\"id_cliente_origem\"], \"left\"\n).groupBy(\n    dim_cliente[\"nome_cliente\"],\n    dim_cliente[\"tipo_cliente\"],\n    dim_cliente[\"cidade\"],\n    dim_cliente[\"estado\"]\n).agg(\n    sum(\"valor_frete\").alias(\"valor_total_frete\"),\n    count(\"*\").alias(\"total_entregas\"),\n    avg(\"valor_frete\").alias(\"ticket_medio\"),\n    sum(\"peso_carga_kg\").alias(\"peso_total_enviado\")\n)\n</code></pre></p> <p>Segmenta\u00e7\u00e3o de Clientes: - \ud83e\udd47 Premium: &gt; R$ 50.000/m\u00eas - \ud83e\udd48 Gold: R$ 20.000 - R$ 50.000/m\u00eas - \ud83e\udd49 Silver: R$ 5.000 - R$ 20.000/m\u00eas - \ud83d\udcca Standard: &lt; R$ 5.000/m\u00eas</p> <p>Frequ\u00eancia de Atualiza\u00e7\u00e3o: Mensal Granularidade: Por cliente, segmento, regi\u00e3o, tipo</p>"},{"location":"kpis_metricas/#metricas-operacionais","title":"\ud83d\udcca M\u00e9tricas Operacionais","text":""},{"location":"kpis_metricas/#metricas-temporais","title":"\ud83d\udcc5 M\u00e9tricas Temporais","text":""},{"location":"kpis_metricas/#total-de-entregas-mensal","title":"Total de Entregas Mensal","text":"<pre><code>metrica_entregas_mes = fato_entregas.join(dim_data, \n    fato_entregas[\"data_inicio_entrega_key\"] == dim_data[\"data_key\"], \"left\"\n).groupBy(\n    dim_data[\"ano\"], \n    dim_data[\"mes\"],\n    dim_data[\"nome_mes\"]\n).agg(\n    count(\"*\").alias(\"total_entregas\"),\n    sum(\"valor_frete\").alias(\"receita_total\"),\n    avg(\"valor_frete\").alias(\"ticket_medio\")\n).orderBy(\"ano\", \"mes\")\n</code></pre>"},{"location":"kpis_metricas/#peso-total-transportado-por-mes","title":"Peso Total Transportado por M\u00eas","text":"<pre><code>metrica_peso_mes = fato_entregas.join(dim_data, \n    fato_entregas[\"data_inicio_entrega_key\"] == dim_data[\"data_key\"], \"left\"\n).groupBy(\n    dim_data[\"ano\"], \n    dim_data[\"mes\"]\n).agg(\n    sum(\"peso_carga_kg\").alias(\"peso_total_kg\"),\n    avg(\"peso_carga_kg\").alias(\"peso_medio_kg\"),\n    count(\"*\").alias(\"total_entregas\")\n).orderBy(\"ano\", \"mes\")\n</code></pre>"},{"location":"kpis_metricas/#metricas-de-manutencao","title":"\ud83d\udd27 M\u00e9tricas de Manuten\u00e7\u00e3o","text":""},{"location":"kpis_metricas/#custo-de-manutencao-por-veiculo","title":"Custo de Manuten\u00e7\u00e3o por Ve\u00edculo","text":"<pre><code>metrica_manutencao = fato_manutencoes.join(dim_veiculo,\n    fato_manutencoes[\"id_veiculo_origem\"] == dim_veiculo[\"id_veiculo_origem\"], \"left\"\n).groupBy(\n    dim_veiculo[\"placa\"],\n    dim_veiculo[\"modelo\"],\n    dim_veiculo[\"ano_fabricacao\"]\n).agg(\n    sum(\"custo_manutencao\").alias(\"custo_total_manutencao\"),\n    count(\"*\").alias(\"total_manutencoes\"),\n    avg(\"custo_manutencao\").alias(\"custo_medio_manutencao\"),\n    sum(\"tempo_parado_horas\").alias(\"tempo_total_parado\")\n)\n</code></pre>"},{"location":"kpis_metricas/#eficiencia-de-combustivel","title":"Efici\u00eancia de Combust\u00edvel","text":"<pre><code>metrica_combustivel = fato_abastecimentos.join(dim_veiculo,\n    fato_abastecimentos[\"id_veiculo_origem\"] == dim_veiculo[\"id_veiculo_origem\"], \"left\"\n).groupBy(\n    dim_veiculo[\"placa\"],\n    dim_veiculo[\"tipo_veiculo\"]\n).agg(\n    sum(\"litros\").alias(\"total_litros\"),\n    sum(\"valor_total\").alias(\"custo_total_combustivel\"),\n    avg(\"valor_total\" / \"litros\").alias(\"preco_medio_por_litro\")\n)\n</code></pre>"},{"location":"kpis_metricas/#dashboards-e-visualizacoes","title":"\ud83d\udcc8 Dashboards e Visualiza\u00e7\u00f5es","text":""},{"location":"kpis_metricas/#dashboard-executivo","title":"\ud83c\udf9b\ufe0f Dashboard Executivo","text":"<p>KPIs Principais: - \ud83d\udcca On-Time Delivery (gauge chart) - \ud83d\udcb0 Revenue mensal (line chart) - \ud83d\ude9b Utiliza\u00e7\u00e3o da frota (bar chart) - \ud83d\udcc8 Tend\u00eancias operacionais (combo chart)</p> <p>Filtros Dispon\u00edveis: - \ud83d\udcc5 Per\u00edodo (dia, semana, m\u00eas, ano) - \ud83c\udf0d Regi\u00e3o (estado, cidade) - \ud83d\udc65 Cliente (individual, segmento) - \ud83d\ude9a Tipo de ve\u00edculo</p>"},{"location":"kpis_metricas/#dashboard-operacional","title":"\ud83d\udcca Dashboard Operacional","text":"<p>M\u00e9tricas Detalhadas: - \ud83d\uddfa\ufe0f Mapa de entregas por regi\u00e3o - \u23f1\ufe0f Tempo m\u00e9dio de entrega por rota - \ud83d\udce6 Volume de carga por tipo - \ud83d\udd27 Status de manuten\u00e7\u00e3o da frota</p>"},{"location":"kpis_metricas/#dashboard-financeiro","title":"\ud83d\udcb0 Dashboard Financeiro","text":"<p>An\u00e1lises Financeiras: - \ud83d\udcb5 Receita por cliente/regi\u00e3o/per\u00edodo - \ud83d\udcca Margem de lucro por rota - \ud83d\udcb8 Custos operacionais (combust\u00edvel, manuten\u00e7\u00e3o, multas) - \ud83d\udcc8 Proje\u00e7\u00f5es e tend\u00eancias</p>"},{"location":"kpis_metricas/#alertas-e-notificacoes","title":"\ud83c\udfaf Alertas e Notifica\u00e7\u00f5es","text":""},{"location":"kpis_metricas/#alertas-criticos","title":"\ud83d\udea8 Alertas Cr\u00edticos","text":"<p>On-Time Delivery &lt; 80%: <pre><code>if otd_percentage &lt; 80:\n    send_alert(\"CR\u00cdTICO: OTD abaixo de 80%\", \n               recipients=[\"gerencia@empresa.com\"])\n</code></pre></p> <p>Custo por KM acima do benchmark: <pre><code>if custo_km &gt; benchmark_custo_km * 1.2:\n    send_alert(\"ATEN\u00c7\u00c3O: Custo por KM 20% acima do benchmark\", \n               recipients=[\"operacoes@empresa.com\"])\n</code></pre></p>"},{"location":"kpis_metricas/#alertas-de-atencao","title":"\u26a0\ufe0f Alertas de Aten\u00e7\u00e3o","text":"<ul> <li>\ud83d\udcc8 Volume an\u00f4malo de entregas (&gt; 150% da m\u00e9dia)</li> <li>\ud83d\udd27 Aumento nos custos de manuten\u00e7\u00e3o (&gt; 20% m\u00eas anterior)</li> <li>\u26fd Consumo excessivo de combust\u00edvel por ve\u00edculo</li> <li>\ud83d\udea8 Aumento nas multas por motorista/ve\u00edculo</li> </ul>"},{"location":"kpis_metricas/#analises-avancadas","title":"\ud83d\udd0d An\u00e1lises Avan\u00e7adas","text":""},{"location":"kpis_metricas/#analise-de-tendencias","title":"\ud83d\udcca An\u00e1lise de Tend\u00eancias","text":"<p>Sazonalidade: <pre><code># An\u00e1lise de padr\u00f5es sazonais\nsazonalidade = fato_entregas.join(dim_data,\n    fato_entregas[\"data_inicio_entrega_key\"] == dim_data[\"data_key\"], \"left\"\n).groupBy(\n    dim_data[\"mes\"],\n    dim_data[\"nome_mes\"]\n).agg(\n    count(\"*\").alias(\"total_entregas\"),\n    avg(\"valor_frete\").alias(\"ticket_medio\")\n).orderBy(\"mes\")\n</code></pre></p> <p>Crescimento YoY (Year over Year): <pre><code># Compara\u00e7\u00e3o ano sobre ano\ncrescimento_yoy = fato_entregas.join(dim_data,\n    fato_entregas[\"data_inicio_entrega_key\"] == dim_data[\"data_key\"], \"left\"\n).groupBy(\n    dim_data[\"ano\"],\n    dim_data[\"mes\"]\n).agg(\n    count(\"*\").alias(\"total_entregas\"),\n    sum(\"valor_frete\").alias(\"receita_total\")\n).withColumn(\"crescimento_percentual\", \n    (col(\"receita_total\") - lag(\"receita_total\").over(window)) / \n    lag(\"receita_total\").over(window) * 100\n)\n</code></pre></p>"},{"location":"kpis_metricas/#analise-de-performance","title":"\ud83c\udfaf An\u00e1lise de Performance","text":"<p>Efici\u00eancia por Motorista: <pre><code>performance_motorista = fato_entregas.join(dim_motorista,\n    fato_entregas[\"id_motorista_origem\"] == dim_motorista[\"id_motorista_origem\"], \"left\"\n).groupBy(\n    dim_motorista[\"nome_motorista\"]\n).agg(\n    count(\"*\").alias(\"total_entregas\"),\n    (sum(when(col(\"status_entrega\") == \"Entregue\", 1).otherwise(0)) / count(\"*\") * 100)\n    .alias(\"taxa_sucesso\"),\n    avg(\"valor_frete\").alias(\"receita_media_por_entrega\")\n)\n</code></pre></p>"},{"location":"kpis_metricas/#catalogo-de-kpis","title":"\ud83d\udccb Cat\u00e1logo de KPIs","text":"KPI Descri\u00e7\u00e3o F\u00f3rmula Frequ\u00eancia Meta On-Time Delivery % entregas no prazo (Entregas no Prazo / Total) \u00d7 100 Di\u00e1ria &gt; 95% Custo por KM Custo m\u00e9dio por quil\u00f4metro Valor Frete / Dist\u00e2ncia KM Semanal &lt; R$ 2,50/km Taxa de Ocupa\u00e7\u00e3o % capacidade utilizada Peso Transportado / Capacidade \u00d7 100 Mensal &gt; 80% Revenue por Cliente Receita gerada por cliente \u03a3 Valor Frete por Cliente Mensal Crescimento 10% Ticket M\u00e9dio Valor m\u00e9dio por entrega \u03a3 Valor Frete / N\u00ba Entregas Mensal &gt; R$ 500 Tempo M\u00e9dio Entrega Tempo m\u00e9dio para entrega M\u00e9dia (Data Fim - Data In\u00edcio) Semanal &lt; 48h Taxa de Avarias % entregas com problemas Entregas com Problema / Total \u00d7 100 Mensal &lt; 2% Consumo Combust\u00edvel Litros por 100km (Litros / KM Rodados) \u00d7 100 Mensal &lt; 35L/100km"},{"location":"kpis_metricas/#proximos-desenvolvimentos","title":"\ud83d\ude80 Pr\u00f3ximos Desenvolvimentos","text":""},{"location":"kpis_metricas/#kpis-planejados","title":"\ud83d\udcca KPIs Planejados","text":"<ul> <li>\ud83e\udd16 Predi\u00e7\u00e3o de Demanda: ML para prever volume de entregas</li> <li>\ud83c\udfaf Score de Satisfa\u00e7\u00e3o: Baseado em feedback dos clientes  </li> <li>\ud83c\udf31 Pegada de Carbono: Emiss\u00f5es de CO\u2082 por entrega</li> <li>\ud83d\udcf1 NPS Log\u00edstico: Net Promoter Score espec\u00edfico para log\u00edstica</li> </ul>"},{"location":"kpis_metricas/#analises-avancadas_1","title":"\ud83d\udd2e An\u00e1lises Avan\u00e7adas","text":"<ul> <li>\ud83e\udde0 Machine Learning: Modelos preditivos para otimiza\u00e7\u00e3o de rotas</li> <li>\ud83d\udcc8 Forecasting: Previs\u00e3o de demanda e capacidade</li> <li>\ud83c\udfaf Otimiza\u00e7\u00e3o: Algoritmos para aloca\u00e7\u00e3o \u00f3tima de recursos</li> <li>\ud83d\udcca Real-time Analytics: Dashboards em tempo real</li> </ul> <p>Todos esses KPIs e m\u00e9tricas s\u00e3o calculados automaticamente pelo pipeline ETL e disponibilizados na camada Gold para consumo por ferramentas de Business Intelligence, garantindo insights precisos e atualizados para tomada de decis\u00f5es estrat\u00e9gicas. </p>"},{"location":"otimizacao_spark/","title":"\u26a1 Otimiza\u00e7\u00e3o do Apache Spark","text":""},{"location":"otimizacao_spark/#visao-geral","title":"\ud83d\udccb Vis\u00e3o Geral","text":"<p>A otimiza\u00e7\u00e3o da conex\u00e3o e configura\u00e7\u00e3o do Apache Spark \u00e9 crucial para obter m\u00e1xima performance no processamento de dados. Este guia apresenta as melhores pr\u00e1ticas e configura\u00e7\u00f5es otimizadas para o projeto ETL.</p>"},{"location":"otimizacao_spark/#configuracoes-de-conexao-otimizadas","title":"\ud83d\udd27 Configura\u00e7\u00f5es de Conex\u00e3o Otimizadas","text":""},{"location":"otimizacao_spark/#sparksession-otimizada","title":"SparkSession Otimizada","text":"<pre><code>def create_optimized_spark_session(app_name=\"projeto_etl_spark\"):\n    \"\"\"\n    Cria uma SparkSession otimizada para o projeto ETL\n    \"\"\"\n    return SparkSession.builder \\\n        .appName(app_name) \\\n        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n        .config(\"spark.jars.packages\", \n                \"io.delta:delta-core_2.12:2.3.0,\"\n                \"org.apache.hadoop:hadoop-azure:3.3.6,\"\n                \"org.apache.hadoop:hadoop-common:3.3.6,\"\n                \"com.microsoft.azure:azure-storage:8.6.6\") \\\n        \\\n        .config(\"spark.sql.warehouse.dir\", \"/tmp/spark-warehouse\") \\\n        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n        \\\n        .config(\"spark.driver.memory\", \"4g\") \\\n        .config(\"spark.driver.maxResultSize\", \"2g\") \\\n        .config(\"spark.driver.cores\", \"2\") \\\n        \\\n        .config(\"spark.executor.memory\", \"4g\") \\\n        .config(\"spark.executor.cores\", \"2\") \\\n        .config(\"spark.executor.instances\", \"2\") \\\n        \\\n        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n        .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n        .config(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\") \\\n        \\\n        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n        .config(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"10000\") \\\n        \\\n        .config(\"spark.default.parallelism\", \"8\") \\\n        .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n        \\\n        .config(\"spark.network.timeout\", \"800s\") \\\n        .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n        .config(\"spark.storage.blockManagerSlaveTimeoutMs\", \"600s\") \\\n        \\\n        .getOrCreate()\n</code></pre>"},{"location":"otimizacao_spark/#otimizacoes-por-ambiente","title":"\ud83d\ude80 Otimiza\u00e7\u00f5es por Ambiente","text":""},{"location":"otimizacao_spark/#desenvolvimento-local","title":"Desenvolvimento Local","text":"<pre><code>def spark_config_development():\n    \"\"\"Configura\u00e7\u00e3o otimizada para desenvolvimento local\"\"\"\n    return {\n        # Recursos limitados para desenvolvimento\n        \"spark.driver.memory\": \"2g\",\n        \"spark.executor.memory\": \"2g\", \n        \"spark.executor.cores\": \"1\",\n        \"spark.executor.instances\": \"1\",\n\n        # Paralelismo reduzido\n        \"spark.default.parallelism\": \"4\",\n        \"spark.sql.shuffle.partitions\": \"50\",\n\n        # Logs mais verbosos para debug\n        \"spark.sql.adaptive.logLevel\": \"INFO\",\n        \"spark.eventLog.enabled\": \"true\",\n        \"spark.eventLog.dir\": \"/tmp/spark-events\",\n\n        # Checkpoint local\n        \"spark.sql.streaming.checkpointLocation\": \"/tmp/spark-checkpoints\"\n    }\n</code></pre>"},{"location":"otimizacao_spark/#producaoazure","title":"Produ\u00e7\u00e3o/Azure","text":"<pre><code>def spark_config_production():\n    \"\"\"Configura\u00e7\u00e3o otimizada para produ\u00e7\u00e3o no Azure\"\"\"\n    return {\n        # Recursos maximizados\n        \"spark.driver.memory\": \"8g\",\n        \"spark.driver.maxResultSize\": \"4g\",\n        \"spark.executor.memory\": \"8g\",\n        \"spark.executor.cores\": \"4\", \n        \"spark.executor.instances\": \"4\",\n\n        # Paralelismo alto\n        \"spark.default.parallelism\": \"16\",\n        \"spark.sql.shuffle.partitions\": \"400\",\n\n        # Otimiza\u00e7\u00f5es de rede para Azure\n        \"spark.network.timeout\": \"1200s\",\n        \"spark.sql.broadcastTimeout\": \"1200s\",\n\n        # Delta Lake otimiza\u00e7\u00f5es\n        \"spark.databricks.delta.optimizeWrite.enabled\": \"true\",\n        \"spark.databricks.delta.autoCompact.enabled\": \"true\",\n\n        # Cache otimizado\n        \"spark.sql.cache.serializer\": \"org.apache.spark.sql.execution.columnar.InMemoryTableScanExec\",\n        \"spark.sql.columnVector.offheap.enabled\": \"true\"\n    }\n</code></pre>"},{"location":"otimizacao_spark/#integracao-com-azure-otimizada","title":"\u2601\ufe0f Integra\u00e7\u00e3o com Azure Otimizada","text":""},{"location":"otimizacao_spark/#configuracao-azure-storage","title":"Configura\u00e7\u00e3o Azure Storage","text":"<pre><code>def configure_azure_storage(spark, account_name, sas_token, containers):\n    \"\"\"\n    Configura conex\u00e3o otimizada com Azure Storage\n    \"\"\"\n    # Configura\u00e7\u00e3o base do Azure\n    spark.conf.set(f\"fs.azure.account.auth.type.{account_name}.dfs.core.windows.net\", \"SAS\")\n    spark.conf.set(f\"fs.azure.sas.token.provider.type.{account_name}.dfs.core.windows.net\", \n                   \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n\n    # Configurar SAS token para cada container\n    for container in containers:\n        spark.conf.set(f\"fs.azure.sas.{container}.{account_name}.blob.core.windows.net\", sas_token)\n\n    # Otimiza\u00e7\u00f5es espec\u00edficas do Azure\n    spark.conf.set(\"fs.azure.io.retry.max.retries\", \"10\")\n    spark.conf.set(\"fs.azure.io.retry.backoff.interval\", \"3s\")\n    spark.conf.set(\"fs.azure.block.size\", \"268435456\")  # 256MB\n    spark.conf.set(\"fs.azure.write.request.size\", \"67108864\")  # 64MB\n\n    # Buffer otimizado\n    spark.conf.set(\"fs.azure.read.request.size\", \"67108864\")  # 64MB\n    spark.conf.set(\"fs.azure.account.keyprovider.{account_name}.dfs.core.windows.net\", \n                   \"org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider\")\n\n# Exemplo de uso\nspark = create_optimized_spark_session()\nconfigure_azure_storage(spark, \"seuaccount\", \"seu_sas_token\", \n                       [\"landing\", \"bronze\", \"silver\", \"gold\"])\n</code></pre>"},{"location":"otimizacao_spark/#pool-de-conexoes-azure","title":"Pool de Conex\u00f5es Azure","text":"<pre><code>class AzureConnectionPool:\n    \"\"\"Pool de conex\u00f5es reutiliz\u00e1veis para Azure\"\"\"\n\n    def __init__(self, account_name, sas_token, max_connections=10):\n        self.account_name = account_name\n        self.sas_token = sas_token\n        self.max_connections = max_connections\n        self._pool = []\n        self._active_connections = 0\n\n    def get_blob_client(self, container_name):\n        \"\"\"Obt\u00e9m cliente blob do pool\"\"\"\n        if self._pool and self._active_connections &lt; self.max_connections:\n            return self._pool.pop()\n\n        from azure.storage.blob import BlobServiceClient\n        client = BlobServiceClient(\n            account_url=f\"https://{self.account_name}.blob.core.windows.net\",\n            credential=self.sas_token\n        )\n        self._active_connections += 1\n        return client.get_container_client(container_name)\n\n    def return_client(self, client):\n        \"\"\"Retorna cliente para o pool\"\"\"\n        if len(self._pool) &lt; self.max_connections:\n            self._pool.append(client)\n        self._active_connections -= 1\n</code></pre>"},{"location":"otimizacao_spark/#otimizacoes-de-performance","title":"\ud83d\udd04 Otimiza\u00e7\u00f5es de Performance","text":""},{"location":"otimizacao_spark/#adaptive-query-execution-aqe","title":"Adaptive Query Execution (AQE)","text":"<pre><code>def enable_aqe_optimizations(spark):\n    \"\"\"Habilita otimiza\u00e7\u00f5es do Adaptive Query Execution\"\"\"\n\n    # AQE Principal\n    spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n    spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n\n    # Otimiza\u00e7\u00e3o de Joins\n    spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n    spark.conf.set(\"spark.sql.adaptive.skewJoin.skewedPartitionFactor\", \"5\")\n    spark.conf.set(\"spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes\", \"256MB\")\n\n    # Local Shuffle Reader\n    spark.conf.set(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\")\n\n    # Broadcast Join Threshold\n    spark.conf.set(\"spark.sql.adaptive.autoBroadcastJoinThreshold\", \"50MB\")\n\n    # Coalesce Partitions\n    spark.conf.set(\"spark.sql.adaptive.coalescePartitions.minPartitionNum\", \"1\")\n    spark.conf.set(\"spark.sql.adaptive.coalescePartitions.initialPartitionNum\", \"200\")\n\n    # Advisory Partition Size\n    spark.conf.set(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"128MB\")\n</code></pre>"},{"location":"otimizacao_spark/#cache-inteligente","title":"Cache Inteligente","text":"<pre><code>class SmartCache:\n    \"\"\"Cache inteligente para DataFrames frequentemente acessados\"\"\"\n\n    def __init__(self, spark):\n        self.spark = spark\n        self.cached_dfs = {}\n        self.access_count = {}\n\n    def cache_if_beneficial(self, df, key, threshold=2):\n        \"\"\"Cache DataFrame se for acessado frequentemente\"\"\"\n        self.access_count[key] = self.access_count.get(key, 0) + 1\n\n        if self.access_count[key] &gt;= threshold and key not in self.cached_dfs:\n            # Cache com storage level otimizado\n            df.cache()\n            df.count()  # For\u00e7a materializa\u00e7\u00e3o\n            self.cached_dfs[key] = df\n            logger.info(f\"DataFrame {key} foi cacheado ap\u00f3s {self.access_count[key]} acessos\")\n\n        return self.cached_dfs.get(key, df)\n\n    def clear_cache(self):\n        \"\"\"Limpa cache de todos os DataFrames\"\"\"\n        for key, df in self.cached_dfs.items():\n            df.unpersist()\n        self.cached_dfs.clear()\n        self.access_count.clear()\n\n# Exemplo de uso\ncache_manager = SmartCache(spark)\ndf_clientes = cache_manager.cache_if_beneficial(df_clientes, \"clientes\")\n</code></pre>"},{"location":"otimizacao_spark/#particionamento-otimizado","title":"Particionamento Otimizado","text":"<pre><code>def optimize_partitioning(df, partition_cols, target_partition_size=\"128MB\"):\n    \"\"\"\n    Otimiza particionamento baseado no tamanho dos dados\n    \"\"\"\n    # Calcular n\u00famero ideal de parti\u00e7\u00f5es\n    df_size_bytes = df.rdd.map(lambda x: len(str(x))).sum()\n    target_size_bytes = int(target_partition_size.replace(\"MB\", \"\")) * 1024 * 1024\n    optimal_partitions = max(1, df_size_bytes // target_size_bytes)\n\n    # Reparticionar se necess\u00e1rio\n    current_partitions = df.rdd.getNumPartitions()\n    if current_partitions != optimal_partitions:\n        if partition_cols:\n            df = df.repartition(optimal_partitions, *partition_cols)\n        else:\n            df = df.repartition(optimal_partitions)\n\n    return df\n\n# Exemplo de uso\ndf_entregas_optimized = optimize_partitioning(\n    df_entregas, \n    [\"ano\", \"mes\"], \n    target_partition_size=\"256MB\"\n)\n</code></pre>"},{"location":"otimizacao_spark/#otimizacoes-de-io","title":"\ud83d\udcbe Otimiza\u00e7\u00f5es de I/O","text":""},{"location":"otimizacao_spark/#leitura-otimizada","title":"Leitura Otimizada","text":"<pre><code>def read_delta_optimized(spark, path, filters=None, columns=None):\n    \"\"\"\n    Leitura otimizada de tabelas Delta\n    \"\"\"\n    reader = spark.read.format(\"delta\")\n\n    # Predicate pushdown\n    if filters:\n        for filter_expr in filters:\n            reader = reader.filter(filter_expr)\n\n    # Column pruning\n    if columns:\n        df = reader.load(path).select(*columns)\n    else:\n        df = reader.load(path)\n\n    # Z-Order optimization hint\n    if hasattr(df, 'hint'):\n        df = df.hint(\"Z_ORDER\", [\"data_processamento\"])\n\n    return df\n\n# Exemplo de uso\ndf_entregas = read_delta_optimized(\n    spark,\n    \"wasbs://silver@account.blob.core.windows.net/entregas\",\n    filters=[\"data_entrega &gt;= '2024-01-01'\"],\n    columns=[\"id_entrega\", \"valor_frete\", \"status_entrega\"]\n)\n</code></pre>"},{"location":"otimizacao_spark/#escrita-otimizada","title":"Escrita Otimizada","text":"<pre><code>def write_delta_optimized(df, path, partition_cols=None, mode=\"overwrite\"):\n    \"\"\"\n    Escrita otimizada para tabelas Delta\n    \"\"\"\n    writer = df.write.format(\"delta\").mode(mode)\n\n    # Configura\u00e7\u00f5es de otimiza\u00e7\u00e3o\n    writer = writer.option(\"overwriteSchema\", \"true\") \\\n                   .option(\"mergeSchema\", \"true\") \\\n                   .option(\"optimizeWrite\", \"true\") \\\n                   .option(\"autoCompact\", \"true\")\n\n    # Particionamento se especificado\n    if partition_cols:\n        writer = writer.partitionBy(*partition_cols)\n\n    # Configurar tamanho de arquivo alvo\n    spark.conf.set(\"spark.databricks.delta.targetFileSize\", \"134217728\")  # 128MB\n\n    writer.save(path)\n\n    # Otimiza\u00e7\u00e3o p\u00f3s-escrita\n    spark.sql(f\"OPTIMIZE delta.`{path}` ZORDER BY (data_processamento)\")\n\n# Exemplo de uso\nwrite_delta_optimized(\n    df_silver,\n    \"wasbs://silver@account.blob.core.windows.net/clientes\",\n    partition_cols=[\"ano\", \"mes\"]\n)\n</code></pre>"},{"location":"otimizacao_spark/#monitoramento-de-performance","title":"\ud83d\udd0d Monitoramento de Performance","text":""},{"location":"otimizacao_spark/#metricas-de-spark","title":"M\u00e9tricas de Spark","text":"<pre><code>class SparkMetricsCollector:\n    \"\"\"Coleta m\u00e9tricas de performance do Spark\"\"\"\n\n    def __init__(self, spark):\n        self.spark = spark\n        self.metrics = {}\n\n    def collect_job_metrics(self, job_description):\n        \"\"\"Coleta m\u00e9tricas de um job espec\u00edfico\"\"\"\n        sc = self.spark.sparkContext\n\n        # M\u00e9tricas b\u00e1sicas\n        self.metrics[job_description] = {\n            \"active_jobs\": len(sc.statusTracker().getActiveJobIds()),\n            \"active_stages\": len(sc.statusTracker().getActiveStageIds()),\n            \"executors\": len(sc.statusTracker().getExecutorInfos()),\n            \"total_cores\": sum(e.totalCores for e in sc.statusTracker().getExecutorInfos()),\n            \"memory_used\": sum(e.memoryUsed for e in sc.statusTracker().getExecutorInfos()),\n            \"memory_total\": sum(e.maxMemory for e in sc.statusTracker().getExecutorInfos())\n        }\n\n    def log_performance_summary(self):\n        \"\"\"Log resumo de performance\"\"\"\n        for job, metrics in self.metrics.items():\n            memory_usage_pct = (metrics[\"memory_used\"] / metrics[\"memory_total\"]) * 100\n            logger.info(f\"\"\"\n            Job: {job}\n            Executors: {metrics[\"executors\"]}\n            Cores: {metrics[\"total_cores\"]}\n            Memory Usage: {memory_usage_pct:.1f}%\n            \"\"\")\n\n# Exemplo de uso\nmetrics_collector = SparkMetricsCollector(spark)\nmetrics_collector.collect_job_metrics(\"bronze_processing\")\n</code></pre>"},{"location":"otimizacao_spark/#alertas-de-performance","title":"Alertas de Performance","text":"<pre><code>def setup_performance_alerts(spark):\n    \"\"\"Configura alertas de performance\"\"\"\n\n    def check_memory_usage():\n        sc = spark.sparkContext\n        executors = sc.statusTracker().getExecutorInfos()\n\n        for executor in executors:\n            if executor.maxMemory &gt; 0:\n                usage_pct = (executor.memoryUsed / executor.maxMemory) * 100\n                if usage_pct &gt; 90:\n                    logger.warning(f\"Executor {executor.executorId} usando {usage_pct:.1f}% da mem\u00f3ria\")\n\n    def check_failed_tasks():\n        sc = spark.sparkContext\n        for stage_id in sc.statusTracker().getActiveStageIds():\n            stage_info = sc.statusTracker().getStageInfo(stage_id)\n            if stage_info and stage_info.numFailedTasks &gt; 0:\n                logger.error(f\"Stage {stage_id} tem {stage_info.numFailedTasks} tasks falhadas\")\n\n    # Executar verifica\u00e7\u00f5es periodicamente\n    import threading\n    import time\n\n    def monitor():\n        while True:\n            check_memory_usage()\n            check_failed_tasks()\n            time.sleep(30)  # Verificar a cada 30 segundos\n\n    monitor_thread = threading.Thread(target=monitor, daemon=True)\n    monitor_thread.start()\n</code></pre>"},{"location":"otimizacao_spark/#configuracao-por-tipo-de-workload","title":"\ud83d\udee0\ufe0f Configura\u00e7\u00e3o por Tipo de Workload","text":""},{"location":"otimizacao_spark/#etl-batch-processing","title":"ETL Batch Processing","text":"<pre><code>def spark_config_etl_batch():\n    \"\"\"Configura\u00e7\u00e3o otimizada para processamento ETL em lote\"\"\"\n    return {\n        # Mem\u00f3ria alta para processamento de grandes volumes\n        \"spark.executor.memory\": \"8g\",\n        \"spark.driver.memory\": \"4g\",\n\n        # Mais parti\u00e7\u00f5es para paralelismo\n        \"spark.sql.shuffle.partitions\": \"400\",\n        \"spark.default.parallelism\": \"200\",\n\n        # Timeout maior para jobs longos\n        \"spark.network.timeout\": \"1800s\",\n        \"spark.sql.broadcastTimeout\": \"1800s\",\n\n        # Compress\u00e3o para economizar I/O\n        \"spark.sql.parquet.compression.codec\": \"snappy\",\n        \"spark.sql.orc.compression.codec\": \"snappy\",\n\n        # Otimiza\u00e7\u00f5es de serializa\u00e7\u00e3o\n        \"spark.serializer\": \"org.apache.spark.serializer.KryoSerializer\",\n        \"spark.kryo.unsafe\": \"true\",\n        \"spark.kryo.registrationRequired\": \"false\"\n    }\n</code></pre>"},{"location":"otimizacao_spark/#real-time-streaming","title":"Real-time Streaming","text":"<pre><code>def spark_config_streaming():\n    \"\"\"Configura\u00e7\u00e3o otimizada para streaming\"\"\"\n    return {\n        # Mem\u00f3ria menor, mais executors\n        \"spark.executor.memory\": \"2g\",\n        \"spark.executor.cores\": \"2\",\n        \"spark.executor.instances\": \"8\",\n\n        # Baixa lat\u00eancia\n        \"spark.streaming.blockInterval\": \"50ms\",\n        \"spark.streaming.receiver.maxRate\": \"10000\",\n\n        # Checkpoint frequente\n        \"spark.sql.streaming.checkpointLocation\": \"/tmp/streaming-checkpoints\",\n        \"spark.sql.streaming.minBatchesToRetain\": \"5\",\n\n        # Buffer menor para baixa lat\u00eancia\n        \"spark.sql.shuffle.partitions\": \"100\"\n    }\n</code></pre>"},{"location":"otimizacao_spark/#benchmark-e-tuning","title":"\ud83d\udcca Benchmark e Tuning","text":""},{"location":"otimizacao_spark/#script-de-benchmark","title":"Script de Benchmark","text":"<pre><code>def benchmark_spark_config(spark, df, operation_name):\n    \"\"\"Executa benchmark de uma opera\u00e7\u00e3o Spark\"\"\"\n    import time\n\n    start_time = time.time()\n\n    # For\u00e7a execu\u00e7\u00e3o com count()\n    if hasattr(df, 'count'):\n        result_count = df.count()\n    else:\n        result_count = \"N/A\"\n\n    end_time = time.time()\n    execution_time = end_time - start_time\n\n    # Coleta m\u00e9tricas do Spark\n    sc = spark.sparkContext\n    executors = sc.statusTracker().getExecutorInfos()\n\n    metrics = {\n        \"operation\": operation_name,\n        \"execution_time\": execution_time,\n        \"record_count\": result_count,\n        \"records_per_second\": result_count / execution_time if execution_time &gt; 0 else 0,\n        \"num_executors\": len(executors),\n        \"total_memory\": sum(e.maxMemory for e in executors),\n        \"memory_used\": sum(e.memoryUsed for e in executors)\n    }\n\n    logger.info(f\"\"\"\n    Benchmark Results for {operation_name}:\n    - Execution Time: {execution_time:.2f}s\n    - Records Processed: {result_count:,}\n    - Records/Second: {metrics['records_per_second']:,.0f}\n    - Memory Usage: {(metrics['memory_used']/metrics['total_memory']*100):.1f}%\n    \"\"\")\n\n    return metrics\n\n# Exemplo de uso\nbenchmark_spark_config(spark, df_bronze_processing, \"Bronze Layer Processing\")\n</code></pre>"},{"location":"otimizacao_spark/#configuracao-dinamica","title":"\u2699\ufe0f Configura\u00e7\u00e3o Din\u00e2mica","text":"<pre><code>class DynamicSparkConfig:\n    \"\"\"Configura\u00e7\u00e3o din\u00e2mica baseada no ambiente e workload\"\"\"\n\n    def __init__(self):\n        self.configs = {}\n\n    def auto_configure(self, data_size_gb, operation_type=\"etl\"):\n        \"\"\"Configura\u00e7\u00e3o autom\u00e1tica baseada no tamanho dos dados\"\"\"\n\n        if data_size_gb &lt; 1:\n            # Dados pequenos\n            memory_per_executor = \"2g\"\n            num_executors = 1\n            shuffle_partitions = 50\n        elif data_size_gb &lt; 10:\n            # Dados m\u00e9dios\n            memory_per_executor = \"4g\"\n            num_executors = 2\n            shuffle_partitions = 200\n        else:\n            # Dados grandes\n            memory_per_executor = \"8g\"\n            num_executors = 4\n            shuffle_partitions = 400\n\n        self.configs = {\n            \"spark.executor.memory\": memory_per_executor,\n            \"spark.executor.instances\": str(num_executors),\n            \"spark.sql.shuffle.partitions\": str(shuffle_partitions),\n            \"spark.default.parallelism\": str(shuffle_partitions // 2)\n        }\n\n        return self.configs\n\n    def apply_to_spark(self, spark):\n        \"\"\"Aplica configura\u00e7\u00f5es ao SparkSession\"\"\"\n        for key, value in self.configs.items():\n            spark.conf.set(key, value)\n\n# Exemplo de uso\nconfig_manager = DynamicSparkConfig()\noptimal_config = config_manager.auto_configure(data_size_gb=5.2)\nconfig_manager.apply_to_spark(spark)\n</code></pre> <p>Essas otimiza\u00e7\u00f5es podem melhorar significativamente a performance do seu pipeline Spark, especialmente quando processando grandes volumes de dados no Azure Data Lake. Lembre-se de sempre fazer benchmark das configura\u00e7\u00f5es em seu ambiente espec\u00edfico para encontrar os valores ideais. </p>"},{"location":"pipeline_etl/","title":"Pipeline ETL","text":""},{"location":"pipeline_etl/#etapas","title":"Etapas:","text":"<ol> <li>Extra\u00e7\u00e3o: SQL Server</li> <li>Carga: Azure Data Lake (camada landing)</li> <li>Transforma\u00e7\u00e3o: Spark com Delta Lake e Iceberg</li> <li>Carga Final: Camada gold</li> </ol>"},{"location":"pipeline_etl/#ferramentas","title":"Ferramentas:","text":"<ul> <li>Airflow (orquestra\u00e7\u00e3o)</li> <li>Spark (processamento)</li> <li>Azure Data Lake (armazenamento)</li> </ul>"},{"location":"uso/","title":"Como Usar","text":""},{"location":"uso/#executando-o-pipeline","title":"Executando o pipeline:","text":"<ol> <li>Configure as credenciais do Azure.</li> <li>Inicie o Airflow com <code>docker-compose up</code>.</li> <li>Acesse o Airflow em <code>localhost:8080</code>.</li> <li>Execute o DAG <code>sqlserver_to_adls_dag</code>.</li> </ol>"},{"location":"uso/#executando-scripts-spark","title":"Executando scripts Spark:","text":"<pre><code>spark-submit data/create_schema_and_columns.py\n</code></pre>"}]}