# ğŸ—ï¸ System Architecture

## ğŸ“‹ Overview

The project implements a **modern data architecture** based on the **Medallion Architecture** methodology (Bronze, Silver, Gold), using best practices for large-scale data processing. The solution is **cloud-native**, **scalable**, and **resilient**.

---

## ğŸ¯ Architectural Principles

### ğŸ”§ **Separation of Concerns**
Each layer has well-defined responsibilities, facilitating maintenance and evolution.

### âš¡ **Scalability First**
Architecture prepared for horizontal and vertical growth according to demand.

### ğŸ›¡ï¸ **Fault Tolerance**
Retry mechanisms, checkpointing, and automatic recovery.

### ğŸ“Š **Data Quality**
Validations and transformations in each layer to ensure data quality.

### ğŸ”’ **Security by Design**
Encryption, access control, and auditing in all components.

---

## ğŸ›ï¸ General Architecture

```mermaid
graph TB
    subgraph "Data Sources"
        SQL[SQL Server<br/>Transactional Database]
    end
    
    subgraph "Orchestration Layer"
        AF[Apache Airflow<br/>Workflow Management]
        AF_DB[(Airflow<br/>Metadata DB)]
        AF --> AF_DB
    end
    
    subgraph "Azure Data Lake - Medallion Architecture"
        LZ[Landing Zone<br/>Raw CSV Files]
        
        subgraph "Bronze Layer"
            BR_DT[Delta Tables<br/>Raw Data]
            BR_META[Metadata<br/>Lineage]
        end
        
        subgraph "Silver Layer"
            SV_DT[Delta Tables<br/>Clean Data]
            SV_QUAL[Data Quality<br/>Validations]
        end
        
        subgraph "Gold Layer"
            GD_DIM[Dimension Tables<br/>Master Data]
            GD_FACT[Fact Tables<br/>Transactional Data]
            GD_KPI[KPI Tables<br/>Aggregated Metrics]
        end
    end
    
    subgraph "Processing Engine"
        SPARK[Apache Spark<br/>Distributed Processing]
        DELTA[Delta Lake<br/>ACID Transactions]
        SPARK --> DELTA
    end
    
    subgraph "Infrastructure"
        DOCKER[Docker Containers<br/>Airflow Environment]
        AZURE[Azure Cloud<br/>Storage & Compute]
    end
    
    subgraph "Analytics & Consumption"
        BI[Power BI<br/>Dashboards]
        API[REST API<br/>Data Access]
        EXPORT[Data Export<br/>CSV/Parquet]
    end
    
    SQL -->|Extract| AF
    AF -->|Orchestrate| LZ
    LZ -->|Ingest| BR_DT
    BR_DT -->|Transform| SV_DT
    SV_DT -->|Aggregate| GD_DIM
    SV_DT -->|Aggregate| GD_FACT
    GD_FACT -->|Calculate| GD_KPI
    
    SPARK -.->|Process| BR_DT
    SPARK -.->|Process| SV_DT
    SPARK -.->|Process| GD_DIM
    SPARK -.->|Process| GD_FACT
    
    GD_KPI -->|Consume| BI
    GD_FACT -->|Query| API
    GD_DIM -->|Export| EXPORT
    
    DOCKER -.->|Host| AF
    AZURE -.->|Store| LZ
    AZURE -.->|Store| BR_DT
    AZURE -.->|Store| SV_DT
    AZURE -.->|Store| GD_DIM
```

---

## ğŸ“Š Medallion Architecture

### ğŸ” **Landing Zone**
**Purpose**: Initial staging of raw extracted data

- **Format**: CSV files
- **Retention**: 30 days
- **Partitioning**: By extraction date
- **Schema**: Schema-on-read

```
landing/
â”œâ”€â”€ customers_20241201_143022.csv
â”œâ”€â”€ drivers_20241201_143025.csv
â”œâ”€â”€ vehicles_20241201_143028.csv
â””â”€â”€ deliveries_20241201_143030.csv
```

### ğŸ¥‰ **Bronze Layer (Raw Data)**
**Purpose**: Complete and immutable historical storage

- **Format**: Delta Lake tables
- **Schema**: Preserves original structure + metadata
- **Partitioning**: By year/month/day
- **Retention**: Unlimited (historical data)

**Features:**
- âœ… ACID transactions via Delta Lake
- âœ… Time travel and versioning
- âœ… Ingestion metadata
- âœ… Automatic compaction

```python
# Bronze Structure
bronze_schema = {
    "original_columns": "preserved_as_extracted",
    "processing_date": "processing_date",
    "processing_timestamp": "processing_timestamp", 
    "source_file_name": "source_file_name",
    "_bronze_ingestion_id": "unique_ingestion_id"
}
```

### ğŸ¥ˆ **Silver Layer (Clean Data)**
**Purpose**: Clean, standardized, and enriched data

- **Format**: Delta Lake tables
- **Schema**: Standardized and normalized
- **Quality**: Applied validations and cleansing
- **Partitioning**: By business date

**Applied Transformations:**
- ğŸ§¹ **Data Cleansing**: Duplicate removal, null values
- ğŸ“ **Standardization**: Format and type standardization
- ğŸ”¤ **Normalization**: Uppercase conversion, trim
- âœ… **Validation**: Business rules and quality
- ğŸ·ï¸ **Enrichment**: Silver metadata addition

```python
# Example of Silver transformations
def silver_transformations(df):
    return df \
        .dropDuplicates() \
        .withColumn("customer_name", upper(trim(col("customer_name")))) \
        .withColumn("cpf_cnpj", regexp_replace(col("cpf_cnpj"), "[^0-9]", "")) \
        .withColumn("_silver_ingestion_timestamp", current_timestamp()) \
        .withColumn("_source_table", lit(table_name))
```

### ğŸ¥‡ **Gold Layer (Business Data)**
**Purpose**: Dimensional model for analytics and BI

- **Format**: Delta Lake tables
- **Schema**: Star schema / Snowflake
- **Optimization**: Indexes and partitioning for queries
- **Aggregations**: Pre-calculated KPIs and metrics

**Dimensional Structure:**

```mermaid
erDiagram
    Fact_Deliveries ||--|| Dim_Date : "delivery_date"
    Fact_Deliveries ||--|| Dim_Customer_Sender : "sender"
    Fact_Deliveries ||--|| Dim_Customer_Recipient : "recipient"
    Fact_Deliveries ||--|| Dim_Driver : "driver"
    Fact_Deliveries ||--|| Dim_Vehicle : "vehicle"
    Fact_Deliveries ||--|| Dim_Route : "route"
    Fact_Deliveries ||--|| Dim_Cargo_Type : "cargo_type"
    
    Fact_Deliveries {
        bigint delivery_id_key PK
        int start_date_key FK
        int estimated_date_key FK
        int end_date_key FK
        int sender_customer_key FK
        int recipient_customer_key FK
        int driver_key FK
        int vehicle_key FK
        int route_key FK
        int cargo_type_key FK
        decimal freight_value
        decimal cargo_weight_kg
        string delivery_status
    }
```

---

## âš™ï¸ Technical Components

### ğŸ›ï¸ **Apache Airflow**
**Responsibility**: Orchestration and scheduling

```python
# Optimized configuration
AIRFLOW_CONFIG = {
    "executor": "LocalExecutor",
    "max_active_runs": 1,
    "max_active_tasks": 4,
    "catchup": False,
    "depends_on_past": False,
    "retries": 3,
    "retry_delay": timedelta(minutes=5)
}
```

**Main DAG:**
```python
@dag(
    dag_id="medallion_architecture_etl",
    schedule="0 2 * * *",  # Daily at 2 AM
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=["etl", "production"]
)
def etl_pipeline():
    extract_task = extract_from_sqlserver()
    bronze_task = process_bronze_layer()
    silver_task = process_silver_layer()
    gold_task = process_gold_layer()
    
    extract_task >> bronze_task >> silver_task >> gold_task
```

### âš¡ **Apache Spark**
**Responsibility**: Distributed data processing

**Optimized Configurations:**
```python
spark_config = {
    # Core Settings
    "spark.app.name": "etl_project_spark",
    "spark.sql.adaptive.enabled": "true",
    "spark.sql.adaptive.coalescePartitions.enabled": "true",
    
    # Delta Lake
    "spark.sql.extensions": "io.delta.sql.DeltaSparkSessionExtension",
    "spark.sql.catalog.spark_catalog": "org.apache.spark.sql.delta.catalog.DeltaCatalog",
    
    # Memory Management
    "spark.driver.memory": "4g",
    "spark.executor.memory": "4g",
    "spark.driver.maxResultSize": "2g",
    
    # Performance
    "spark.serializer": "org.apache.spark.serializer.KryoSerializer",
    "spark.sql.adaptive.skewJoin.enabled": "true",
    
    # Azure Integration
    "spark.hadoop.fs.azure.account.auth.type": "SAS",
    "spark.hadoop.fs.azure.sas.token.provider.type": "org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider"
}
```

### ğŸ”º **Delta Lake**
**Responsibility**: ACID transactions and versioning

**Benefits:**
- âœ… **ACID Transactions**: Consistency in concurrent operations
- âœ… **Time Travel**: Access to historical versions
- âœ… **Schema Evolution**: Schema changes without breaking compatibility
- âœ… **Merge Operations**: Efficient upserts
- âœ… **Automatic Compaction**: Automatic file optimization

```python
# Example of merge operation
delta_table = DeltaTable.forPath(spark, gold_path)
delta_table.alias("target") \
    .merge(updates_df.alias("source"), "target.id = source.id") \
    .whenMatchedUpdateAll() \
    .whenNotMatchedInsertAll() \
    .execute()
```

---

## â˜ï¸ Azure Infrastructure

### ğŸ—„ï¸ **Azure Data Lake Storage Gen2**
**Configuration:**
- **Replication**: LRS (Locally Redundant Storage)
- **Access Tier**: Hot (frequently accessed data)
- **Hierarchical Namespace**: Enabled
- **Security**: SAS tokens with granular permissions

**Container Structure:**
```
adlsaccount/
â”œâ”€â”€ landing/          # Raw CSV files
â”œâ”€â”€ bronze/           # Delta tables - raw data
â”œâ”€â”€ silver/           # Delta tables - clean data
â”œâ”€â”€ gold/             # Delta tables - dimensional model
â””â”€â”€ checkpoints/      # Spark checkpoints
```

### ğŸ” **Security**
- **Authentication**: SAS Tokens with expiration
- **Authorization**: RBAC in Azure AD
- **Encryption**: At rest and in transit
- **Network**: Private endpoints (when necessary)

---

## ğŸ”„ Detailed Data Flow

### 1. **Extraction (SQL Server â†’ Landing Zone)**
```python
def extract_from_sqlserver():
    # Optimized connection with connection pool
    engine = create_engine(
        connection_string,
        pool_size=10,
        pool_recycle=3600,
        pool_pre_ping=True
    )
    
    # Incremental extraction based on timestamp
    query = f"""
    SELECT * FROM {schema}.{table} 
    WHERE last_modified >= '{last_extraction_time}'
    """
    
    df = pd.read_sql(query, engine)
    
    # Upload to ADLS with timestamp
    filename = f"{table}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
    upload_to_adls(df, filename)
```

### 2. **Bronze Processing (Landing â†’ Bronze)**
```python
def process_bronze_layer():
    # Read CSVs with schema inference
    df = spark.read \
        .option("header", "true") \
        .option("inferSchema", "true") \
        .csv(landing_path)
    
    # Add metadata
    df_bronze = df \
        .withColumn("processing_date", current_date()) \
        .withColumn("processing_timestamp", current_timestamp()) \
        .withColumn("source_file_name", input_file_name())
    
    # Write in Delta format
    df_bronze.write \
        .format("delta") \
        .mode("append") \
        .option("mergeSchema", "true") \
        .save(bronze_path)
```

### 3. **Silver Processing (Bronze â†’ Silver)**
```python
def process_silver_layer():
    # Read from Bronze layer
    df_bronze = spark.read.format("delta").load(bronze_path)
    
    # Apply quality transformations
    df_silver = df_bronze \
        .dropDuplicates() \
        .filter(col("id").isNotNull()) \
        .withColumn("name", upper(trim(col("name")))) \
        .withColumn("cpf", regexp_replace(col("cpf"), "[^0-9]", ""))
    
    # Business validations
    df_validated = apply_business_rules(df_silver)
    
    # Write with partitioning
    df_validated.write \
        .format("delta") \
        .mode("overwrite") \
        .partitionBy("year", "month") \
        .save(silver_path)
```

### 4. **Gold Processing (Silver â†’ Gold)**
```python
def process_gold_layer():
    # Create dimensions
    create_dimensions()
    
    # Create fact tables
    create_fact_tables()
    
    # Calculate KPIs
    calculate_kpis()
```

---

## ğŸ“Š Partitioning Strategy

### **Bronze Layer**
```
bronze/
â””â”€â”€ customers/
    â””â”€â”€ year=2024/
        â””â”€â”€ month=12/
            â””â”€â”€ day=01/
                â”œâ”€â”€ part-00000.parquet
                â””â”€â”€ _delta_log/
```

### **Silver Layer**
```
silver/
â””â”€â”€ customers/
    â””â”€â”€ year=2024/
        â””â”€â”€ month=12/
            â”œâ”€â”€ part-00000.parquet
            â””â”€â”€ _delta_log/
```

### **Gold Layer**
```
gold/
â”œâ”€â”€ dim_customer/
â”‚   â””â”€â”€ part-00000.parquet
â”œâ”€â”€ dim_date/
â”‚   â””â”€â”€ part-00000.parquet
â””â”€â”€ fact_deliveries/
    â””â”€â”€ delivery_date=2024-12-01/
        â””â”€â”€ part-00000.parquet
```

---

## ğŸ” Monitoring and Observability

### **Collected Metrics**
- â±ï¸ **Performance**: Execution time per task
- ğŸ“Š **Volume**: Number of records processed
- ğŸ’¾ **Storage**: Data size per layer
- âŒ **Errors**: Failure rate and error types
- ğŸ”„ **Throughput**: Records processed per second

### **Configured Alerts**
- ğŸš¨ **DAG execution failure**
- â° **SLA breach** (execution > 2 hours)
- ğŸ“ˆ **Anomalous data volume**
- ğŸ’¾ **Low disk space**

### **Dashboards**
- ğŸ“Š **Airflow UI**: Execution status
- ğŸ“ˆ **Spark UI**: Job performance
- â˜ï¸ **Azure Monitor**: Infrastructure metrics

---

## ğŸ”„ Backup and Recovery Strategy

### **Backup**
- ğŸ“… **Daily**: Automatic snapshot of Gold tables
- ğŸ“… **Weekly**: Complete backup of Silver and Bronze layers
- ğŸ“… **Monthly**: Archive to long-term storage

### **Recovery**
- ğŸ”„ **Time Travel**: Delta Lake allows point-in-time recovery
- ğŸ“‹ **Replay**: Re-execution of DAGs from any date
- ğŸ”„ **Rollback**: Reversion to previous version in case of issues

---

## ğŸš€ Scalability

### **Horizontal Scaling**
- **Spark**: Increase number of executors
- **Airflow**: Multiple workers
- **Azure**: Auto-scaling based on demand

### **Vertical Scaling**
- **Memory**: Increase memory per executor
- **CPU**: More cores per machine
- **Storage**: Upgrade to premium SSDs

### **Optimizations**
- **Caching**: Frequently accessed data
- **Indexing**: Z-Order indexes in Delta Lake
- **Compaction**: Automatic optimization of small files

---

This architecture ensures **high availability**, **scalability**, and **maintainability**, following industry best practices for modern data pipelines.