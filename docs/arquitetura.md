# ğŸ—ï¸ Arquitetura do Sistema

## ğŸ“‹ VisÃ£o Geral

O projeto implementa uma **arquitetura moderna de dados** baseada na metodologia **Medallion Architecture** (Bronze, Silver, Gold), utilizando as melhores prÃ¡ticas para processamento de dados em larga escala. A soluÃ§Ã£o Ã© **cloud-native**, **escalÃ¡vel** e **resiliente**.

---

## ğŸ¯ PrincÃ­pios Arquiteturais

### ğŸ”§ **Separation of Concerns**
Cada camada tem responsabilidades bem definidas, facilitando manutenÃ§Ã£o e evoluÃ§Ã£o.

### âš¡ **Scalability First**
Arquitetura preparada para crescimento horizontal e vertical conforme demanda.

### ğŸ›¡ï¸ **Fault Tolerance**
Mecanismos de retry, checkpointing e recuperaÃ§Ã£o automÃ¡tica.

### ğŸ“Š **Data Quality**
ValidaÃ§Ãµes e transformaÃ§Ãµes em cada camada para garantir qualidade dos dados.

### ğŸ”’ **Security by Design**
Criptografia, controle de acesso e auditoria em todos os componentes.

---

## ğŸ›ï¸ Arquitetura Geral

```mermaid
graph TB
    subgraph "Data Sources"
        SQL[SQL Server<br/>Transactional Database]
    end
    
    subgraph "Orchestration Layer"
        AF[Apache Airflow<br/>Workflow Management]
        AF_DB[(Airflow<br/>Metadata DB)]
        AF --> AF_DB
    end
    
    subgraph "Azure Data Lake - Medallion Architecture"
        LZ[Landing Zone<br/>Raw CSV Files]
        
        subgraph "Bronze Layer"
            BR_DT[Delta Tables<br/>Raw Data]
            BR_META[Metadata<br/>Lineage]
        end
        
        subgraph "Silver Layer"
            SV_DT[Delta Tables<br/>Clean Data]
            SV_QUAL[Data Quality<br/>Validations]
        end
        
        subgraph "Gold Layer"
            GD_DIM[Dimension Tables<br/>Master Data]
            GD_FACT[Fact Tables<br/>Transactional Data]
            GD_KPI[KPI Tables<br/>Aggregated Metrics]
        end
    end
    
    subgraph "Processing Engine"
        SPARK[Apache Spark<br/>Distributed Processing]
        DELTA[Delta Lake<br/>ACID Transactions]
        SPARK --> DELTA
    end
    
    subgraph "Infrastructure"
        DOCKER[Docker Containers<br/>Airflow Environment]
        AZURE[Azure Cloud<br/>Storage & Compute]
    end
    
    subgraph "Analytics & Consumption"
        BI[Power BI<br/>Dashboards]
        API[REST API<br/>Data Access]
        EXPORT[Data Export<br/>CSV/Parquet]
    end
    
    SQL -->|Extract| AF
    AF -->|Orchestrate| LZ
    LZ -->|Ingest| BR_DT
    BR_DT -->|Transform| SV_DT
    SV_DT -->|Aggregate| GD_DIM
    SV_DT -->|Aggregate| GD_FACT
    GD_FACT -->|Calculate| GD_KPI
    
    SPARK -.->|Process| BR_DT
    SPARK -.->|Process| SV_DT
    SPARK -.->|Process| GD_DIM
    SPARK -.->|Process| GD_FACT
    
    GD_KPI -->|Consume| BI
    GD_FACT -->|Query| API
    GD_DIM -->|Export| EXPORT
    
    DOCKER -.->|Host| AF
    AZURE -.->|Store| LZ
    AZURE -.->|Store| BR_DT
    AZURE -.->|Store| SV_DT
    AZURE -.->|Store| GD_DIM
```

---

## ğŸ“Š Arquitetura Medallion

### ğŸ” **Landing Zone**
**Finalidade**: Staging inicial dos dados brutos extraÃ­dos

- **Formato**: CSV files
- **RetenÃ§Ã£o**: 30 dias
- **Particionamento**: Por data de extraÃ§Ã£o
- **Schema**: Schema-on-read

```
landing/
â”œâ”€â”€ clientes_20241201_143022.csv
â”œâ”€â”€ motoristas_20241201_143025.csv
â”œâ”€â”€ veiculos_20241201_143028.csv
â””â”€â”€ entregas_20241201_143030.csv
```

### ğŸ¥‰ **Bronze Layer (Raw Data)**
**Finalidade**: Armazenamento histÃ³rico completo e imutÃ¡vel

- **Formato**: Delta Lake tables
- **Schema**: Preserva estrutura original + metadados
- **Particionamento**: Por ano/mÃªs/dia
- **RetenÃ§Ã£o**: Ilimitada (dados histÃ³ricos)

**CaracterÃ­sticas:**
- âœ… ACID transactions via Delta Lake
- âœ… Time travel e versionamento
- âœ… Metadados de ingestÃ£o
- âœ… CompactaÃ§Ã£o automÃ¡tica

```python
# Estrutura Bronze
bronze_schema = {
    "original_columns": "preservadas_como_extraÃ­das",
    "processing_date": "data_do_processamento",
    "processing_timestamp": "timestamp_do_processamento", 
    "source_file_name": "nome_do_arquivo_origem",
    "_bronze_ingestion_id": "id_Ãºnico_da_ingestÃ£o"
}
```

### ğŸ¥ˆ **Silver Layer (Clean Data)**
**Finalidade**: Dados limpos, padronizados e enriquecidos

- **Formato**: Delta Lake tables
- **Schema**: Padronizado e normalizado
- **Qualidade**: ValidaÃ§Ãµes e limpezas aplicadas
- **Particionamento**: Por data de negÃ³cio

**TransformaÃ§Ãµes Aplicadas:**
- ğŸ§¹ **Data Cleansing**: RemoÃ§Ã£o de duplicatas, valores nulos
- ğŸ“ **Standardization**: PadronizaÃ§Ã£o de formatos e tipos
- ğŸ”¤ **Normalization**: ConversÃ£o para maiÃºsculas, trim
- âœ… **Validation**: Regras de negÃ³cio e qualidade
- ğŸ·ï¸ **Enrichment**: AdiÃ§Ã£o de metadados Silver

```python
# Exemplo de transformaÃ§Ãµes Silver
def silver_transformations(df):
    return df \
        .dropDuplicates() \
        .withColumn("nome_cliente", upper(trim(col("nome_cliente")))) \
        .withColumn("cpf_cnpj", regexp_replace(col("cpf_cnpj"), "[^0-9]", "")) \
        .withColumn("_silver_ingestion_timestamp", current_timestamp()) \
        .withColumn("_source_table", lit(table_name))
```

### ğŸ¥‡ **Gold Layer (Business Data)**
**Finalidade**: Modelo dimensional para analytics e BI

- **Formato**: Delta Lake tables
- **Schema**: Star schema / Snowflake
- **OtimizaÃ§Ã£o**: Ãndices e particionamento para queries
- **AgregaÃ§Ãµes**: KPIs e mÃ©tricas prÃ©-calculadas

**Estrutura Dimensional:**

```mermaid
erDiagram
    Fato_Entregas ||--|| Dim_Data : "data_entrega"
    Fato_Entregas ||--|| Dim_Cliente_Remetente : "remetente"
    Fato_Entregas ||--|| Dim_Cliente_Destinatario : "destinatario"
    Fato_Entregas ||--|| Dim_Motorista : "motorista"
    Fato_Entregas ||--|| Dim_Veiculo : "veiculo"
    Fato_Entregas ||--|| Dim_Rota : "rota"
    Fato_Entregas ||--|| Dim_Tipo_Carga : "tipo_carga"
    
    Fato_Entregas {
        bigint id_entrega_key PK
        int data_inicio_key FK
        int data_previsao_key FK
        int data_fim_key FK
        int cliente_remetente_key FK
        int cliente_destinatario_key FK
        int motorista_key FK
        int veiculo_key FK
        int rota_key FK
        int tipo_carga_key FK
        decimal valor_frete
        decimal peso_carga_kg
        string status_entrega
    }
```

---

## âš™ï¸ Componentes TÃ©cnicos

### ğŸ›ï¸ **Apache Airflow**
**Responsabilidade**: OrquestraÃ§Ã£o e agendamento

```python
# ConfiguraÃ§Ã£o otimizada
AIRFLOW_CONFIG = {
    "executor": "LocalExecutor",
    "max_active_runs": 1,
    "max_active_tasks": 4,
    "catchup": False,
    "depends_on_past": False,
    "retries": 3,
    "retry_delay": timedelta(minutes=5)
}
```

**DAG Principal:**
```python
@dag(
    dag_id="sqlserver_to_bronze_adls",
    schedule="0 2 * * *",  # Daily at 2 AM
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=["etl", "production"]
)
def etl_pipeline():
    extract_task = extract_from_sqlserver()
    bronze_task = process_bronze_layer()
    silver_task = process_silver_layer()
    gold_task = process_gold_layer()
    
    extract_task >> bronze_task >> silver_task >> gold_task
```

### âš¡ **Apache Spark**
**Responsabilidade**: Processamento distribuÃ­do de dados

**ConfiguraÃ§Ãµes Otimizadas:**
```python
spark_config = {
    # Core Settings
    "spark.app.name": "projeto_etl_spark",
    "spark.sql.adaptive.enabled": "true",
    "spark.sql.adaptive.coalescePartitions.enabled": "true",
    
    # Delta Lake
    "spark.sql.extensions": "io.delta.sql.DeltaSparkSessionExtension",
    "spark.sql.catalog.spark_catalog": "org.apache.spark.sql.delta.catalog.DeltaCatalog",
    
    # Memory Management
    "spark.driver.memory": "4g",
    "spark.executor.memory": "4g",
    "spark.driver.maxResultSize": "2g",
    
    # Performance
    "spark.serializer": "org.apache.spark.serializer.KryoSerializer",
    "spark.sql.adaptive.skewJoin.enabled": "true",
    
    # Azure Integration
    "spark.hadoop.fs.azure.account.auth.type": "SAS",
    "spark.hadoop.fs.azure.sas.token.provider.type": "org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider"
}
```

### ğŸ”º **Delta Lake**
**Responsabilidade**: ACID transactions e versionamento

**BenefÃ­cios:**
- âœ… **ACID Transactions**: ConsistÃªncia em operaÃ§Ãµes concorrentes
- âœ… **Time Travel**: Acesso a versÃµes histÃ³ricas
- âœ… **Schema Evolution**: MudanÃ§as de schema sem quebrar compatibilidade
- âœ… **Merge Operations**: Upserts eficientes
- âœ… **Automatic Compaction**: OtimizaÃ§Ã£o automÃ¡tica de arquivos

```python
# Exemplo de merge operation
delta_table = DeltaTable.forPath(spark, gold_path)
delta_table.alias("target") \
    .merge(updates_df.alias("source"), "target.id = source.id") \
    .whenMatchedUpdateAll() \
    .whenNotMatchedInsertAll() \
    .execute()
```

---

## â˜ï¸ Infraestrutura Azure

### ğŸ—„ï¸ **Azure Data Lake Storage Gen2**
**ConfiguraÃ§Ã£o:**
- **Replication**: LRS (Locally Redundant Storage)
- **Access Tier**: Hot (dados frequentemente acessados)
- **Hierarchical Namespace**: Habilitado
- **Security**: SAS tokens com permissÃµes granulares

**Estrutura de Containers:**
```
adlsaccount/
â”œâ”€â”€ landing/          # Raw CSV files
â”œâ”€â”€ bronze/           # Delta tables - raw data
â”œâ”€â”€ silver/           # Delta tables - clean data
â”œâ”€â”€ gold/             # Delta tables - dimensional model
â””â”€â”€ checkpoints/      # Spark checkpoints
```

### ğŸ” **SeguranÃ§a**
- **Authentication**: SAS Tokens com expiraÃ§Ã£o
- **Authorization**: RBAC no Azure AD
- **Encryption**: At rest e in transit
- **Network**: Private endpoints (quando necessÃ¡rio)

---

## ğŸ”„ Fluxo de Dados Detalhado

### 1. **ExtraÃ§Ã£o (SQL Server â†’ Landing Zone)**
```python
def extract_from_sqlserver():
    # ConexÃ£o otimizada com pool de conexÃµes
    engine = create_engine(
        connection_string,
        pool_size=10,
        pool_recycle=3600,
        pool_pre_ping=True
    )
    
    # ExtraÃ§Ã£o incremental baseada em timestamp
    query = f"""
    SELECT * FROM {schema}.{table} 
    WHERE last_modified >= '{last_extraction_time}'
    """
    
    df = pd.read_sql(query, engine)
    
    # Upload para ADLS com timestamp
    filename = f"{table}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
    upload_to_adls(df, filename)
```

### 2. **Processamento Bronze (Landing â†’ Bronze)**
```python
def process_bronze_layer():
    # Leitura dos CSVs com schema inference
    df = spark.read \
        .option("header", "true") \
        .option("inferSchema", "true") \
        .csv(landing_path)
    
    # AdiÃ§Ã£o de metadados
    df_bronze = df \
        .withColumn("processing_date", current_date()) \
        .withColumn("processing_timestamp", current_timestamp()) \
        .withColumn("source_file_name", input_file_name())
    
    # Escrita em Delta format
    df_bronze.write \
        .format("delta") \
        .mode("append") \
        .option("mergeSchema", "true") \
        .save(bronze_path)
```

### 3. **Processamento Silver (Bronze â†’ Silver)**
```python
def process_silver_layer():
    # Leitura da camada Bronze
    df_bronze = spark.read.format("delta").load(bronze_path)
    
    # AplicaÃ§Ã£o de transformaÃ§Ãµes de qualidade
    df_silver = df_bronze \
        .dropDuplicates() \
        .filter(col("id").isNotNull()) \
        .withColumn("nome", upper(trim(col("nome")))) \
        .withColumn("cpf", regexp_replace(col("cpf"), "[^0-9]", ""))
    
    # ValidaÃ§Ãµes de negÃ³cio
    df_validated = apply_business_rules(df_silver)
    
    # Escrita com particionamento
    df_validated.write \
        .format("delta") \
        .mode("overwrite") \
        .partitionBy("ano", "mes") \
        .save(silver_path)
```

### 4. **Processamento Gold (Silver â†’ Gold)**
```python
def process_gold_layer():
    # CriaÃ§Ã£o das dimensÃµes
    create_dimensions()
    
    # CriaÃ§Ã£o das tabelas fato
    create_fact_tables()
    
    # CÃ¡lculo de KPIs
    calculate_kpis()
```

---

## ğŸ“Š EstratÃ©gia de Particionamento

### **Bronze Layer**
```
bronze/
â””â”€â”€ clientes/
    â””â”€â”€ year=2024/
        â””â”€â”€ month=12/
            â””â”€â”€ day=01/
                â”œâ”€â”€ part-00000.parquet
                â””â”€â”€ _delta_log/
```

### **Silver Layer**
```
silver/
â””â”€â”€ clientes/
    â””â”€â”€ ano=2024/
        â””â”€â”€ mes=12/
            â”œâ”€â”€ part-00000.parquet
            â””â”€â”€ _delta_log/
```

### **Gold Layer**
```
gold/
â”œâ”€â”€ dim_cliente/
â”‚   â””â”€â”€ part-00000.parquet
â”œâ”€â”€ dim_data/
â”‚   â””â”€â”€ part-00000.parquet
â””â”€â”€ fato_entregas/
    â””â”€â”€ data_entrega=2024-12-01/
        â””â”€â”€ part-00000.parquet
```

---

## ğŸ” Monitoramento e Observabilidade

### **MÃ©tricas Coletadas**
- â±ï¸ **Performance**: Tempo de execuÃ§Ã£o por task
- ğŸ“Š **Volume**: Quantidade de registros processados
- ğŸ’¾ **Storage**: Tamanho dos dados por camada
- âŒ **Errors**: Taxa de falhas e tipos de erro
- ğŸ”„ **Throughput**: Registros processados por segundo

### **Alertas Configurados**
- ğŸš¨ **Falha de execuÃ§Ã£o** de DAGs
- â° **SLA breach** (execuÃ§Ã£o > 2 horas)
- ğŸ“ˆ **Volume anÃ´malo** de dados
- ğŸ’¾ **EspaÃ§o em disco** baixo

### **Dashboards**
- ğŸ“Š **Airflow UI**: Status das execuÃ§Ãµes
- ğŸ“ˆ **Spark UI**: Performance das jobs
- â˜ï¸ **Azure Monitor**: MÃ©tricas de infraestrutura

---

## ğŸ”„ EstratÃ©gia de Backup e Recovery

### **Backup**
- ğŸ“… **Daily**: Snapshot automÃ¡tico das tabelas Gold
- ğŸ“… **Weekly**: Backup completo das camadas Silver e Bronze
- ğŸ“… **Monthly**: Archive para storage de longo prazo

### **Recovery**
- ğŸ”„ **Time Travel**: Delta Lake permite recuperaÃ§Ã£o point-in-time
- ğŸ“‹ **Replay**: Re-execuÃ§Ã£o de DAGs a partir de qualquer data
- ğŸ”„ **Rollback**: ReversÃ£o para versÃ£o anterior em caso de problemas

---

## ğŸš€ Escalabilidade

### **Horizontal Scaling**
- **Spark**: Aumento do nÃºmero de executors
- **Airflow**: MÃºltiplos workers
- **Azure**: Auto-scaling baseado em demanda

### **Vertical Scaling**
- **Memory**: Aumento da memÃ³ria por executor
- **CPU**: Mais cores por mÃ¡quina
- **Storage**: Upgrade para SSDs premium

### **OtimizaÃ§Ãµes**
- **Caching**: Dados frequentemente acessados
- **Indexing**: Ãndices Z-Order no Delta Lake
- **Compaction**: OtimizaÃ§Ã£o automÃ¡tica de arquivos pequenos

---

Esta arquitetura garante **alta disponibilidade**, **escalabilidade** e **manutenibilidade**, seguindo as melhores prÃ¡ticas da indÃºstria para pipelines de dados modernos. 