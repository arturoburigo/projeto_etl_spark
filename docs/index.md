# ğŸš€ Projeto ETL com Apache Spark & Azure Data Lake

<div class="grid cards" markdown>

-   :material-rocket-launch:{ .lg .middle } **Pipeline ETL Moderno**

    ---

    Pipeline escalÃ¡vel para processamento de dados em larga escala usando Apache Spark, orquestrado por Apache Airflow e armazenado no Azure Data Lake.

    [:octicons-arrow-right-24: InÃ­cio RÃ¡pido](inicio_rapido.md)

-   :material-cloud:{ .lg .middle } **Arquitetura Medallion**

    ---

    ImplementaÃ§Ã£o da arquitetura Medallion (Bronze, Silver, Gold) para organizaÃ§Ã£o e processamento de dados com qualidade empresarial.

    [:octicons-arrow-right-24: Ver Arquitetura](arquitetura.md)

-   :material-chart-line:{ .lg .middle } **Analytics & KPIs**

    ---

    Modelo dimensional com KPIs de negÃ³cio para anÃ¡lise de performance logÃ­stica e mÃ©tricas operacionais.

    [:octicons-arrow-right-24: Ver KPIs](kpis_metricas.md)

-   :material-cog:{ .lg .middle } **OrquestraÃ§Ã£o AvanÃ§ada**

    ---

    DAGs parametrizÃ¡veis no Airflow com monitoramento, retry automÃ¡tico e notificaÃ§Ãµes de status.

    [:octicons-arrow-right-24: Ver Airflow](airflow.md)

</div>

---

## ğŸ¯ Sobre o Projeto

Este projeto implementa um **pipeline ETL completo e moderno** que demonstra as melhores prÃ¡ticas para processamento de dados em larga escala. O sistema processa dados de um contexto de **logÃ­stica e transporte**, incluindo informaÃ§Ãµes sobre clientes, motoristas, veÃ­culos, entregas, rotas e manutenÃ§Ãµes.

### âœ¨ Principais CaracterÃ­sticas

- **ğŸ”„ ETL Completo**: ExtraÃ§Ã£o do SQL Server, transformaÃ§Ã£o com Spark, carregamento no Data Lake
- **ğŸ—ï¸ Arquitetura Medallion**: OrganizaÃ§Ã£o em camadas Bronze, Silver e Gold
- **âš¡ Processamento DistribuÃ­do**: Apache Spark com Delta Lake para ACID transactions
- **ğŸ›ï¸ OrquestraÃ§Ã£o**: Apache Airflow para automaÃ§Ã£o e monitoramento
- **â˜ï¸ Cloud Native**: IntegraÃ§Ã£o completa com Azure Data Lake Storage
- **ğŸ“Š Analytics Ready**: Modelo dimensional para Business Intelligence

---

## ğŸ› ï¸ Stack TecnolÃ³gico

<div class="grid cards" markdown>

-   :material-language-python:{ .lg .middle } **Python 3.10+**
    
    ---
    
    Linguagem principal para desenvolvimento do pipeline, com bibliotecas especializadas em processamento de dados.

-   :simple-apachespark:{ .lg .middle } **Apache Spark 3.x**
    
    ---
    
    Engine de processamento distribuÃ­do para transformaÃ§Ãµes de dados em larga escala com Delta Lake.

-   :simple-apacheairflow:{ .lg .middle } **Apache Airflow 2.x**
    
    ---
    
    Plataforma de orquestraÃ§Ã£o para automaÃ§Ã£o, agendamento e monitoramento do pipeline.

-   :simple-microsoftazure:{ .lg .middle } **Azure Data Lake**
    
    ---
    
    Armazenamento escalÃ¡vel e seguro para dados estruturados e semi-estruturados.

-   :simple-docker:{ .lg .middle } **Docker & Compose**
    
    ---
    
    ContainerizaÃ§Ã£o para ambiente consistente e deploy simplificado.

-   :simple-terraform:{ .lg .middle } **Terraform**
    
    ---
    
    Infrastructure as Code para provisionamento automatizado de recursos Azure.

</div>

---

## ğŸ“Š Modelo de Dados

O projeto processa dados de um sistema de **logÃ­stica e transporte** com as seguintes entidades principais:

```mermaid
erDiagram
    CLIENTES ||--o{ ENTREGAS : "remetente/destinatario"
    MOTORISTAS ||--o{ ENTREGAS : "responsavel"
    VEICULOS ||--o{ ENTREGAS : "transporta"
    VEICULOS ||--o{ MANUTENCOES : "sofre"
    VEICULOS ||--o{ ABASTECIMENTOS : "consome"
    VEICULOS ||--o{ MULTAS : "recebe"
    ROTAS ||--o{ ENTREGAS : "utiliza"
    TIPOS_CARGA ||--o{ ENTREGAS : "categoria"
    ENTREGAS ||--o{ COLETAS : "inclui"
    
    CLIENTES {
        int id_cliente
        string nome_cliente
        string tipo_cliente
        string cpf_cnpj
        string email
        string telefone
        string endereco
    }
    
    ENTREGAS {
        int id_entrega
        int id_cliente_remetente
        int id_cliente_destinatario
        int id_motorista
        int id_veiculo
        int id_rota
        date data_inicio
        date data_previsao
        string status_entrega
        decimal valor_frete
    }
    
    VEICULOS {
        int id_veiculo
        string placa
        string modelo
        string marca
        int ano_fabricacao
        decimal capacidade_kg
        string tipo_veiculo
    }
```

---

## ğŸš€ InÃ­cio RÃ¡pido

### 1. **PrÃ©-requisitos**

Certifique-se de ter instalado:

- [x] Python 3.10+
- [x] Docker & Docker Compose
- [x] Azure CLI
- [x] Poetry (gerenciador de dependÃªncias)

### 2. **InstalaÃ§Ã£o**

```bash
# Clone o repositÃ³rio
git clone https://github.com/seu-usuario/projeto_etl_spark.git
cd projeto_etl_spark

# Instale as dependÃªncias
poetry install

# Configure as variÃ¡veis de ambiente
cp .env.example .env
# Edite o arquivo .env com suas credenciais Azure
```

### 3. **ExecuÃ§Ã£o**

```bash
# Inicie o Airflow
cd astro
astro dev start

# Acesse a interface web
# http://localhost:8080 (admin/admin)
```

### 4. **Execute o Pipeline**

1. Navegue atÃ© a DAG `sqlserver_to_bronze_adls`
2. Clique em "Trigger DAG"
3. Monitore a execuÃ§Ã£o na interface do Airflow

!!! tip "Dica"
    Para uma configuraÃ§Ã£o mais detalhada, consulte o [Guia de InstalaÃ§Ã£o](instalacao.md) completo.

---

## ğŸ“ˆ Pipeline de Dados

### ğŸ”„ Fluxo de ExecuÃ§Ã£o

```mermaid
graph LR
    A[SQL Server] --> B[Landing Zone]
    B --> C[Bronze Layer]
    C --> D[Silver Layer]
    D --> E[Gold Layer]
    E --> F[Analytics & BI]
    
    subgraph "Processamento"
        C -.-> G[Spark ETL]
        D -.-> G
        E -.-> G
    end
    
    subgraph "OrquestraÃ§Ã£o"
        H[Airflow DAGs]
        H -.-> B
        H -.-> C
        H -.-> D
        H -.-> E
    end
```

### ğŸ“Š Camadas de Dados

| Camada | DescriÃ§Ã£o | Formato | Finalidade |
|--------|-----------|---------|------------|
| **ğŸ” Landing** | Dados brutos extraÃ­dos | CSV | Staging inicial |
| **ğŸ¥‰ Bronze** | Dados histÃ³ricos completos | Delta | Data Lake |
| **ğŸ¥ˆ Silver** | Dados limpos e padronizados | Delta | Analytics |
| **ğŸ¥‡ Gold** | Modelo dimensional | Delta | Business Intelligence |

---

## ğŸ¯ KPIs e MÃ©tricas

O projeto calcula automaticamente os seguintes indicadores de performance:

<div class="grid cards" markdown>

-   :material-truck-delivery:{ .lg .middle } **On-Time Delivery**

    ---

    Percentual de entregas realizadas dentro do prazo estabelecido.
    
    **Meta**: > 95%

-   :material-map-marker-path:{ .lg .middle } **Custo por Rota**

    ---

    Custo mÃ©dio de frete por quilÃ´metro em cada rota.
    
    **AnÃ¡lise**: Semanal

-   :material-truck:{ .lg .middle } **UtilizaÃ§Ã£o da Frota**

    ---

    Total de entregas por tipo de veÃ­culo e taxa de ocupaÃ§Ã£o.
    
    **FrequÃªncia**: Mensal

-   :material-account-cash:{ .lg .middle } **Revenue por Cliente**

    ---

    Valor total de frete gerado por cada cliente.
    
    **SegmentaÃ§Ã£o**: Por regiÃ£o

</div>

---

## ğŸ“š PrÃ³ximos Passos

<div class="grid cards" markdown>

-   :material-book-open-page-variant:{ .lg .middle } **[ğŸ“– DocumentaÃ§Ã£o Completa](instalacao.md)**

    ---

    Guias detalhados de instalaÃ§Ã£o, configuraÃ§Ã£o e uso do sistema.

-   :material-architecture:{ .lg .middle } **[ğŸ—ï¸ Arquitetura](arquitetura.md)**

    ---

    VisÃ£o detalhada da arquitetura do sistema e decisÃµes de design.

-   :material-pipe:{ .lg .middle } **[ğŸ”§ Pipeline ETL](pipeline_etl.md)**

    ---

    DocumentaÃ§Ã£o tÃ©cnica do pipeline de dados e transformaÃ§Ãµes.

-   :material-test-tube:{ .lg .middle } **[ğŸ§ª Testes](testes.md)**

    ---

    EstratÃ©gia de testes, cobertura e como executar os testes.

</div>

---

## ğŸ¤ ContribuiÃ§Ã£o

ContribuiÃ§Ãµes sÃ£o sempre bem-vindas! Este projeto segue as melhores prÃ¡ticas de desenvolvimento colaborativo:

- **Code Review**: Todos os PRs passam por revisÃ£o
- **Testes Automatizados**: Cobertura de testes > 80%
- **DocumentaÃ§Ã£o**: Toda funcionalidade deve ser documentada
- **PadrÃµes**: Seguimos PEP 8 e usamos Black para formataÃ§Ã£o

[Como Contribuir â†’](contribuicao.md){ .md-button .md-button--primary }

---

## ğŸ‘¥ Equipe

Este projeto foi desenvolvido por uma equipe multidisciplinar de especialistas em dados:

- **Arturo Burigo** - Tech Lead & Architecture
- **Luiz Bezerra** - Data Engineer
- **Gabriel Morona** - Spark Developer  
- **Maria Laura** - Data Analyst
- **Amanda Dimas** - QA Engineer

---

!!! info "LicenÃ§a"
    Este projeto estÃ¡ licenciado sob a **MIT License**. Veja o arquivo [LICENSE](https://github.com/seu-usuario/projeto_etl_spark/blob/main/LICENSE) para detalhes.