# ğŸš€ InÃ­cio RÃ¡pido

## ğŸ“‹ PrÃ©-requisitos

Antes de comeÃ§ar, certifique-se de ter os seguintes itens instalados em seu sistema:

### ğŸ› ï¸ **Ferramentas Essenciais**

=== "Windows"
    ```powershell
    # Instalar Python 3.10+
    winget install Python.Python.3.10
    
    # Instalar Docker Desktop
    winget install Docker.DockerDesktop
    
    # Instalar Azure CLI
    winget install Microsoft.AzureCLI
    
    # Instalar Git
    winget install Git.Git
    ```

=== "macOS"
    ```bash
    # Instalar Homebrew (se nÃ£o tiver)
    /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
    
    # Instalar Python 3.10+
    brew install python@3.10
    
    # Instalar Docker Desktop
    brew install --cask docker
    
    # Instalar Azure CLI
    brew install azure-cli
    
    # Instalar Git
    brew install git
    ```

=== "Linux (Ubuntu/Debian)"
    ```bash
    # Atualizar repositÃ³rios
    sudo apt update
    
    # Instalar Python 3.10+
    sudo apt install python3.10 python3.10-pip python3.10-venv
    
    # Instalar Docker
    curl -fsSL https://get.docker.com -o get-docker.sh
    sh get-docker.sh
    
    # Instalar Azure CLI
    curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash
    
    # Instalar Git
    sudo apt install git
    ```

### ğŸ“¦ **Poetry (Gerenciador de DependÃªncias)**

```bash
# Instalar Poetry
curl -sSL https://install.python-poetry.org | python3 -

# Adicionar ao PATH (adicione ao seu .bashrc/.zshrc)
export PATH="$HOME/.local/bin:$PATH"

# Verificar instalaÃ§Ã£o
poetry --version
```

---

## âš¡ InstalaÃ§Ã£o em 5 Minutos

### **1. Clone o RepositÃ³rio**

```bash
git clone https://github.com/seu-usuario/projeto_etl_spark.git
cd projeto_etl_spark
```

### **2. Configure o Ambiente Python**

```bash
# Criar ambiente virtual com Poetry
poetry install

# Ativar o ambiente virtual
poetry shell
```

### **3. Configure as VariÃ¡veis de Ambiente**

```bash
# Copiar arquivo de exemplo
cp .env.example .env

# Editar com suas credenciais
nano .env  # ou vim .env, ou qualquer editor de sua preferÃªncia
```

**Exemplo de configuraÃ§Ã£o `.env`:**
```bash
# Azure Data Lake
ADLS_ACCOUNT_NAME=seuaccountstorage
ADLS_FILE_SYSTEM_NAME=landing
ADLS_BRONZE_CONTAINER_NAME=bronze
ADLS_SILVER_CONTAINER_NAME=silver
ADLS_GOLD_CONTAINER_NAME=gold
ADLS_SAS_TOKEN="sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupitfx&se=2024-12-31T23:59:59Z&st=2024-01-01T00:00:00Z&spr=https&sig=SUA_ASSINATURA_SAS"

# SQL Server
SQL_SERVER=seu-servidor.database.windows.net
SQL_DATABASE=LogisticaDB
SQL_SCHEMA=dbo
SQL_USERNAME=admin
SQL_PASSWORD=SuaSenhaSegura123!

# Spark Configuration (opcional)
SPARK_DRIVER_MEMORY=4g
SPARK_EXECUTOR_MEMORY=4g
SPARK_EXECUTOR_CORES=2
```

### **4. Inicie o Airflow**

```bash
# Navegar para o diretÃ³rio do Airflow
cd astro

# Iniciar o ambiente Airflow
astro dev start
```

!!! tip "Dica"
    O comando `astro dev start` pode demorar alguns minutos na primeira execuÃ§Ã£o, pois precisa baixar as imagens Docker.

### **5. Acesse a Interface Web**

ApÃ³s a inicializaÃ§Ã£o completa, acesse:

- ğŸŒ **Airflow UI**: [http://localhost:8080](http://localhost:8080)
- ğŸ‘¤ **Credenciais**: `admin` / `admin`

---

## ğŸ¯ Primeira ExecuÃ§Ã£o

### **1. Verifique as ConexÃµes**

No Airflow UI, vÃ¡ para **Admin â†’ Connections** e verifique se as conexÃµes estÃ£o configuradas:

- âœ… `azure_data_lake_default`
- âœ… `sql_server_default`

### **2. Execute o Pipeline**

1. Navegue para **DAGs** na interface do Airflow
2. Encontre a DAG `sqlserver_to_bronze_adls`
3. Clique no botÃ£o **â–¶ï¸ Trigger DAG**
4. Monitore a execuÃ§Ã£o na aba **Graph View**

### **3. Verifique os Resultados**

ApÃ³s a execuÃ§Ã£o bem-sucedida, vocÃª deve ver:

- âœ… **Landing Zone**: Arquivos CSV no container `landing`
- âœ… **Bronze Layer**: Tabelas Delta no container `bronze`
- âœ… **Silver Layer**: Dados limpos no container `silver`
- âœ… **Gold Layer**: Modelo dimensional no container `gold`

---

## ğŸ”§ ConfiguraÃ§Ã£o AvanÃ§ada

### **Azure Data Lake Setup**

Se vocÃª ainda nÃ£o tem um Azure Data Lake configurado:

```bash
# Login no Azure
az login

# Criar Resource Group
az group create --name rg-projeto-etl --location brazilsouth

# Criar Storage Account
az storage account create \
    --name seuaccountstorage \
    --resource-group rg-projeto-etl \
    --location brazilsouth \
    --sku Standard_LRS \
    --kind StorageV2 \
    --hierarchical-namespace true

# Criar containers
az storage container create --name landing --account-name seuaccountstorage
az storage container create --name bronze --account-name seuaccountstorage
az storage container create --name silver --account-name seuaccountstorage
az storage container create --name gold --account-name seuaccountstorage

# Gerar SAS Token (vÃ¡lido por 1 ano)
az storage account generate-sas \
    --account-name seuaccountstorage \
    --account-key $(az storage account keys list --account-name seuaccountstorage --query '[0].value' -o tsv) \
    --expiry 2024-12-31T23:59:59Z \
    --permissions racwdlup \
    --resource-types sco \
    --services bfqt
```

### **SQL Server Setup (Opcional)**

Se vocÃª quiser usar dados de exemplo:

```bash
# Navegar para o diretÃ³rio de dados
cd data

# Executar script de criaÃ§Ã£o de tabelas
python create_tables.py

# Gerar dados sintÃ©ticos
python faker_data.py
```

---

## ğŸ§ª Teste RÃ¡pido

### **Teste de ConexÃ£o SQL Server**

```bash
# Dentro do ambiente Poetry
poetry shell

# Executar teste de conexÃ£o
python astro/tests/test_sqlserver_connection.py
```

### **Teste de DAG**

```bash
# Teste de sintaxe da DAG
python astro/dags/main.py

# Teste unitÃ¡rio
pytest astro/tests/test_dag_example.py
```

### **Teste de Spark**

```python
# Abrir Python interativo
python

# Testar Spark
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("teste_rapido") \
    .getOrCreate()

# Criar DataFrame de teste
df = spark.createDataFrame([(1, "teste"), (2, "spark")], ["id", "nome"])
df.show()

spark.stop()
```

---

## ğŸ“Š Monitoramento

### **Logs do Airflow**

```bash
# Ver logs em tempo real
astro dev logs

# Logs especÃ­ficos de um serviÃ§o
astro dev logs scheduler
astro dev logs webserver
```

### **Interface de Monitoramento**

- ğŸ“Š **Airflow UI**: [http://localhost:8080](http://localhost:8080)
- ğŸ“ˆ **Flower (Celery)**: [http://localhost:5555](http://localhost:5555)
- ğŸ—„ï¸ **Postgres**: [http://localhost:5432](http://localhost:5432)

---

## ğŸš¨ Troubleshooting

### **Problemas Comuns**

#### **1. Erro de ConexÃ£o com Azure**

```bash
# Verificar conectividade
az storage blob list --container-name landing --account-name seuaccountstorage

# Testar SAS Token
curl "https://seuaccountstorage.blob.core.windows.net/landing?sv=2022-11-02&ss=bfqt..."
```

#### **2. Erro de MemÃ³ria no Spark**

Edite o arquivo `.env`:
```bash
SPARK_DRIVER_MEMORY=2g
SPARK_EXECUTOR_MEMORY=2g
```

#### **3. Airflow nÃ£o Inicia**

```bash
# Parar todos os serviÃ§os
astro dev stop

# Limpar containers
docker system prune -f

# Reiniciar
astro dev start
```

#### **4. Problemas de PermissÃ£o**

```bash
# Linux/macOS - Ajustar permissÃµes
sudo chown -R $USER:$USER .
chmod -R 755 .
```

### **Comandos Ãšteis**

```bash
# Verificar status dos containers
docker ps

# Ver logs de um container especÃ­fico
docker logs <container_id>

# Reiniciar apenas o Airflow
astro dev restart

# Executar comando dentro do container
astro dev bash
```

---

## ğŸ“š PrÃ³ximos Passos

Agora que vocÃª tem o ambiente funcionando, explore:

1. ğŸ“– **[Arquitetura](arquitetura.md)** - Entenda como o sistema funciona
2. ğŸ”§ **[Pipeline ETL](pipeline_etl.md)** - Detalhes do processamento de dados
3. ğŸ“Š **[KPIs e MÃ©tricas](kpis_metricas.md)** - Indicadores calculados
4. ğŸ›ï¸ **[Airflow](airflow.md)** - OrquestraÃ§Ã£o avanÃ§ada
5. ğŸ§ª **[Testes](testes.md)** - Como testar o sistema

---

## ğŸ†˜ Precisa de Ajuda?

- ğŸ“– **DocumentaÃ§Ã£o**: [docs/](../index.md)
- ğŸ› **Issues**: [GitHub Issues](https://github.com/seu-usuario/projeto_etl_spark/issues)
- ğŸ’¬ **DiscussÃµes**: [GitHub Discussions](https://github.com/seu-usuario/projeto_etl_spark/discussions)
- ğŸ“§ **Email**: contato@projeto-etl.com

---

!!! success "ParabÃ©ns! ğŸ‰"
    VocÃª configurou com sucesso o projeto ETL com Apache Spark e Azure Data Lake. O pipeline estÃ¡ pronto para processar dados em larga escala! 