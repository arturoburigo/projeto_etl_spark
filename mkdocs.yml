site_name: Projeto ETL com Apache Spark & Azure Data Lake
site_description: Pipeline ETL moderno para processamento de dados em larga escala usando Apache Spark, Airflow e Azure Data Lake
site_author: Equipe de Data Engineering
site_url: https://seu-dominio.github.io/projeto_etl_spark/

# Repository
repo_name: projeto_etl_spark
repo_url: https://github.com/seu-usuario/projeto_etl_spark
edit_uri: edit/main/docs/

# Configuration
theme:
  name: material
  language: pt
  
  # Color scheme
  palette:
    - scheme: default
      primary: blue
      accent: orange
      toggle:
        icon: material/brightness-7
        name: Switch to dark mode
    - scheme: slate
      primary: blue
      accent: orange
      toggle:
        icon: material/brightness-4
        name: Switch to light mode
  
  # Features
  features:
    - navigation.tabs
    - navigation.tabs.sticky
    - navigation.sections
    - navigation.expand
    - navigation.path
    - navigation.top
    - search.highlight
    - search.share
    - search.suggest
    - toc.follow
    - content.code.copy
    - content.code.select
    - content.code.annotate
    - content.tabs.link
  
  # Icons
  icon:
    repo: fontawesome/brands/github
    edit: material/pencil
    view: material/eye
  
  # Logo
  logo: assets/logo.png
  favicon: assets/favicon.ico

# Plugins
plugins:
  - search:
      lang: pt
  - mermaid2:
      arguments:
        theme: base
        themeVariables:
          primaryColor: '#1976d2'
          primaryTextColor: '#ffffff'
  - git-revision-date-localized:
      type: date
      locale: pt
  - minify:
      minify_html: true

# Extensions
markdown_extensions:
  - abbr
  - admonition
  - attr_list
  - def_list
  - footnotes
  - md_in_html
  - toc:
      permalink: true
  - pymdownx.arithmatex:
      generic: true
  - pymdownx.betterem:
      smart_enable: all
  - pymdownx.caret
  - pymdownx.details
  - pymdownx.emoji:
      emoji_generator: !!python/name:material.extensions.emoji.to_svg
      emoji_index: !!python/name:material.extensions.emoji.twemoji
  - pymdownx.highlight:
      anchor_linenums: true
      line_spans: __span
      pygments_lang_class: true
  - pymdownx.inlinehilite
  - pymdownx.keys
  - pymdownx.magiclink:
      repo_url_shorthand: true
      user: seu-usuario
      repo: projeto_etl_spark
  - pymdownx.mark
  - pymdownx.smartsymbols
  - pymdownx.superfences:
      custom_fences:
        - name: mermaid
          class: mermaid
          format: !!python/name:pymdownx.superfences.fence_code_format
  - pymdownx.tabbed:
      alternate_style: true
  - pymdownx.tasklist:
      custom_checkbox: true
  - pymdownx.tilde

# Navigation
nav:
  - ğŸ  InÃ­cio: index.md
  - ğŸ“š DocumentaÃ§Ã£o:
    - ğŸš€ InÃ­cio RÃ¡pido: inicio_rapido.md
    - ğŸ› ï¸ InstalaÃ§Ã£o: instalacao.md
    - âš™ï¸ ConfiguraÃ§Ã£o: configuracao.md
    - ğŸ” VariÃ¡veis de Ambiente: env_example.md
    - ğŸ—ï¸ Arquitetura: arquitetura.md
  - ğŸ”§ Pipeline ETL:
    - ğŸ“Š VisÃ£o Geral: pipeline_etl.md
    - ğŸ¥‰ Camada Bronze: camada_bronze.md
    - ğŸ¥ˆ Camada Silver: camada_silver.md
    - ğŸ¥‡ Camada Gold: camada_gold.md
    - ğŸ“ˆ KPIs e MÃ©tricas: kpis_metricas.md
    - âš¡ OtimizaÃ§Ã£o Spark: otimizacao_spark.md
  - ğŸ›ï¸ OrquestraÃ§Ã£o:
    - Apache Airflow: airflow.md
    - ğŸ“‹ DAGs: dags.md
    - ğŸ” Monitoramento: monitoramento.md
  - ğŸ§ª Desenvolvimento:
    - ğŸ—ï¸ Estrutura do Projeto: estrutura_projeto.md
    - ğŸ§ª Testes: testes.md
    - ğŸ“ Como Usar: uso.md
    - ğŸ¤ ContribuiÃ§Ã£o: contribuicao.md
  - ğŸš€ Deploy:
    - â˜ï¸ Azure: deploy_azure.md
    - ğŸ³ Docker: deploy_docker.md
    - ğŸ—ï¸ Terraform: terraform.md
  - ğŸ“– ReferÃªncia:
    - ğŸ”§ API: api_reference.md
    - ğŸ“Š Schemas: schemas.md
    - â“ FAQ: faq.md
    - ğŸ†˜ Troubleshooting: troubleshooting.md

# Extra
extra:
  social:
    - icon: fontawesome/brands/github
      link: https://github.com/arturoburigo
    - icon: fontawesome/brands/linkedin
      link: https://www.linkedin.com/in/arturoburigo/
  
  version:
    provider: mike
    default: latest

# Copyright
copyright: Copyright &copy; 2025 Equipe de Data Engineering